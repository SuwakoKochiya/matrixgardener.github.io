<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="ma在 Github 上的个人博客">
    <meta name="keyword" content="">
    <meta name="theme-color" content="#600090">
    <meta name="msapplication-navbutton-color" content="#600090">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#600090">
    <link rel="shortcut icon" href="https://cdn4.iconfinder.com/data/icons/ionicons/512/icon-person-128.png">
    <link rel="alternate" type="application/atom+xml" title="Matrix" href="/atom.xml">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.css">
    <title>
        
        03机器学习实战-第3章 决策树｜Matrix&#39;s blog
        
    </title>

    <link rel="canonical" href="http://matrixgardener.github.io/2018/01/28/03机器学习实战-第3章 决策树/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/blog-style.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<style>

    header.intro-header {
        background-image: url('/image/img11.png')
    }
</style>
<!-- hack iOS CSS :active style -->
<body ontouchstart="" class="animated fadeIn">
<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top " id="nav-top" data-ispost = "true" data-istags="false
" data-ishome = "false" >
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand animated pulse" href="/">
                <span class="brand-logo">
                    Matrix
                </span>
                's Blog
            </a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <!-- /.navbar-collapse -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
					
                    
                        
							
                        <li>
                            <a href="/Tags/">Tags</a>
                        </li>
							
						
                    
                        
							
								
							
						
                    
                        
							
                        <li>
                            <a href="/categories/">categories</a>
                        </li>
							
						
                    
					
					
						<li>
							<a href="/about">About</a>
						</li>
					
                </ul>
            </div>
        </div>
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
//    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

<!-- Main Content -->

<!--only post-->


<img class="wechat-title-img"
     src="/image/img7.png">


<style>
    
    header.intro-header {
        background-image: url('/image/img7.png')
    }

    
</style>

<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center">
                <div class="post-heading">
                    <h1>03机器学习实战-第3章 决策树</h1>
                    
                    <span class="meta">
                         作者 matrixgardener
                        <span>
                          日期 2018-01-28
                         </span>
                    </span>
                    <div class="tags text-center">
                        
                        <a class="tag" href="/tags/#机器学习"
                           title="机器学习">机器学习</a>
                        
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="post-title-haojen">
        <span>
            03机器学习实战-第3章 决策树
        </span>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->
            <div class="col-lg-8 col-lg-offset-1 col-sm-9 post-container">
                <p>[TOC]</p>
<h1 id="第3章-决策树"><a href="#第3章-决策树" class="headerlink" title="第3章 决策树"></a>第3章 决策树</h1><h2 id="本章内容"><a href="#本章内容" class="headerlink" title="本章内容"></a>本章内容</h2><blockquote>
<ul>
<li>决策树简介  </li>
<li>在数据集中度量一致性  </li>
<li>使用递归构造决策树     </li>
<li>使用Matplotlib绘制树形图   </li>
</ul>
</blockquote>
<h2 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h2><blockquote>
<p>优点：计算复杂度不高，输出易于理解，对中间值得确实不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题<br>使用数据类型：数值型和标称型  </p>
</blockquote>
<p><strong>创建分支伪代码函数createBranch()如下：</strong><br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">检测数据集中的每个指向是否属于同一个分类：</span><br><span class="line">    <span class="keyword">IF</span> <span class="keyword">so</span> <span class="keyword">return</span> 类标签</span><br><span class="line">    <span class="keyword">Else</span></span><br><span class="line">        寻找划分数据集的最好特征</span><br><span class="line">        划分数据集</span><br><span class="line">        创建分支节点</span><br><span class="line">            <span class="keyword">for</span> 每个划分的子集</span><br><span class="line">                调用函数createBranch并增加返回结果到分支节点中</span><br><span class="line">        <span class="keyword">return</span> 分支节点</span><br></pre></td></tr></table></figure></p>
<p>上述是一个递归函数</p>
<h2 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h2><blockquote>
<p>(1) 收集数据：可以使用任何方法。<br>(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>(4) 训练算法：构造树的数据结构。<br>(5) 测试算法：使用经验树计算错误率。<br>(6) 使用算法：词步骤可以使用于任何监督学习算法，而使用决策树可能更好地理解数据的内在含义。  </p>
</blockquote>
<p><strong> 摘要</strong></p>
<ol>
<li>信息论相关知识</li>
<li>决策树算法原理</li>
<li>代码实现与解释   </li>
</ol>
<p>今天总结决策树算法，目前建立决策树有三种主要算法：ID3、C4.5以及CART。由于算法知识点比较琐碎，我分成两节来总结。</p>
<p>第一节主要是梳理决策树算法中ID3和C4.5的知识点；第二节主要梳理剪枝技术、CART算法和随机森林算法的知识。</p>
<h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><h3 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1.信息熵"></a>1.信息熵</h3><p>在决策树算法中，熵是一个非常非常重要的概念。</p>
<p>一件事发生的概率越小，我们说它所蕴含的信息量越大。</p>
<p>比如：我们听女人能怀孕不奇怪，如果某天听到哪个男人怀孕了，我们就会觉得emmm…信息量很大了。</p>
<p>所以我们这样衡量信息量：</p>
<script type="math/tex; mode=display">
i(y)=-log{P(y)}</script><p>其中，$P(y)$是事件发生的概率。</p>
<p>信息熵就是所有可能发生事件的信息量的期望：  </p>
<script type="math/tex; mode=display">
H(Y)=-\sum_{i=1}^{n}P(y_i)log{P(y_i)}</script><p>表达了$Y$事件发生的不确定度。  </p>
<h3 id="2-条件熵"><a href="#2-条件熵" class="headerlink" title="2.条件熵"></a>2.条件熵</h3><p>条件熵：表示在X给定条件下，$Y$的条件概率分布的熵对$X$的数学期望。其数学推导如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} % requires amsmath; align* for no eq. number
H(Y|X) & =\sum_{x\in{X}}P{(x)}H(Y|X=x) \\
   & =-\sum_{x\in{X}}P(x)\sum_{y\in{Y}}P(y|x)log{P(y|x)}\\
   & =-\sum_{x\in{X}}\sum_{y\in{Y}}P(x,y)log{P(y|x)}
\end{aligned}</script><p>条件熵$H（Y|X）$表示在已知随机变量$X$的条件下随机变量Y的不确定性。注意一下，条件熵中X也是一个变量，意思是在一个变量$X$的条件下（变量$X$的每个值都会取到），另一个变量$Y$的熵对$X$的期望。</p>
<p>举个例子</p>
<p>例：女生决定主不主动追一个男生的标准有两个：颜值和身高，如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>颜值</th>
<th>身高</th>
<th>追不追</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>帅</td>
<td>高</td>
<td>追</td>
</tr>
<tr>
<td>2</td>
<td>帅</td>
<td>不高</td>
<td>追</td>
</tr>
<tr>
<td>3</td>
<td>不帅</td>
<td>高</td>
<td>不追</td>
</tr>
</tbody>
</table>
</div>
<p>上表中随机变量$Y=\{追，不追\}$，$P(Y=追)=2/3$，$P(Y=不追)=1/3$，得到$Y$的熵：</p>
<script type="math/tex; mode=display">
\begin{aligned} % requires amsmath; align* for no eq. number
H(Y) & =-\frac{2}{3}log\frac{2}{3}-\frac{1}{3}log\frac{1}{3} \\
   & =0.918
\end{aligned}</script><p>这里还有一个特征变量$X$，$X=｛高，不高｝$。当$X=高$时，追的个数为1，占1/2，不追的个数为1，占1/2，此时：</p>
<script type="math/tex; mode=display">
H(Y|X=高)=-\frac{1}{2}log\frac{1}{2}-\frac{1}{2}log\frac{1}{2}</script><p>同理：</p>
<script type="math/tex; mode=display">
H(Y|X=不高)=-{1}log{1}-{1}log{1}</script><p>（注意：我们一般约定，当$p=0$时，$plogp=0$）</p>
<p>所以我们得到条件熵的计算公式：  </p>
<script type="math/tex; mode=display">
\begin{aligned} % requires amsmath; align* for no eq. number
H(Y|X=身高) & =P(X=不高)*H(Y|X=不高)+P(X=高)*H(Y|X=高)\\
            & =0.67
\end{aligned}</script><h3 id="3-信息增益"><a href="#3-信息增益" class="headerlink" title="3.信息增益"></a>3.信息增益</h3><p>当我们用另一个变量$X$对原变量$Y$分类后，原变量$Y$的不确定性就会减小了（即熵值减小）。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：</p>
<script type="math/tex; mode=display">
Gain(Y,X)=H(Y)-H(Y|X)</script><p>此外，信息论中还有互信息、交叉熵等概念，它们与本算法关系不大，这里不展开。 </p>
<h2 id="代码实现与解读"><a href="#代码实现与解读" class="headerlink" title="代码实现与解读"></a>代码实现与解读</h2><p><strong>1.计算给定数据的香浓熵 </strong>    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算给定数据集的香农熵</span></span><br><span class="line"><span class="comment">#从math中导入log函数</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)   <span class="comment">#计算实例中的个数</span></span><br><span class="line">    </span><br><span class="line">    labelCounts = &#123;&#125;    <span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:    <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">        </span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]    <span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():     <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">            </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>       <span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">          </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>     <span class="comment">#标签为currentLabel的个数加1       </span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries    <span class="comment">#计算每一个标签的概率p</span></span><br><span class="line">        </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)    <span class="comment">#log base 2利用公式计算香农熵</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="comment">#change to discrete values</span></span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line">myDat,labels=createDataSet()</span><br><span class="line">myDat,labels</span><br></pre></td></tr></table></figure>
<pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]],
 [&#39;no surfacing&#39;, &#39;flippers&#39;])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calcShannonEnt(myDat)</span><br></pre></td></tr></table></figure>
<pre><code>0.9709505944546686
</code></pre><p><strong>2.创建选取的数据特征属性划分数据集</strong></p>
<p>程序清单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照给定特征划分数据集</span></span><br><span class="line"><span class="comment">#参数解释：dataSet待划分数据集</span></span><br><span class="line"><span class="comment">#axis：划分数据集的特征，这个函数里指函数第几列</span></span><br><span class="line"><span class="comment">#value：特征返回值，指的是特征划分的标准</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []     <span class="comment">#创建一个新列表</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:   <span class="comment">#如果这组数据特征值等于特征返回值的话</span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec = featVec[:axis]       <span class="comment">#这两行是把原来的数据除掉划分数据的特征那一列 </span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)   <span class="comment">#把列表reduceFeatVect放入retDataSet中</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">1</span>) </span><br><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line"><span class="comment"># 将myDat的第1列按照取出所有等于1的方式划分</span></span><br></pre></td></tr></table></figure>
<pre><code>[[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[[1, &#39;no&#39;], [1, &#39;no&#39;]]
</code></pre><p><strong>3.根据信息增益准则，选取最好的划分特征</strong></p>
<p>程序清单：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#找到最好的数据集划分方式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>    <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)   <span class="comment">#计算数据原始香农熵</span></span><br><span class="line">   </span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>   <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):       </span><br><span class="line">        </span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">        </span><br><span class="line">        uniqueVals = set(featList)    <span class="comment">#set()，生成一个集合数据类型，其和列表类型一样，不同之处在于</span></span><br><span class="line">                                      <span class="comment">#集合数据类型里面的值不重复，是唯一的</span></span><br><span class="line">        </span><br><span class="line">        newEntropy = <span class="number">0.0</span>    <span class="comment">#初始化新熵为0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:    <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">           </span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)    <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">            </span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))   <span class="comment">#计算概率</span></span><br><span class="line">            </span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)    <span class="comment">#计算新熵 </span></span><br><span class="line">       </span><br><span class="line">        infoGain = baseEntropy - newEntropy     <span class="comment">#计算信息增益calculate the info gain; ie reduction in entropy</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):      <span class="comment">#得到最大的信息增益和选取特征 #compare this to the best gain so far</span></span><br><span class="line">            bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature                      <span class="comment">#returns an integer</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">    <span class="comment">#计算数据原始香农熵</span></span><br><span class="line"><span class="comment"># numFeatures</span></span><br><span class="line">baseEntropy = calcShannonEnt(myDat)</span><br><span class="line">print(<span class="string">"numFeatures=%d"</span> %numFeatures) </span><br><span class="line">print(<span class="string">"原始熵是："</span>,baseEntropy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):        <span class="comment">#iterate over all the features</span></span><br><span class="line">        <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(<span class="string">"第%d个特征的所有取值"</span> %i,featList)</span><br><span class="line">    </span><br><span class="line">    uniqueVals = set(featList) </span><br><span class="line">    <span class="comment">#初始化新熵为0#get a set of unique values</span></span><br><span class="line">    newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">    print(<span class="string">"-简化取值:"</span>,uniqueVals)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">        subDataSet = splitDataSet(myDat, i, value)</span><br><span class="line">        print(<span class="string">"--按照%d划分取值"</span>% value,subDataSet)</span><br><span class="line">            <span class="comment">#计算概率</span></span><br><span class="line">        prob = len(subDataSet)/float(len(myDat))</span><br><span class="line">            <span class="comment">#计算新熵</span></span><br><span class="line">        print(<span class="string">"---去此值的概率是："</span>,prob)</span><br><span class="line">        newEntropy += prob * calcShannonEnt(subDataSet)   </span><br><span class="line">            <span class="comment">#计算信息增益</span></span><br><span class="line">        print(<span class="string">"---新熵是"</span>,newEntropy)</span><br><span class="line">    infoGain = baseEntropy - newEntropy     <span class="comment">#calculate the info gain; ie reduction in entropy</span></span><br><span class="line">    print(<span class="string">"-----信息增益"</span>,infoGain)</span><br><span class="line">        <span class="comment">#得到最大的信息增益和选取特征</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (infoGain &gt; bestInfoGain):       <span class="comment">#compare this to the best gain so far</span></span><br><span class="line">        bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">        bestFeature = i</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"此时最好的熵是"</span>,bestInfoGain,<span class="string">"此时最佳特征值是"</span>,bestFeature)</span><br></pre></td></tr></table></figure>
<pre><code>numFeatures=2
原始熵是： 0.9709505944546686
第0个特征的所有取值 [1, 1, 1, 0, 0]
-简化取值: {0, 1}
--按照0划分取值 [[1, &#39;no&#39;], [1, &#39;no&#39;]]
---去此值的概率是： 0.4
---新熵是 0.0
--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]
---去此值的概率是： 0.6
---新熵是 0.5509775004326937
-----信息增益 0.4199730940219749
此时最好的熵是 0.4199730940219749 此时最佳特征值是 0
第1个特征的所有取值 [1, 1, 0, 1, 1]
-简化取值: {0, 1}
--按照0划分取值 [[1, &#39;no&#39;]]
---去此值的概率是： 0.2
---新熵是 0.0
--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;], [0, &#39;no&#39;]]
---去此值的概率是： 0.8
---新熵是 0.8
-----信息增益 0.17095059445466854
此时最好的熵是 0.4199730940219749 此时最佳特征值是 0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chooseBestFeatureToSplit(myDat)</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><p><strong>从数据集构造决策树算法：其工作原理如下：</strong></p>
<ol>
<li>得到原始数据集  </li>
<li>基于最好的属性值划分数据集（可能存在大于两个分支的数据集划分）    </li>
<li>第一次划分后，数据被向下传递到树分支的下一个节点（可以用递归的思想）</li>
</ol>
<p><strong>递归的条件： </strong><br>程序遍历完所有划分数据集属性，或者每个分支下的所有实例都具有相同的分支。</p>
<p><strong>4.多数表决器</strong></p>
<p>程序清单：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多数表决器</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    </span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="comment">#for程序用来计数</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): </span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#排序函数</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><strong>5.创建决策树</strong>  </p>
<p>程序清单：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建树</span></span><br><span class="line"><span class="comment">#     myDat  = [[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no']]</span></span><br><span class="line"><span class="comment">#     labels = ['no surfacing','flippers']</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#classLsit里面是dataSet里面的标签</span></span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList): <span class="comment">#第一个终止条件：所有类标签都相同，country（）函数用来计数0</span></span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]<span class="comment">#stop splitting when all of the classes are equal</span></span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: <span class="comment">#第二个终止条件：用完了所有的特征#stop splitting when there are no more features in dataSet</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#得到标签里的所有属性值</span></span><br><span class="line">    uniqueVals = set(featValues)<span class="comment">#得到属性值集合</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]       <span class="comment">#copy all of labels, so trees don't mess up existing labels</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}
</code></pre><p><strong>6.使用决策树进行分类</strong></p>
<p>程序清单：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用决策树分类函数</span></span><br><span class="line"><span class="comment">#三个参数意义：input：决策树；featLabels：特征标签；testVec：测试向量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>
<p><strong>7.决策树在磁盘中的存储与导入</strong>  </p>
<p>程序清单：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#将磁盘中的对象加载出来，这里filename就是上面函数中的txt文件</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br><span class="line"><span class="comment"># storeTree(myTree,'classifierStorage.txt')</span></span><br><span class="line"><span class="comment"># grabTree('classifierStorage.txt')</span></span><br></pre></td></tr></table></figure>
<pre><code>---------------------------------------------------------------------------

IndexError                                Traceback (most recent call last)

&lt;ipython-input-16-33c9af9c39fa&gt; in &lt;module&gt;()
----&gt; 1 createTree(myDat,labels)
      2 # storeTree(myTree,&#39;classifierStorage.txt&#39;)
      3 # grabTree(&#39;classifierStorage.txt&#39;)


&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)
     21     for value in uniqueVals:
     22         subLabels = labels[:]       #copy all of labels, so trees don&#39;t mess up existing labels
---&gt; 23         myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
     24     return myTree


&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)
     14         return majorityCnt(classList)
     15     bestFeat = chooseBestFeatureToSplit(dataSet)
---&gt; 16     bestFeatLabel = labels[bestFeat]
     17     myTree = {bestFeatLabel:{}}
     18     del(labels[bestFeat])


IndexError: list index out of range
</code></pre><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol>
<li>CART决策树</li>
<li>决策树的剪枝技术</li>
<li>Bagging与随机森林</li>
<li>决策树中缺失值的处理</li>
<li>决策树代码</li>
</ol>
<p>注：本节代码对应第九章“树回归”</p>
<h2 id="CART决策树"><a href="#CART决策树" class="headerlink" title="CART决策树"></a>CART决策树</h2><p>为什么同样作为建立决策树的三种算法之一，我们要将CART算法单独拿出来讲。</p>
<p>因为ID3算法和C4.5算法采用了较为复杂的熵来度量，所以它们只能处理分类问题。而CART算法既能处理分类问题，又能处理回归问题。</p>
<p>对于分类树，CART采用基尼指数最小化准则；对于回归树，CART采用平方误差最小化准则</p>
<h3 id="1-CART分类树"><a href="#1-CART分类树" class="headerlink" title="1.CART分类树"></a>1.CART分类树</h3><p>CART分类树与上一节讲述的ID3算法和C4.5算法在原理部分差别不大，唯一的区别在于划分属性的原则。CART选择“基尼指数”作为划分属性的选择。</p>
<p>Gini指数作为一种做特征选择的方式，其表征了特征的不纯度。</p>
<p>在具体的分类问题中，对于数据集D，我们假设有K个类别，每个类别出现的概率为$P_k$，则数据集$D$的基尼指数的表达式为：</p>
<script type="math/tex; mode=display">
Gini=1-\sum_{k=1}^{K}{P_k}^2</script><p>我们取一个极端情况，如果数据集合中的类别只有一类，那么：</p>
<script type="math/tex; mode=display">
Gini(D)=0</script><p>我们发现，当只有一类时，数据的不纯度是最低的，所以Gini指数等于零。Gini(D)越小，则数据集D的纯度越高。</p>
<p>特别地，对于样本D，如果我们选择特征A的某个值a，把D分成$D_1$和$D_2$两部分，则此时，Gini指数为：  </p>
<script type="math/tex; mode=display">
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>与信息增益类似，我们可以计算如下表达式：  </p>
<script type="math/tex; mode=display">
\Delta{Gini(A)}=Gini(D)-Gini(D,A)</script><p>即以特征A划分后，数据不纯度减少的程度。显然，我们在做特征选取时，应该选择最大的一个。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat,labels</span><br></pre></td></tr></table></figure>
<pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]],
 [&#39;no surfacing&#39;, &#39;flippers&#39;])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">currentLabel</span><br></pre></td></tr></table></figure>
<pre><code>&#39;no&#39;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">labelCounts = &#123;&#125;<span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]<span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">    <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">        labelCounts[currentLabel] = <span class="number">0</span><span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">    labelCounts[currentLabel] += <span class="number">1</span><span class="comment">#标签为currentLabel的个数加1</span></span><br><span class="line">labelCounts</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;yes&#39;: 2, &#39;no&#39;: 3}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(featList)</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1, 1, 0, 0]
[1, 1, 0, 1, 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1, &#39;yes&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">numFeatures</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
<pre><code>0
1
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">classList</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;yes&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;no&#39;, &#39;no&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bestFeat = chooseBestFeatureToSplit(myDat)</span><br><span class="line">bestFeatLabel = labels[bestFeat]</span><br><span class="line">myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">myTree</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;no surfacing&#39;: {}}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">featValues</span><br></pre></td></tr></table></figure>
<pre><code>[1, 1, 1, 0, 0]
</code></pre><p>至此，我们完成了决策树算法原理和主要代码的学习。</p>
<p>下一节我们将学习CART算法、随机森林算法以及剪枝技术。</p>
<p>以上原理部分主要来自于《机器学习》—周志华，《统计学习方法》—李航，《机器学习实战》—Peter Harrington。代码部分主要来自于《机器学习实战》，我对代码进行了版本的改进，文中代码用Python3实现，这是机器学习主流语言，本人也会尽力对代码做出较为详尽的注释。</p>
<h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p><strong>熵</strong>： 熵（entropy）指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p>
<p><strong>信息熵（香农熵）</strong>： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p>
<p>信息增益： 在划分数据集前后信息发生的变化称为信息增益。</p>
<h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br>我们使用 createBranch() 方法，如下所示：</p>
<blockquote>
<p>检测数据集中的所有数据的分类标签是否相同:<br>         If so return 类标签<br>            Else:<br>                寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）<br>                划分数据集<br>                创建分支节点<br>                        for 每个划分的子集<br>                                调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中<br>                return 分支节点    </p>
</blockquote>
<p>​        </p>
<h4 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h4><blockquote>
<p>收集数据：可以使用任何方法。<br>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>训练算法：构造树的数据结构。<br>测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充）<br>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。     </p>
</blockquote>
<p><strong>决策树 算法特点</strong></p>
<blockquote>
<p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题。<br>适用数据类型：数值型和标称型。</p>
</blockquote>
<h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>1.算法简介</strong></p>
<p>决策树算法是一类常见的分类和回归算法，顾名思义，决策树是基于树的结构来进行决策的。</p>
<p>以二分类为例，我们希望从给定训练集中学得一个模型来对新的样例进行分类。</p>
<p><strong>举个例子</strong></p>
<p>有一个划分是不是鸟类的数据集合，如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>是否会飞</th>
<th>是否会跑</th>
<th>属于鸟类</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>是</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>2</td>
<td>是</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>3</td>
<td>是</td>
<td>否</td>
<td>否</td>
</tr>
<tr>
<td>4</td>
<td>否</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>5</td>
<td>否</td>
<td>是</td>
<td>否</td>
</tr>
</tbody>
</table>
</div>
<p>这时候我们建立这样一个决策树：  </p>
<p><img src="https://pic4.zhimg.com/80/v2-4a601bdc74abb553c0873fbd61597035_hd.jpg" ,width="400,height=400"></p>
<p>当我们有了一组新的数据时，我们就可以根据这个决策树判断出是不是鸟类。创建决策树的伪代码如下：  </p>
<p><img src="https://pic4.zhimg.com/80/v2-c226901dc50538bd40410e7aae938f47_hd.jpg" ,width="400,eight=400"></p>
<p>生成决策树是一个递归的过程，在决策树算法中，当出现下列三种情况时，导致递归返回： </p>
<p>(1)当前节点包含的样本属于同一种类，无需划分；</p>
<p>(2)当前属性集合为空，或者所有样本在所有属性上取值相同，无法划分；</p>
<p>(3)当前节点包含的样本集合为空，无法划分。</p>
<p><strong>2.属性选择</strong></p>
<p>在决策树算法中，最重要的就是划分属性的选择，即我们选择哪一个属性来进行划分。三种划分属性的主要算法是：ID3、C4.5以及CART。</p>
<p><strong>2.1 ID3算法</strong></p>
<p>ID3算法所采用的度量标准就是我们前面所提到的“信息增益”。当属性a的信息增益最大时，则意味着用a属性划分，其所获得的“纯度”提升最大。我们所要做的，就是找到信息增益最大的属性。由于前面已经强调了信息增益的概念，这里不再赘述。</p>
<p><strong>2.2 C4.5算法</strong></p>
<p>实际上，信息增益准则对于可取值数目较多的属性会有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：  </p>
<script type="math/tex; mode=display">
Gain\_ratio(Y,X)=\frac{Gain(Y,X)}{H(X)}</script><p>其中，分子为信息增益，分母为属性$X$的熵。</p>
<p>需要注意的是，增益率准则对可取值数目较少的属性有所偏好。</p>
<p>所以一般这样选取划分属性：<strong>先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</strong>。</p>
<p><strong>2.3 CART算法</strong></p>
<p>ID3算法和C4.5算法主要存在三个问题：</p>
<p>(1)每次选取最佳特征来分割数据，并按照该特征的所有取值来进行划分。也就是说，如果一个特征有4种取值，那么数据就将被切成4份，一旦特征被切分后，该特征就不会再起作用，有观点认为这种切分方式过于迅速。</p>
<p>(2)它们不能处理连续型特征。只有事先将连续型特征转换为离散型，才能在上述算法中使用。</p>
<p>(3)会产生过拟合问题。</p>
<p>为了解决上述(1)、(2)问题，产生了CART算法，它主要的衡量指标是基尼系数。为了解决问题(3)，主要采用剪枝技术和随机森林算法，这部分内容，下一次再详细讲述。</p>
<p>上述就是决策树算法的原理部分，下面展示完整代码和注释。代码中主要采用的是ID3算法。</p>

                <hr>
                

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2018/02/16/七堂极简物理课/" data-toggle="tooltip" data-placement="top"
                           title="《七堂极简物理课》">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2018/01/27/01机器学习实战-第1章 机器学习基础/" data-toggle="tooltip" data-placement="top"
                           title="01机器学习实战-机器学习基础">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                

                


                <!--加入新的评论系统-->
                

                

            </div>

            <div class="hidden-xs col-sm-3 toc-col">
                <div class="toc-wrap">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#第3章-决策树"><span class="toc-text">第3章 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#本章内容"><span class="toc-text">本章内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树的构造"><span class="toc-text">决策树的构造</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树的一般流程"><span class="toc-text">决策树的一般流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息论"><span class="toc-text">信息论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-信息熵"><span class="toc-text">1.信息熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-条件熵"><span class="toc-text">2.条件熵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-信息增益"><span class="toc-text">3.信息增益</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码实现与解读"><span class="toc-text">代码实现与解读</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#摘要"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART决策树"><span class="toc-text">CART决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-CART分类树"><span class="toc-text">1.CART分类树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树-原理"><span class="toc-text">决策树 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树-须知概念"><span class="toc-text">决策树 须知概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#信息熵-amp-信息增益"><span class="toc-text">信息熵 &amp; 信息增益</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树-工作原理"><span class="toc-text">决策树 工作原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#决策树-开发流程"><span class="toc-text">决策树 开发流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树算法"><span class="toc-text">决策树算法</span></a></li></ol></li></ol>
                </div>
            </div>
        </div>

        <div class="row">
            <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5 class="text-center">
                        <a href="/tags/">FEATURED TAGS</a>
                    </h5>
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#机器学习"
                           title="机器学习">机器学习</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
            </div>
        </div>

    </div>
</article>







<!-- Footer -->
<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center">
                <br>
                <ul class="list-inline text-center">
                
                
                
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/matrixgardener">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                

                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/matrixgardener">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Matrix 2018
                    <br>
                    <span id="busuanzi_container_site_pv" style="font-size: 12px;">PV: <span id="busuanzi_value_site_pv"></span> Times</span>
                    <br>
                    Theme by <a href="https://haojen.github.io/">Haojen Ma</a>
                </p>

            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/blog.js"></script>

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://matrixgardener.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Google Analytics -->



<!-- Baidu Tongji -->


<!-- swiftype -->
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','','2.0.0');
</script>

<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- 动态背景 -->
<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

<!--wechat title img-->
<img class="wechat-title-img" src="/image/img11.png">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":2,"width":120,"height":240,"position":"right","hOffset":-20,"vOffset":-40},"mobile":{"show":true,"scale":0.5,"motion":true},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>
