<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>欢迎来到我的博客</title>
      <link href="/2018/07/30/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/07/30/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>这里写摘要。test<br><a id="more"></a></p><p>这是我的第一篇博文，测试用。</p>]]></content>
      
      <categories>
          
          <category> Sports </category>
          
          <category> Baseball </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 主题测试 </tag>
            
            <tag> Injury </tag>
            
            <tag> Fight </tag>
            
            <tag> Shocking </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/07/30/hello-world/"/>
      <url>/2018/07/30/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>哲学家都干了什么</title>
      <link href="/2018/03/30/%E3%80%8A%E5%93%B2%E5%AD%A6%E5%AE%B6%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%E3%80%8B/"/>
      <url>/2018/03/30/%E3%80%8A%E5%93%B2%E5%AD%A6%E5%AE%B6%E9%83%BD%E5%B9%B2%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%E3%80%8B/</url>
      <content type="html"><![CDATA[<p>【读书笔记】：哲学家们都干了些什么？ </p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">前言</span><br><span class="line"></span><br><span class="line">上篇 理性的崛起</span><br><span class="line"></span><br><span class="line">第一章哲学好讨厌</span><br><span class="line">第二章少年国王</span><br><span class="line">第三章使徒行传</span><br><span class="line">第四章上帝之城</span><br><span class="line">第五章异教徒</span><br><span class="line">第六章神们自己</span><br><span class="line">第七章群魔</span><br><span class="line">第八章异端的权利</span><br><span class="line">第九章奇怪的论调</span><br><span class="line">第十章童年的终结</span><br><span class="line">第十一章理性主义</span><br><span class="line">第十二章形而上学</span><br><span class="line">第十三章二元论</span><br><span class="line">第十四章唯我论</span><br><span class="line">第十五章寒冬夜行人</span><br><span class="line">第十六章双星</span><br><span class="line">第十七章名利场</span><br><span class="line">第十八章巨匠与杰作</span><br><span class="line">第十九章机械论</span><br><span class="line">第二十章决定论</span><br><span class="line">第二十一章暴风雨</span><br><span class="line">第二十二章哲学怪兽</span><br><span class="line">第二十三章谎言的衰落</span><br><span class="line">第二十四章远离尘嚣</span><br><span class="line">第二十五章王者之风</span><br><span class="line"></span><br><span class="line">下篇 理性的陨落</span><br><span class="line"></span><br><span class="line">第一章傲慢与偏见</span><br><span class="line">第二章悲观主义</span><br><span class="line">第三章理性的危机</span><br><span class="line">第四章瞧！这个人</span><br><span class="line">第五章钢铁之躯</span><br><span class="line">第六章被侮辱的与被损害的</span><br><span class="line">第七章科学新发现：理性的反击</span><br><span class="line">第八章人猿星球</span><br><span class="line">第九章科学倒打一耙</span><br><span class="line">第十章寻欢作乐</span><br><span class="line">第十一章快乐王子</span><br><span class="line">第十二章逻辑实证主义</span><br><span class="line">第十三章实用主义</span><br><span class="line">第十四章终结形而上学</span><br><span class="line">第十五章实用主义的科学</span><br><span class="line">第十六章科学是什么</span><br><span class="line">第十七章永恒的终结</span><br><span class="line">第十八章存在与虚无</span><br><span class="line">第十九章西西弗的神话</span><br><span class="line">第二十章人生的意义</span><br></pre></td></tr></table></figure><h1 id="上篇：理性的崛起"><a href="#上篇：理性的崛起" class="headerlink" title="上篇：理性的崛起"></a>上篇：理性的崛起</h1><p>苏格拉底$\to$柏拉图$\to$亚里士多德$\to$亚里士多德$\to$亚历山大</p><p><strong>犹太教和基督教的关系</strong></p><p>犹太人中诞生犹太教，基督教是从犹太教发展来的。都信奉上帝，都相信会有救世主来拯救他们（“基督”和“弥赛亚”是一个词，都是救世主的意思）。</p><div class="table-container"><table><thead><tr><th style="text-align:left">犹太教</th><th style="text-align:left">基督教</th></tr></thead><tbody><tr><td style="text-align:left">犹太人产生</td><td style="text-align:left">从犹太教产生</td></tr><tr><td style="text-align:left">不承认耶稣是救世主，认为救世主还没来</td><td style="text-align:left">认为救世主就是耶稣</td></tr><tr><td style="text-align:left">信奉《旧约》</td><td style="text-align:left">信奉《旧约》《新约》：区别是记录耶稣降生之前和之后的事情</td></tr></tbody></table></div><p>特殊人物：保罗</p><pre><code>拥有罗马公民身份。早年保罗是犹太教徒，积极迫害基督徒。《使徒行传》记载，保罗在追捕耶稣门徒的路上突见耶稣显灵，从这天起，保罗从迫害者转变成虔诚的基督徒。他的皈依对基督教即为重要。他做的最重要的事是：向犹太人以外的民族传播基督教。他的武器就是希腊哲学。他撰写大量神学文章称作《保罗新书》，后来成为《新约》的重要组成部分。</code></pre><p>——此时哲学称为<strong>教父哲学</strong>。</p><p><strong>经院哲学</strong>——集大成者托马斯·阿奎纳</p><p>马丁·路德</p><p>罗马一方称为天主教，路德一方称为新教，东边罗马帝国还有个东正教。</p><p><strong>笛卡尔——我思故我在</strong></p><pre><code>只要有了怀疑的念头，就说明“我”肯定是存在的-“我”要是不存在就不会有这些年念头了。“我思”和“我在”不是因果关系，而是推理样衣关系，即：从前者为真可以推导出后者为真。也就是从“我思”为真，可以推导出“我在”为真。而不是说“我不思”的时候就“我不在”了，在不在我们不知道。</code></pre><p><strong>形而上学</strong></p><p>追问这个世界的本质：世界的本质是物质的还是精神的？解释的本质是物理定律还是我们对物理定律的信念？</p><p>回答这些问题都是形而上学的问题：</p><p>“世界本质是什么”的问题：在哲学里称作“本体论”。</p><p>“哪些知识是真实可信”的问题：在哲学里又称作“认识论”。</p><p><strong>二元论</strong>：心灵一个元，外界一个元。这两个元是互相独立的、平等的，虽然可以互相影响，但谁也不能完全决定另外一个。</p><p>从二元论的角度说，他人对我们的评价和我们的精神世界无关，我们可以完全忽视。但是对于我们在乎的人，这点事极难做到的。一旦做到了，我们也就成了完全不关心任何人的冷血动物。<br>实际上，当我们在乎外人的感受时，就相当于我们把自己的喜怒哀乐寄托于外物，我们即不可能控制一切外物，也不可能让他人的感受总符合我们的意愿。因此不仅仅是二元论，其他自我安慰的手段对于我们说关心的人都有些束手无策。</p><p>反对二元论：唯物主义：说世界的本质是物质的，我们精神世界不过是大脑生理活动的结果。换句话说，精神是从物质中产生的。这种观点叫做物质一元论。</p><p>同时有唯心主义一元论，认为世界的本质是精神的，外面世界不过是自己心灵的产物。</p><p>从二元论进一步得到唯我论。：假设我们只停留在“我在”的阶段，我们只能确认我自己的存在，外界一切存在不存在我不知道，这叫做“唯我论”。</p><p>“唯我论”还可以和目的论结合在一起。：就是认为世间万物是因为某种目的而存在的。</p><p>但是唯我论和目的论能赋予人生一种特殊的美。能给与我们一个理解人生的全新视角。</p><p><strong>斯宾诺莎</strong>：他是笛卡尔的继承者。：按照欧式几何学的模式来建立哲学体系：具体来说，就是先找出一些不言自明的公社，再以这些公社为基础，按照演绎推理的方法建立整个哲学体系。</p><p>他的观点：<strong>实体</strong>的特征是自己就是机制存在的原因，不依赖外物存在。意味着，外物也不可能摧毁实体。推出实体肯定是永远存在的。实体是无限的、是唯一的、不可分的、是善的。</p><p>科学靠归纳法搞研究。事实上，我们今天取得的所有科学成就，都是综合使用归纳法和演绎推理的结果。</p><p>科学派哲学家：洛克：“儿童心灵是白纸”也承认人的本能是天生的。</p><p>笛卡尔、斯宾诺莎代表的数学派，被称为“理性主义”。</p><p>洛克代表的科学家被称为“经验主义”。</p><div class="table-container"><table><thead><tr><th style="text-align:left">理论名称</th><th>理性主义</th><th style="text-align:left">经验主义</th></tr></thead><tbody><tr><td style="text-align:left">代表人物</td><td>数学派哲学家</td><td style="text-align:left">科学派哲学家</td></tr><tr><td style="text-align:left">研究方法</td><td>演绎法</td><td style="text-align:left">归纳法</td></tr><tr><td style="text-align:left">优点</td><td>严谨</td><td style="text-align:left">产生新知识</td></tr><tr><td style="text-align:left">缺点</td><td>不产生新知识，公社未必可靠</td><td style="text-align:left">结论不能保证绝对正确，永远有出错的可能</td></tr></tbody></table></div><p>用一个比喻来描述两个学派的特点：</p><ul><li><p>假如哲学是一座通向终极定理的巴别塔的话，那么理性主义者的塔高耸入云，每搭建一次，都似乎马上可以触摸到天堂。但是这座塔的根基却是几根木头，经验主义者们经常来溜达，随便踹上几脚，这座塔就塌了。</p></li><li><p>经验主义不同，他们的塔盖得极为结实。但是由于能力有限，他们只能零零散散地在各地建造一些矮塔，这些塔既连不到一块，又没法盖得很高。因此经验主义者们的塔虽然结实，却根本没法满足人类的要求，盖得再多也没有用。</p></li></ul>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
          <category> Philosophy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Essay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《七堂极简物理课》</title>
      <link href="/2018/02/16/%E4%B8%83%E5%A0%82%E6%9E%81%E7%AE%80%E7%89%A9%E7%90%86%E8%AF%BE/"/>
      <url>/2018/02/16/%E4%B8%83%E5%A0%82%E6%9E%81%E7%AE%80%E7%89%A9%E7%90%86%E8%AF%BE/</url>
      <content type="html"><![CDATA[<p>《七堂极简物理课》</p><p>第一课：最美的理论</p><p>爱因斯坦 广义相对论 </p><p>“牛顿试图解释物体下落和行星运转的原因。他假设在万物之间存在一种相互吸引的“力量”，他称之为“引力”。那么这个力是如何牵引两个相距甚远，中间又空无一物的物体的呢？这位伟大的现代科学之父对此显得谨慎小心，未敢大胆提出假设。牛顿想象物体是在空间中运动的，他认为空间是一个巨大的空容器，一个能装下宇宙的大盒子，也是一个硕大无朋的框架，所有物体都在其中做直线运动，直到有一个力使它们的轨道发生弯曲。至于“空间”，或者说牛顿想象的这个可以容纳世界的容器是由什么做成的，牛顿也没有给出答案。就在爱因斯坦出生前的几年，英国的两位大物理学家——法拉第（ Michael Faraday）和麦克斯韦（ James Maxwell）——为牛顿冰冷的世界添加了新鲜的内容：电磁场。所谓“电磁场”，是一种无处不在的真实存在，它可以传递无线电波，可以布满整个空间；它可以振动，也可以波动，就像起伏的湖面一样；它还可以将电力“四处传播”。爱因斯坦……很快他想到，就像电力一样，引力一定也是由一种场来传播的，一定存在一种类似于“电场”的“引力场”。他想弄明白这个“引力场”是如何运作的，以及怎样用方程对其进行描述。就在这时，他灵光一闪，想到了一个非同凡响的点子，一个百分百天才的想法：引力场不“弥漫”于空间，因为它本身就是空间。这就是广义相对论的思想。其实，牛顿的那个承载物体运动的“空间”与“引力场”是同一个东西。这是一个惊世骇俗的理论，对宇宙做了惊人的简化：空间不再是一种有别于物质的东西，而是构成世界的“物质”成分之一，一种可以波动、弯曲、变形的实体。我们不再身处一个看不见的坚硬框架里，而更像是深陷在一个巨大的容易形变的软体动物中。太阳会使其周围的空间发生弯曲，所以地球并不是在某种神秘力量的牵引下绕着太阳旋转，而是在一个倾斜的空间中行进，就好像弹珠在漏斗中滚动一样：漏斗中心并不会产生什么神秘的“力量”，是弯曲的漏斗壁使弹珠滚动的。所以无论是行星绕着太阳转，还是物体下落，都是因为空间发生了弯曲。那么我们该如何描述这种空间的弯曲呢？ 19世纪最伟大的数学家、“数学王子”卡尔·弗里德里希·高斯（ Carl Friedrich Gauss）已经写出了描述二维曲面（比如小山丘的表面）的公式。他还让自己的得意门生将这一理论推广到三维乃至更高维的曲面。这位学生就是波恩哈德·黎曼（ Bernhard Riemann），他就此问题写了一篇重量级的博士论文，但当时看起来全然无用。黎曼论文的结论是，任何一个弯曲空间的特征都可以用一个数学量来描述，如今我们称之为“黎曼曲率”，用大写的“ R”来表示。后来爱因斯坦也写了一个方程，将这个 R与物质的能量等价起来，也就是说：空间在有物质的地方会发生弯曲。就这么简单。这个方程只有半行的长度，仅此而已。空间弯曲这个观点，现在变成了一个方程。然而，这个方程中却蕴含着一个光彩夺目的宇宙。首先，这个方程描述了空间如何在恒星周围发生弯曲。由于这个弯曲，不仅行星要在轨道上绕着恒星转，就连光也发生了偏折，不再走直线。爱因斯坦预测，太阳会使光线偏折。在 1919年，这个偏折被测量出来，从而证实了他的这一预测。其实不仅是空间，时间也同样会发生弯曲。爱因斯坦曾预言，在高空中，在离太阳更近的地方，时间会过得比较快，而在低的地方，离地球近的地方时间则过得比较慢。这一预测后来也经测量得到了证实。</p><p>当一个大恒星燃烧完自己所有的燃料（氢）时，它就会熄灭。残留的部分因为没有燃烧产生的热量的支撑，会因为自身的重量而坍塌，导致空间强烈弯曲，最终塌陷成一个真真正正的洞。这就是著名的“黑洞”。……整个宇宙空间可以膨胀和收缩。爱因斯坦的方程还指出，空间不可能一直保持静止，它一定是在不断膨胀的。……这个方程还预测，这个膨胀是由一个极小、极热的年轻宇宙的爆炸引发的：这就是我们所说的“宇宙大爆炸”。……但大量证据纷纷出现在我们眼前，直至在太空中观测到了“宇宙背景辐射”，也就是原始爆炸的余热里弥漫的光。事实证明，爱因斯坦方程的预言是正确的。此外，这个理论还说，空间会像海平面一样起伏，目前人们已经在宇宙中的双星上观测到了“引力波”的这种效应，与爱因斯坦理论的预言惊人一致，精确到了千亿分之一。……所有这一切都源自一个朴素的直觉，那就是，空间和引力场本是一回事。这一切也可以归结为一个简洁的方程，……”</p><p>第二课 量子</p><p>量子力学诞生于1900年，……德国物理学家马克思·普朗克（Max Planck）计算了一个“热匣子”内处于平衡态的电磁场。为此他用了一个巧妙的办法：假设电磁场的能量都分布在一个个“量子”上，也就是说能量是一包一包或一块一块的。用这个方法算出的结果与测量得到的数据完全吻合（所以应该算是正确的），但却与当时人们的认知背道而驰，因为人们认为能量是连续变动的，硬把它说是由一堆“碎砖块”构成的，简直是无稽之谈。</p><p>对于普朗克来说，把能量视为一个个能量包块的集合只是计算上使用的一个特殊策略，就连他自己也不明白为什么这种方法会奏效。然而五年后，有事爱因斯坦，终于认识到这些“能量包”是真是存在的。</p><p>爱因斯坦指出光是由成包的光粒子构成的，今天我们称之为“光子”。他在那篇文章的引言中写道：“在我看来，如果我们假设光的能量在空间中的分布是不连续的，我们就能更好地理解有关黑体辐射、荧光、紫外线产生的阴极射线，以及有关其他有关光的发射和转化的现象。依据这个假设，点光源发射出的一束光线的能量，并不会在越来越广的空间中连续分布，而是由有限数目的‘能量量子’组成，它们在空间中点状分布，作为能量发射和吸收的最小单元，能量量子不可再分。”</p><p>20世纪10-20年代，丹麦人尼尔斯·波尔（Dane Niels Bohr）引领了这一理论的发展，他了解到原子核内电子的能量跟光能一样，只是特定值，而更重要的是，电子只有在特定的能量之下才能从一个原子轨道“跳跃”到另一个原子轨道上，并同时释放或吸收一个光子，这就是著名的“量子跃迁”。 </p><p>1925年，量子理论的方程终于出现了，取代整个牛顿力学。</p><p>率先为这个新理论列出方程的是一个非常年轻的德国天才——维尔纳·海森堡（Werner Heisenberg），他所依据的理念简直让人晕头转向。</p><p>海森堡想象电子并非一只存在，只有人看到它们的时，或者更确切的说，只有和其他东西相互作用时它们才会存在。当它们与其他东西相撞时，就会以一个可计算的概率在某个地方出现。从一个轨道到另一个轨道的“量子跃迁”是它们现身的唯一方式：一个电子就是相互作用下的一连串跳跃。如果么有受到打扰，电子就没有固定的栖身之所，它甚至不会存在于一个所谓的“地方”。</p><p>在量子力学中，没有一样东西拥有确定的位置，除非它撞上了别的东西。为了描述电子从一种相互作用到另一个相互作用的飞跃，就要借助一个抽象的公式，它只存在于抽象的数学空间，而不存在于真实空间。</p><p>更糟的是，这些从一处到另一处的飞跃大多是随机的，不可预测的。我们无法预知一个电子再次出现会是在哪儿，只能计算它出现在这里或那里的“概率”。这个概率问题直捣物理的核心，可原本物理学的一切问题都是被那些普遍且不可改变的铁律所控制的。</p><p>一个世纪过去了，我们还停在原点。量子力学的方程以及用它们得出的结果每天都被应用于物理、工程、化学、生物乃至更广阔的领域中。量子力学对于当代科技的整体发展有着至关重要的意义。没有量子力学就不会出现晶体管。然而这些方程仍然十分神秘，因为它们并不描述在一个物理系统内发生了什么，而只说明一个物理系统是如何影响另外一个物理系统的。</p><p>第三课 宇宙的构造</p><p>20世纪上半叶，爱因斯坦用相对论描述了空间和时间的运作方式，而波尔和他年轻的门徒们则用一系列方程捕捉到了物质奇怪的量子特性。20世纪下半叶，物理学家们在此基础上，把这两个新理论广泛应用在了自然界的各个领域：从宏观世界的宇宙构造，到微观世界的基本粒子。</p><p>哥白尼的日心说</p><p>太阳系只是不计其数的星系中的一个，而我们的太阳也只是众多恒星中普普通通的一颗，是浩瀚银河系星云中的沧海一粟。</p><p>但是在20世纪30年代，天文学家对星云（恒星之间近乎白色的云团）进行精确的测量后发现，银河系本身也只是众多星系间浩瀚星云中的一粒尘埃。这些星系一只蔓延到我们最强大的天文望远镜也看不到的地方。</p><p>这片均匀无边的宇宙并不像看上去那么简单。就像我在第一节课中解释过的那样，空间不是一马平川，而是弯曲的。宇宙布满了星系，所以我们想象它的纹理会像海浪一样起伏，激烈处还会产生黑洞空穴。</p><p>我们今天终于知道，这个布满星系、富有弹性的浩瀚宇宙是大约150亿年前由一个极热极密的小星云演化来的。</p><p>宇宙的诞生的时候就像一个小球，大爆炸后一直膨胀到它现在的规模。这就是我们现在对宇宙最大程度的了解了。</p><p>第四课 粒子</p><p>我们身边所有物体都是由电子、夸克、光子和胶子组成的。它们就是粒子物理学中所讲的“基本粒子”。除此之外还有几种粒子，例如中微子（neutrino）——它布满整个宇宙，但并不跟我们发生交互作用，还有希格斯玻色子（Higgs boson）——不久前日内瓦欧洲核子研究中心的大型强子对撞机发现的粒子。但这些粒子并不多，只有不到十种。这些少量的基本原料，如同大型乐高玩具中的小积木，靠它们建造出了我们身边的整个物质世界。</p><p>量子力学描述了这些粒子的性质和运动方式。这些粒子当然并不像小石子那般真实可感，而是相应的场的“量子”，比方说光子是电磁场的“量子”。就跟在法拉第和麦克斯韦的电磁场中一样，它们是这些变化的基底场的元激发，是极小的移动的波包。它们的消失和重现遵循量子力学的奇特定律：存在的每样东西都是不稳定的，永远都在从一种相互作用跃迁到另一种相互作用。</p><p>即使我们观察的是空间中一块没有原子的区域，还是可以探测到粒子的微小涌动。彻底的虚空是不存在，就像最平静的海面，我们凑近看还是会发现细微的波动和振荡。构成世界的各种场也会轻微的波动起伏，我们可以想象，组成世界的基本粒子在这样的波动中不断产生、消失。</p><p>这就是量子力学和粒子理论描述的世界。这同牛顿和拉普拉斯（Laplace）的世界相去甚远：在那里，冰冷的小石子在不变的集合空间里沿着精确而漫长的轨迹永恒不变地运动着。量子力学和粒子试验告诉我们，世界是物体连续、永不停歇的涌动，是稍纵即逝的实体不断出现和消失，是一系列的振荡，就像20世界60年代时髦的嬉皮时代，一个由事件而非物体构成的世界。</p><p>第五课 空间的颗粒</p><p>“圈量子引力”试图将广义相对论和量子力学统一起来。</p><p>它的中心思想很简单。广义相对论告诉我们空间不是一个静止的盒子，而是在不断运动，像一个移动中的巨大软体动物，可以压缩和扭曲，而我们被包在里面。另一方面，量子力学告诉我们，所有这样的场都“由量子构成”，具有精细的颗粒状结构。于是物理空间当然也是“由量子构成的”。</p><p>这正是圈量子引力的核心结论：空间是不连续的，不可被无限分割，而是由细小的颗粒，或者说“空间原子”构成。这些颗粒极其微小，比最小的原子核还要小几亿亿倍。圈量子引力用数学形式描述了这些“空间原子”，也给出了它们演化的方程。它们被称为“圈”或环，因为它们环环相扣，形成了一个相互关联的网络，从而编制出空间的纹理，就像细密织成的巨大锁子甲上的小铁圈一样。</p><p>第六课 概率、时间和黑洞的热</p><p>英国物理学家麦克斯韦和奥地利物理学家玻尔兹曼（Ludwig Boltzmann）发现了热的本质。</p><p>玻尔兹曼发现其中的原因惊人的简单：这完全是随机的。玻尔兹曼的解释非常精妙，用到了概率的概念。热量从热的物体跑到冷的物体并非遵循什么绝对的定律，只是这种情况发生的概率比较大而已。原因在于：从统计学的角度看，一个快速运动的热物体的原子更有可能撞上一个冷物体的原子，传递给它一部分能量；而相反过程发生的概率则很小。在碰撞的过程中能量是是守恒的，但当发生大量偶然碰撞时，能量倾向于平均分布。就这样，相互接触的物体温度趋向于相同。热的物体和冷的物体接触后温度不降反升的情况并非不可能，只是概率小的可怜罢了。</p><p>尾声 我们</p>]]></content>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>三体-星际穿越</title>
      <link href="/2018/02/02/%E4%B8%89%E4%BD%93-%E7%BB%B4%E5%BE%B7%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E7%94%B7%E4%B8%BB/"/>
      <url>/2018/02/02/%E4%B8%89%E4%BD%93-%E7%BB%B4%E5%BE%B7%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E7%94%B7%E4%B8%BB/</url>
      <content type="html"><![CDATA[<p>我是在敦煌的戈壁滩上，看完整个《三体》，</p><p>1月份敦煌的戈壁滩，气候极其干燥，没有一丝下雪的痕迹，室外阳光高照，但总是狂风大作，仿佛要狠狠的从戈壁滩上刮走什么，在这种时候仍需要每日进入戈壁腹地进行测试工作，</p><p>清晨赶着晨曦上车，左右颠簸中来到地图上也没标记的地点，从车上下车前，每人都会领到帽子护膝围脖等用品，那几日，往往赶上狂风大作，下车后每一步都不像是走路，而是与大风作对，风沙从你的衣领裤腿缝隙钻入，的工作听着风从车的缝隙中钻入，人在外面没有防护措施的呆不过十分钟，就会被大风带走身上大部分的热量，人在车外工作十分钟就会迫不及待的跳上车子，在这里，前前后后十几公里都不会有人存在，整个车子都沉浸在风沙中，你只会觉得仿佛在外星球，而你坐的车就是你的飞船，你只会觉得整个戈壁滩上，只有这个车子才是你唯一的依靠，出了车子，你绝对活不过数十分钟的。<br>就是在这种环境下，我看完了整个三体，<br>将车想做是飞船，将外面的戈壁滩想做的荒凉的沙漠，<br>而我们的保证仅仅是几厘米厚的钢板，</p><p>其中的故事情节，有太多的解读，这里写下我的感受最深的。<br>维德-为了目的不择手段，是为了人类的延续，真正的男人。不达目的誓不罢休，可惜他与书中的当权者，或者是外星人的选择无关，如果设想最后的执剑人是维德，恐怕最后的结局也会改写吧。<br>反观女主，优柔寡断，在关键时刻，没能把握机会，从一开始，三体人就布下所有的计划，等罗辑交出执剑人身份的时候，就发动进攻。</p><p>星际穿越，也是我看的感触比较深的电影，<br>一、是在男主骗女主要去新的星球，最后将机会给了女主，那句“物理学第二定律，总要留下些什么”<br>二、结尾，男主在恢复后，义无反顾的“偷来”宇航船，去奔向女主，不论是什么原因，爱情还是同甘共苦的情谊，感受到了人类的顽强生生不息，这种感觉不是简单的迸发，而是整个电影的铺垫，</p><p>《星际穿越》诺兰大神，拍出了不一样的意味，整个电影不像一部电影，而像一部纪录片，其中无数的人性冲突，理性的煎熬，诺兰德早就知道计算结果的局限，直到生命的最后一刻，才说出那句对不起，男主的女儿，苦苦思索，到电影最后，终于思索出，书架上的暗示，整个电影不喜不悲，或者说已经很艺术的处理物理环境的恶劣，人类无论是哲学上的处于“不自知”的状态，还是在整个宇宙环境下，都是脆弱的，人类距离外太空，只有几公里的大气层和地球的磁场的保护，没有的磁场保护，人类只能存活半小时，没有大气层，人类连十分钟都活不到，</p><p>在这部电影中，我看出了不一样的情节。“无论乐观悲观，只想完成任务。”电影简化了人性的选择，除了马特戴蒙的角色，无一不是按照使命完成任务，他们在物理条件面前，想到过自己么，理性会高于人性，这19/12位宇航员，都是这样的么。</p><p>最后的男主被未来人类拯救来到高维空间，这里有一个疑问，此时被拯救的男主改变了时间线，必将产生新的未来人类，旧的未来人类消失，根据“祖父悖论”，那也就不会去高维空间，那也就不会解决公式，那新的未来人类也不会产生，但时间线继续发展，还会产生旧的未来人类，旧的未来人类继续拯救男主，产生新的未来人类，旧的未来人类消失……以此类推，人类历史会不会形成闭环？<br>或者新的未来人类仍然拯救男主，来到高维空间……此时旧的未来人类和新的未来人类在时间线中做出了同样一件事情。那根据电影中的设定，他们必然通过重力进行沟通，（设定每次的选择都会产生新的时间线，新的宇宙，这也是平行宇宙的理解）</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>03机器学习实战-第3章 决策树</title>
      <link href="/2018/01/28/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC3%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2018/01/28/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC3%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><h1 id="第3章-决策树"><a href="#第3章-决策树" class="headerlink" title="第3章 决策树"></a>第3章 决策树</h1><h2 id="本章内容"><a href="#本章内容" class="headerlink" title="本章内容"></a>本章内容</h2><blockquote><ul><li>决策树简介  </li><li>在数据集中度量一致性  </li><li>使用递归构造决策树     </li><li>使用Matplotlib绘制树形图   </li></ul></blockquote><h2 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h2><blockquote><p>优点：计算复杂度不高，输出易于理解，对中间值得确实不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题<br>使用数据类型：数值型和标称型  </p></blockquote><p><strong>创建分支伪代码函数createBranch()如下：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">检测数据集中的每个指向是否属于同一个分类：</span><br><span class="line">    IF so return 类标签</span><br><span class="line">    Else</span><br><span class="line">        寻找划分数据集的最好特征</span><br><span class="line">        划分数据集</span><br><span class="line">        创建分支节点</span><br><span class="line">            for 每个划分的子集</span><br><span class="line">                调用函数createBranch并增加返回结果到分支节点中</span><br><span class="line">        return 分支节点</span><br></pre></td></tr></table></figure></p><p>上述是一个递归函数</p><h2 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h2><blockquote><p>(1) 收集数据：可以使用任何方法。<br>(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>(4) 训练算法：构造树的数据结构。<br>(5) 测试算法：使用经验树计算错误率。<br>(6) 使用算法：词步骤可以使用于任何监督学习算法，而使用决策树可能更好地理解数据的内在含义。  </p></blockquote><p><strong> 摘要</strong></p><ol><li>信息论相关知识</li><li>决策树算法原理</li><li>代码实现与解释   </li></ol><p>今天总结决策树算法，目前建立决策树有三种主要算法：ID3、C4.5以及CART。由于算法知识点比较琐碎，我分成两节来总结。</p><p>第一节主要是梳理决策树算法中ID3和C4.5的知识点；第二节主要梳理剪枝技术、CART算法和随机森林算法的知识。</p><h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><h3 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1.信息熵"></a>1.信息熵</h3><p>在决策树算法中，熵是一个非常非常重要的概念。</p><p>一件事发生的概率越小，我们说它所蕴含的信息量越大。</p><p>比如：我们听女人能怀孕不奇怪，如果某天听到哪个男人怀孕了，我们就会觉得emmm…信息量很大了。</p><p>所以我们这样衡量信息量：</p><script type="math/tex; mode=display">i(y)=-log{P(y)}</script><p>其中，$P(y)$是事件发生的概率。</p><p>信息熵就是所有可能发生事件的信息量的期望：  </p><script type="math/tex; mode=display">H(Y)=-\sum_{i=1}^{n}P(y_i)log{P(y_i)}</script><p>表达了$Y$事件发生的不确定度。  </p><h3 id="2-条件熵"><a href="#2-条件熵" class="headerlink" title="2.条件熵"></a>2.条件熵</h3><p>条件熵：表示在X给定条件下，$Y$的条件概率分布的熵对$X$的数学期望。其数学推导如下：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y|X) & =\sum_{x\in{X}}P{(x)}H(Y|X=x) \\   & =-\sum_{x\in{X}}P(x)\sum_{y\in{Y}}P(y|x)log{P(y|x)}\\   & =-\sum_{x\in{X}}\sum_{y\in{Y}}P(x,y)log{P(y|x)}\end{aligned}</script><p>条件熵$H（Y|X）$表示在已知随机变量$X$的条件下随机变量Y的不确定性。注意一下，条件熵中X也是一个变量，意思是在一个变量$X$的条件下（变量$X$的每个值都会取到），另一个变量$Y$的熵对$X$的期望。</p><p>举个例子</p><p>例：女生决定主不主动追一个男生的标准有两个：颜值和身高，如下表所示：</p><div class="table-container"><table><thead><tr><th></th><th>颜值</th><th>身高</th><th>追不追</th></tr></thead><tbody><tr><td>1</td><td>帅</td><td>高</td><td>追</td></tr><tr><td>2</td><td>帅</td><td>不高</td><td>追</td></tr><tr><td>3</td><td>不帅</td><td>高</td><td>不追</td></tr></tbody></table></div><p>上表中随机变量$Y=\{追，不追\}$，$P(Y=追)=2/3$，$P(Y=不追)=1/3$，得到$Y$的熵：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y) & =-\frac{2}{3}log\frac{2}{3}-\frac{1}{3}log\frac{1}{3} \\   & =0.918\end{aligned}</script><p>这里还有一个特征变量$X$，$X=｛高，不高｝$。当$X=高$时，追的个数为1，占1/2，不追的个数为1，占1/2，此时：</p><script type="math/tex; mode=display">H(Y|X=高)=-\frac{1}{2}log\frac{1}{2}-\frac{1}{2}log\frac{1}{2}</script><p>同理：</p><script type="math/tex; mode=display">H(Y|X=不高)=-{1}log{1}-{1}log{1}</script><p>（注意：我们一般约定，当$p=0$时，$plogp=0$）</p><p>所以我们得到条件熵的计算公式：  </p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y|X=身高) & =P(X=不高)*H(Y|X=不高)+P(X=高)*H(Y|X=高)\\            & =0.67\end{aligned}</script><h3 id="3-信息增益"><a href="#3-信息增益" class="headerlink" title="3.信息增益"></a>3.信息增益</h3><p>当我们用另一个变量$X$对原变量$Y$分类后，原变量$Y$的不确定性就会减小了（即熵值减小）。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：</p><script type="math/tex; mode=display">Gain(Y,X)=H(Y)-H(Y|X)</script><p>此外，信息论中还有互信息、交叉熵等概念，它们与本算法关系不大，这里不展开。 </p><h2 id="代码实现与解读"><a href="#代码实现与解读" class="headerlink" title="代码实现与解读"></a>代码实现与解读</h2><p><strong>1.计算给定数据的香浓熵 </strong>    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算给定数据集的香农熵</span></span><br><span class="line"><span class="comment">#从math中导入log函数</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)   <span class="comment">#计算实例中的个数</span></span><br><span class="line">    </span><br><span class="line">    labelCounts = &#123;&#125;    <span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:    <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">        </span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]    <span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():     <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">            </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>       <span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">          </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>     <span class="comment">#标签为currentLabel的个数加1       </span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries    <span class="comment">#计算每一个标签的概率p</span></span><br><span class="line">        </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)    <span class="comment">#log base 2利用公式计算香农熵</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="comment">#change to discrete values</span></span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line">myDat,labels=createDataSet()</span><br><span class="line">myDat,labels</span><br></pre></td></tr></table></figure><pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]], [&#39;no surfacing&#39;, &#39;flippers&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calcShannonEnt(myDat)</span><br></pre></td></tr></table></figure><pre><code>0.9709505944546686</code></pre><p><strong>2.创建选取的数据特征属性划分数据集</strong></p><p>程序清单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照给定特征划分数据集</span></span><br><span class="line"><span class="comment">#参数解释：dataSet待划分数据集</span></span><br><span class="line"><span class="comment">#axis：划分数据集的特征，这个函数里指函数第几列</span></span><br><span class="line"><span class="comment">#value：特征返回值，指的是特征划分的标准</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []     <span class="comment">#创建一个新列表</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:   <span class="comment">#如果这组数据特征值等于特征返回值的话</span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec = featVec[:axis]       <span class="comment">#这两行是把原来的数据除掉划分数据的特征那一列 </span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)   <span class="comment">#把列表reduceFeatVect放入retDataSet中</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">1</span>) </span><br><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line"><span class="comment"># 将myDat的第1列按照取出所有等于1的方式划分</span></span><br></pre></td></tr></table></figure><pre><code>[[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>[[1, &#39;no&#39;], [1, &#39;no&#39;]]</code></pre><p><strong>3.根据信息增益准则，选取最好的划分特征</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#找到最好的数据集划分方式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>    <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)   <span class="comment">#计算数据原始香农熵</span></span><br><span class="line">   </span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>   <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):       </span><br><span class="line">        </span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">        </span><br><span class="line">        uniqueVals = set(featList)    <span class="comment">#set()，生成一个集合数据类型，其和列表类型一样，不同之处在于</span></span><br><span class="line">                                      <span class="comment">#集合数据类型里面的值不重复，是唯一的</span></span><br><span class="line">        </span><br><span class="line">        newEntropy = <span class="number">0.0</span>    <span class="comment">#初始化新熵为0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:    <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">           </span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)    <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">            </span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))   <span class="comment">#计算概率</span></span><br><span class="line">            </span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)    <span class="comment">#计算新熵 </span></span><br><span class="line">       </span><br><span class="line">        infoGain = baseEntropy - newEntropy     <span class="comment">#计算信息增益calculate the info gain; ie reduction in entropy</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):      <span class="comment">#得到最大的信息增益和选取特征 #compare this to the best gain so far</span></span><br><span class="line">            bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature                      <span class="comment">#returns an integer</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">    <span class="comment">#计算数据原始香农熵</span></span><br><span class="line"><span class="comment"># numFeatures</span></span><br><span class="line">baseEntropy = calcShannonEnt(myDat)</span><br><span class="line">print(<span class="string">"numFeatures=%d"</span> %numFeatures) </span><br><span class="line">print(<span class="string">"原始熵是："</span>,baseEntropy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):        <span class="comment">#iterate over all the features</span></span><br><span class="line">        <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(<span class="string">"第%d个特征的所有取值"</span> %i,featList)</span><br><span class="line">    </span><br><span class="line">    uniqueVals = set(featList) </span><br><span class="line">    <span class="comment">#初始化新熵为0#get a set of unique values</span></span><br><span class="line">    newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">    print(<span class="string">"-简化取值:"</span>,uniqueVals)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">        subDataSet = splitDataSet(myDat, i, value)</span><br><span class="line">        print(<span class="string">"--按照%d划分取值"</span>% value,subDataSet)</span><br><span class="line">            <span class="comment">#计算概率</span></span><br><span class="line">        prob = len(subDataSet)/float(len(myDat))</span><br><span class="line">            <span class="comment">#计算新熵</span></span><br><span class="line">        print(<span class="string">"---去此值的概率是："</span>,prob)</span><br><span class="line">        newEntropy += prob * calcShannonEnt(subDataSet)   </span><br><span class="line">            <span class="comment">#计算信息增益</span></span><br><span class="line">        print(<span class="string">"---新熵是"</span>,newEntropy)</span><br><span class="line">    infoGain = baseEntropy - newEntropy     <span class="comment">#calculate the info gain; ie reduction in entropy</span></span><br><span class="line">    print(<span class="string">"-----信息增益"</span>,infoGain)</span><br><span class="line">        <span class="comment">#得到最大的信息增益和选取特征</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (infoGain &gt; bestInfoGain):       <span class="comment">#compare this to the best gain so far</span></span><br><span class="line">        bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">        bestFeature = i</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"此时最好的熵是"</span>,bestInfoGain,<span class="string">"此时最佳特征值是"</span>,bestFeature)</span><br></pre></td></tr></table></figure><pre><code>numFeatures=2原始熵是： 0.9709505944546686第0个特征的所有取值 [1, 1, 1, 0, 0]-简化取值: {0, 1}--按照0划分取值 [[1, &#39;no&#39;], [1, &#39;no&#39;]]---去此值的概率是： 0.4---新熵是 0.0--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]---去此值的概率是： 0.6---新熵是 0.5509775004326937-----信息增益 0.4199730940219749此时最好的熵是 0.4199730940219749 此时最佳特征值是 0第1个特征的所有取值 [1, 1, 0, 1, 1]-简化取值: {0, 1}--按照0划分取值 [[1, &#39;no&#39;]]---去此值的概率是： 0.2---新熵是 0.0--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;], [0, &#39;no&#39;]]---去此值的概率是： 0.8---新熵是 0.8-----信息增益 0.17095059445466854此时最好的熵是 0.4199730940219749 此时最佳特征值是 0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chooseBestFeatureToSplit(myDat)</span><br></pre></td></tr></table></figure><pre><code>0</code></pre><p><strong>从数据集构造决策树算法：其工作原理如下：</strong></p><ol><li>得到原始数据集  </li><li>基于最好的属性值划分数据集（可能存在大于两个分支的数据集划分）    </li><li>第一次划分后，数据被向下传递到树分支的下一个节点（可以用递归的思想）</li></ol><p><strong>递归的条件： </strong><br>程序遍历完所有划分数据集属性，或者每个分支下的所有实例都具有相同的分支。</p><p><strong>4.多数表决器</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多数表决器</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    </span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="comment">#for程序用来计数</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): </span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#排序函数</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>5.创建决策树</strong>  </p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建树</span></span><br><span class="line"><span class="comment">#     myDat  = [[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no']]</span></span><br><span class="line"><span class="comment">#     labels = ['no surfacing','flippers']</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#classLsit里面是dataSet里面的标签</span></span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList): <span class="comment">#第一个终止条件：所有类标签都相同，country（）函数用来计数0</span></span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]<span class="comment">#stop splitting when all of the classes are equal</span></span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: <span class="comment">#第二个终止条件：用完了所有的特征#stop splitting when there are no more features in dataSet</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#得到标签里的所有属性值</span></span><br><span class="line">    uniqueVals = set(featValues)<span class="comment">#得到属性值集合</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]       <span class="comment">#copy all of labels, so trees don't mess up existing labels</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br></pre></td></tr></table></figure><pre><code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}</code></pre><p><strong>6.使用决策树进行分类</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用决策树分类函数</span></span><br><span class="line"><span class="comment">#三个参数意义：input：决策树；featLabels：特征标签；testVec：测试向量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure><p><strong>7.决策树在磁盘中的存储与导入</strong>  </p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#将磁盘中的对象加载出来，这里filename就是上面函数中的txt文件</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br><span class="line"><span class="comment"># storeTree(myTree,'classifierStorage.txt')</span></span><br><span class="line"><span class="comment"># grabTree('classifierStorage.txt')</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------IndexError                                Traceback (most recent call last)&lt;ipython-input-16-33c9af9c39fa&gt; in &lt;module&gt;()----&gt; 1 createTree(myDat,labels)      2 # storeTree(myTree,&#39;classifierStorage.txt&#39;)      3 # grabTree(&#39;classifierStorage.txt&#39;)&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)     21     for value in uniqueVals:     22         subLabels = labels[:]       #copy all of labels, so trees don&#39;t mess up existing labels---&gt; 23         myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)     24     return myTree&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)     14         return majorityCnt(classList)     15     bestFeat = chooseBestFeatureToSplit(dataSet)---&gt; 16     bestFeatLabel = labels[bestFeat]     17     myTree = {bestFeatLabel:{}}     18     del(labels[bestFeat])IndexError: list index out of range</code></pre><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li>CART决策树</li><li>决策树的剪枝技术</li><li>Bagging与随机森林</li><li>决策树中缺失值的处理</li><li>决策树代码</li></ol><p>注：本节代码对应第九章“树回归”</p><h2 id="CART决策树"><a href="#CART决策树" class="headerlink" title="CART决策树"></a>CART决策树</h2><p>为什么同样作为建立决策树的三种算法之一，我们要将CART算法单独拿出来讲。</p><p>因为ID3算法和C4.5算法采用了较为复杂的熵来度量，所以它们只能处理分类问题。而CART算法既能处理分类问题，又能处理回归问题。</p><p>对于分类树，CART采用基尼指数最小化准则；对于回归树，CART采用平方误差最小化准则</p><h3 id="1-CART分类树"><a href="#1-CART分类树" class="headerlink" title="1.CART分类树"></a>1.CART分类树</h3><p>CART分类树与上一节讲述的ID3算法和C4.5算法在原理部分差别不大，唯一的区别在于划分属性的原则。CART选择“基尼指数”作为划分属性的选择。</p><p>Gini指数作为一种做特征选择的方式，其表征了特征的不纯度。</p><p>在具体的分类问题中，对于数据集D，我们假设有K个类别，每个类别出现的概率为$P_k$，则数据集$D$的基尼指数的表达式为：</p><script type="math/tex; mode=display">Gini=1-\sum_{k=1}^{K}{P_k}^2</script><p>我们取一个极端情况，如果数据集合中的类别只有一类，那么：</p><script type="math/tex; mode=display">Gini(D)=0</script><p>我们发现，当只有一类时，数据的不纯度是最低的，所以Gini指数等于零。Gini(D)越小，则数据集D的纯度越高。</p><p>特别地，对于样本D，如果我们选择特征A的某个值a，把D分成$D_1$和$D_2$两部分，则此时，Gini指数为：  </p><script type="math/tex; mode=display">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>与信息增益类似，我们可以计算如下表达式：  </p><script type="math/tex; mode=display">\Delta{Gini(A)}=Gini(D)-Gini(D,A)</script><p>即以特征A划分后，数据不纯度减少的程度。显然，我们在做特征选取时，应该选择最大的一个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat,labels</span><br></pre></td></tr></table></figure><pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]], [&#39;no surfacing&#39;, &#39;flippers&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">currentLabel</span><br></pre></td></tr></table></figure><pre><code>&#39;no&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">labelCounts = &#123;&#125;<span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]<span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">    <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">        labelCounts[currentLabel] = <span class="number">0</span><span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">    labelCounts[currentLabel] += <span class="number">1</span><span class="comment">#标签为currentLabel的个数加1</span></span><br><span class="line">labelCounts</span><br></pre></td></tr></table></figure><pre><code>{&#39;yes&#39;: 2, &#39;no&#39;: 3}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(featList)</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 0, 0][1, 1, 0, 1, 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>[1, 1, &#39;yes&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">numFeatures</span><br></pre></td></tr></table></figure><pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><pre><code>01</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">classList</span><br></pre></td></tr></table></figure><pre><code>[&#39;yes&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;no&#39;, &#39;no&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bestFeat = chooseBestFeatureToSplit(myDat)</span><br><span class="line">bestFeatLabel = labels[bestFeat]</span><br><span class="line">myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">myTree</span><br></pre></td></tr></table></figure><pre><code>{&#39;no surfacing&#39;: {}}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">featValues</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 0, 0]</code></pre><p>至此，我们完成了决策树算法原理和主要代码的学习。</p><p>下一节我们将学习CART算法、随机森林算法以及剪枝技术。</p><p>以上原理部分主要来自于《机器学习》—周志华，《统计学习方法》—李航，《机器学习实战》—Peter Harrington。代码部分主要来自于《机器学习实战》，我对代码进行了版本的改进，文中代码用Python3实现，这是机器学习主流语言，本人也会尽力对代码做出较为详尽的注释。</p><h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p><strong>熵</strong>： 熵（entropy）指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p><p><strong>信息熵（香农熵）</strong>： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p><p>信息增益： 在划分数据集前后信息发生的变化称为信息增益。</p><h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br>我们使用 createBranch() 方法，如下所示：</p><blockquote><p>检测数据集中的所有数据的分类标签是否相同:<br>         If so return 类标签<br>            Else:<br>                寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）<br>                划分数据集<br>                创建分支节点<br>                        for 每个划分的子集<br>                                调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中<br>                return 分支节点    </p></blockquote><p>​        </p><h4 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h4><blockquote><p>收集数据：可以使用任何方法。<br>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>训练算法：构造树的数据结构。<br>测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充）<br>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。     </p></blockquote><p><strong>决策树 算法特点</strong></p><blockquote><p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题。<br>适用数据类型：数值型和标称型。</p></blockquote><h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>1.算法简介</strong></p><p>决策树算法是一类常见的分类和回归算法，顾名思义，决策树是基于树的结构来进行决策的。</p><p>以二分类为例，我们希望从给定训练集中学得一个模型来对新的样例进行分类。</p><p><strong>举个例子</strong></p><p>有一个划分是不是鸟类的数据集合，如下：</p><div class="table-container"><table><thead><tr><th></th><th>是否会飞</th><th>是否会跑</th><th>属于鸟类</th></tr></thead><tbody><tr><td>1</td><td>是</td><td>是</td><td>否</td></tr><tr><td>2</td><td>是</td><td>是</td><td>是</td></tr><tr><td>3</td><td>是</td><td>否</td><td>否</td></tr><tr><td>4</td><td>否</td><td>是</td><td>否</td></tr><tr><td>5</td><td>否</td><td>是</td><td>否</td></tr></tbody></table></div><p>这时候我们建立这样一个决策树：  </p><p><img src="https://pic4.zhimg.com/80/v2-4a601bdc74abb553c0873fbd61597035_hd.jpg" ,width="400,height=400"></p><p>当我们有了一组新的数据时，我们就可以根据这个决策树判断出是不是鸟类。创建决策树的伪代码如下：  </p><p><img src="https://pic4.zhimg.com/80/v2-c226901dc50538bd40410e7aae938f47_hd.jpg" ,width="400,eight=400"></p><p>生成决策树是一个递归的过程，在决策树算法中，当出现下列三种情况时，导致递归返回： </p><p>(1)当前节点包含的样本属于同一种类，无需划分；</p><p>(2)当前属性集合为空，或者所有样本在所有属性上取值相同，无法划分；</p><p>(3)当前节点包含的样本集合为空，无法划分。</p><p><strong>2.属性选择</strong></p><p>在决策树算法中，最重要的就是划分属性的选择，即我们选择哪一个属性来进行划分。三种划分属性的主要算法是：ID3、C4.5以及CART。</p><p><strong>2.1 ID3算法</strong></p><p>ID3算法所采用的度量标准就是我们前面所提到的“信息增益”。当属性a的信息增益最大时，则意味着用a属性划分，其所获得的“纯度”提升最大。我们所要做的，就是找到信息增益最大的属性。由于前面已经强调了信息增益的概念，这里不再赘述。</p><p><strong>2.2 C4.5算法</strong></p><p>实际上，信息增益准则对于可取值数目较多的属性会有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：  </p><script type="math/tex; mode=display">Gain\_ratio(Y,X)=\frac{Gain(Y,X)}{H(X)}</script><p>其中，分子为信息增益，分母为属性$X$的熵。</p><p>需要注意的是，增益率准则对可取值数目较少的属性有所偏好。</p><p>所以一般这样选取划分属性：<strong>先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</strong>。</p><p><strong>2.3 CART算法</strong></p><p>ID3算法和C4.5算法主要存在三个问题：</p><p>(1)每次选取最佳特征来分割数据，并按照该特征的所有取值来进行划分。也就是说，如果一个特征有4种取值，那么数据就将被切成4份，一旦特征被切分后，该特征就不会再起作用，有观点认为这种切分方式过于迅速。</p><p>(2)它们不能处理连续型特征。只有事先将连续型特征转换为离散型，才能在上述算法中使用。</p><p>(3)会产生过拟合问题。</p><p>为了解决上述(1)、(2)问题，产生了CART算法，它主要的衡量指标是基尼系数。为了解决问题(3)，主要采用剪枝技术和随机森林算法，这部分内容，下一次再详细讲述。</p><p>上述就是决策树算法的原理部分，下面展示完整代码和注释。代码中主要采用的是ID3算法。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>01机器学习实战-机器学习基础</title>
      <link href="/2018/01/27/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC1%E7%AB%A0%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/27/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC1%E7%AB%A0%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><ul><li>主要来源自《机器学习实战》《机器学习》《利用Python进行数据分析》，还有一些网站资料</li></ul><h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><h2 id="第1章-机器学习基础"><a href="#第1章-机器学习基础" class="headerlink" title="第1章 机器学习基础"></a>第1章 机器学习基础</h2><p><strong>机器学习概述</strong></p><p>机器学习就是无序的数据转换成有用信息。</p><blockquote><ol><li><p>获取海量的数据   </p></li><li><p>从海量数据中获取有用信息</p></li></ol></blockquote><h3 id="机器学习场景"><a href="#机器学习场景" class="headerlink" title="机器学习场景"></a>机器学习场景</h3><blockquote><p>例如：识别动物猫<br>模式识别（官方标准）：人们通过大量的经验，得到结论，从而判断它就是猫。<br>机器学习（数据学习）：人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。<br>深度学习（深入数据）：人们通过深入了解它，发现它会’喵喵’的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域：语音识别、图像识别）   </p></blockquote><p>模式识别（pattern recognition）：模式识别是最古老的（作为一个术语而言，可以说是很过时的）。<br>    我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。<br>    通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。    </p><p>机器学习（machine learning）：机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。<br>    在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。<br>    “机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。<br>    机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。    </p><p>深度学习（deep learning）：深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。<br>    深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</p><p>参考地址： </p><p><a href="http://www.csdn.net/article/2015-03-24/2824301" target="_blank" rel="noopener">http://www.csdn.net/article/2015-03-24/2824301</a></p><p><a href="http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q" target="_blank" rel="noopener">http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q</a></p><blockquote><p>机器学习已应用于多个领域，远远超出大多数人的想象，横跨：计算机科学、工程技术和统计学等多个学科。</p></blockquote><ul><li>搜索引擎：根据你的搜索点击，优化你下次的搜索结果。   </li><li>垃圾邮件：会自动的过滤垃圾广告邮件到垃圾箱内。   </li><li>超市优惠券：你会发现，你在购买小孩子尿布时候，售货员会赠送给你一张优惠券可以兑换免费啤酒。  </li><li>邮件邮寄：手写软件自动识别寄送贺卡的地址。  </li><li>申请贷款：通过你最近的金融活动信息进行综合评定，决定你是否合格。  </li></ul><h3 id="机器学习组成"><a href="#机器学习组成" class="headerlink" title="机器学习组成"></a>机器学习组成</h3><p><strong>主要任务</strong></p><ul><li>分类：将实例数据划分到合适的类别中。  </li><li>回归：主要用于预测数值型数据（示例：数据通过给定数据点耐力和最优曲线）   </li></ul><p><strong>监督学习</strong></p><ul><li>必须确定目标变量的值，以便机器学习可以发现特征值和目标变量之间的关系。（包括分类和回归）  </li><li>样本集：训练数据+测试数据<ul><li>训练样本=特征（feature）+目标变量（label：分类-离散值/回归-连续值）  </li><li>特征通常是训练样本集的列，它们是独立测量得到的。 </li><li>目标变量：目标变量是机器学习预测算法的测试结果。<ul><li>在分类算法中目标变量的类型通常是标称型（如：真与假），而在回归算法中通常是连续型（如：1~100）</li></ul></li></ul></li></ul><ul><li>知识表示：<br>1.可以采用规则集的形式【例如：数学成绩大于90分为优秀】<br>2.可以采用概率分布的形式【例如：通过统计分布，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】<br>3.可以使用训练样本集中的一个实例【例如：通过样本集合，我们训练处一个模型实例，得出年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】</li></ul><p><strong>非监督学习</strong>  </p><ul><li>数据没有类别，也不会给定目标值。  </li><li>聚类：在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。  </li><li>此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。</li></ul><p><strong>训练过程</strong>  </p><p><img src="https://pic4.zhimg.com/80/v2-711466f238bbb969618e6fe669a16a4f_hd.jpg" ,width="400,height=400"></p><p><strong>算法汇总</strong>   </p><center>**用于执行分类、回归、聚类和密度估计的机器学习算法**</center><div class="table-container"><table><thead><tr><th><p align="center">监督学习的用途</p></th></tr></thead><tbody><tr><td><p align="left">k-近邻算法</p></td><td><p align="left">线性回归</p></td></tr><tr><td><p align="left">朴素贝叶斯算法</p></td><td><p align="left">局部加权线性回归</p></td></tr><tr><td><p align="left">支持向量机</p></td><td><p align="left">Ridge回归</p></td></tr><tr><td><p align="left">决策树</p></td><td><p align="left">Lasso最小回归系数估计</p></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th><p align="center">无监督学习的用途</p></th></tr></thead><tbody><tr><td><p align="left">K-均值</p></td><td><p align="left">最大期望算法</p></td></tr><tr><td><p align="left">DBSCAN</p></td><td><p align="left">Parzen窗设计</p></td></tr></tbody></table></div><p>机器学习使用</p><blockquote><p>选择算法需要考虑的两个问题</p></blockquote><p>1.算法场景  </p><ul><li>预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法   </li><li>给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。    </li></ul><p>2.需要收集或分析的数据是什么  </p><blockquote><p>举例</p></blockquote><p><img src="https://pic3.zhimg.com/80/v2-88ee740c5e4a2a2bdab7e08043321e08_hd.jpg" ,width="400,height=400"></p><blockquote><p>机器学习开发流程</p></blockquote><ul><li>收集数据: 收集样本数据</li><li>准备数据: 注意数据的格式</li><li>分析数据: 为了确保数据集中没有垃圾数据；<br>  如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；<br>  另外该步骤需要人工干预，会降低自动化系统的价值。</li><li>训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤</li><li>测试算法: [机器学习算法核心]评估算法效果</li><li>使用算法: 将机器学习算法转为应用程序</li></ul><p><strong>Python语言优势</strong></p><ol><li>可执行伪代码</li><li>Python比较流行：使用广泛、代码范例多、丰富模块库，开发周期短</li><li>Python语言的特色：清晰简练、易于理解</li><li>Python语言的缺点：唯一不足的是性能问题</li><li>Python相关的库    </li></ol><ul><li>科学函数库：SciPy、NumPy（底层语言：C和Fortran）</li><li>绘图工具库：Matplotlib</li></ul><h3 id="科普"><a href="#科普" class="headerlink" title="科普"></a>科普</h3><p>　　机器学习（Machine Learning）专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合。  ——来自 百度百科</p><p>简单来讲，机器学习就是一门让机器能够进行自我学习并不断优化功能的学科。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>00机器学习实战-机器学习的数学基础</title>
      <link href="/2018/01/26/00%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/26/00%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><h1 id="机器学习：机器学习的数学基础"><a href="#机器学习：机器学习的数学基础" class="headerlink" title="机器学习：机器学习的数学基础"></a>机器学习：机器学习的数学基础</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>机器学习的特点就是：以计算机为工具和平台，以数据为研究对象，以学习方法为中心；<br>是概率论、线性代数、数值计算、信息论、最优化理论和计算机科学等多个领域的交叉学科。</p><h2 id="二、线性代数"><a href="#二、线性代数" class="headerlink" title="二、线性代数"></a>二、线性代数</h2><h3 id="2-1-标量"><a href="#2-1-标量" class="headerlink" title="2.1 标量"></a>2.1 标量</h3><p>一个标量就是一个单独的数，一般用小写的的变量名称表示。</p><h3 id="2-2-向量"><a href="#2-2-向量" class="headerlink" title="2.2 向量"></a>2.2 向量</h3><p>一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱：</p><script type="math/tex; mode=display">X=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\quad</script><p>我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。 </p><h3 id="2-3-矩阵"><a href="#2-3-矩阵" class="headerlink" title="2.3 矩阵"></a>2.3 矩阵</h3><p>矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如$A$。如果一个实数矩阵高度为$m$，宽度为$n$，那么我们说$A\epsilon R^{m\times n}$。</p><script type="math/tex; mode=display">A=\begin{bmatrix}a_{11} & a_{12} & \dots & a_{1n}\\a_{21} & a_{22} & \dots & a_{2n}\\\vdots & \vdots &  & \dots\\a_{m1} & a_{m2} & \dots & a_{mn}\end{bmatrix}\quad</script><p>矩阵这东西在机器学习中就不要太重要了！实际上，如果我们现在有$N$个用户的数据，每条数据含有$M$个特征，那其实它对应的就是一个$N<em>M$的矩阵呀；再比如，一张图由$16</em>16$的像素点组成，那这就是一个$16*16$的矩阵了。</p><h3 id="2-4-张量"><a href="#2-4-张量" class="headerlink" title="2.4 张量"></a>2.4 张量</h3><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。</p><p>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：</p><p><img src="https://pic4.zhimg.com/v2-c0c16793d4662bfcdd7e112030096f94_r.jpg" alt=""></p><p>其中表的横轴表示图片的宽度值，这里只截取$0-319$；表的纵轴表示图片的高度值，这里只截取$0-4$；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为$[1.0,1.0,1.0]$，代表的就是$RGB$三原色在图片的这个位置的取值情况（即$R=1.0$，$G=1.0$，$B=1.0$）。</p><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p><h3 id="2-5-范数"><a href="#2-5-范数" class="headerlink" title="2-5 范数"></a>2-5 范数</h3><p>有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数$(norm)$的函数衡量矩阵大小。$L_p$范数如下：</p><script type="math/tex; mode=display">\left| \left| x \right| \right| _{p}^{} =\left( \sum_{i}^{}{\left| x_{i} \right| ^{p} } \right) _{}^{\frac{1}{p} }</script><p>所以：</p><p>$L_1$范数$ \left| \left| x \right| \right|$：为$x$向量各个元素绝对值之和；</p><p>$L<em>2$范数$ \left| \left| x \right| \right| </em>{2}$：为$x$向量各个元素平方和的开方。</p><p>这里先说明一下，在机器学习中，$ L_1$范数和$ L_2$范数很常见，主要用在损失函数中起到一个限制模型参数复杂度的作用，至于为什么要限制模型的复杂度，这又涉及到机器学习中常见的过拟合问题。具体的概念在后续文章中会有详细的说明和推导，大家先记住：这个东西很重要，实际中经常会涉及到，面试中也常会被问到！！！</p><h3 id="2-6-特征分解"><a href="#2-6-特征分解" class="headerlink" title="2.6 特征分解"></a>2.6 特征分解</h3><p>许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。</p><p>方阵$ A$的特征向量是指与$ A$相乘后相当于对该向量进行缩放的非零向量$ \nu$ ：</p><script type="math/tex; mode=display">A\nu =\lambda \nu</script><p>标量$\lambda$被称为这个特征向量对应的特征值。 </p><p>使用特征分解去分析矩阵$ A$时，得到特征向量构成的矩阵$ V$和特征值构成的向量$\lambda$，我们可以重新将A写作：</p><script type="math/tex; mode=display">A=Vdiag\left( \lambda \right) V^{-1}</script><h3 id="2-7-奇异值分解（Singular-Value-Decomposition，SVD）"><a href="#2-7-奇异值分解（Singular-Value-Decomposition，SVD）" class="headerlink" title="2-7 奇异值分解（Singular Value Decomposition，SVD）"></a>2-7 奇异值分解（Singular Value Decomposition，SVD）</h3><p>矩阵的特征分解是有前提条件的，那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称$ SVD$。通过奇异分解，我们会得到一些类似于特征分解的信息。</p><p>它的具体做法是将一个普通矩阵分解为奇异向量和奇异值。比如将矩阵$ A$分解成三个矩阵的乘积：</p><script type="math/tex; mode=display">A=UDV^{T}</script><p>假设<strong>$ A$是一个$m\times n$矩阵，那么$ U$是一个$m\times m$矩阵，$D$是一个$m\times n$矩阵，$V$是一个$n\times n$矩阵</strong>。</p><p>这些矩阵每一个都拥有特殊的结构，其中$U$和$V$都是正交矩阵，$D$是对角矩阵（注意，$D$不一定是方阵）。对角矩阵$D$对角线上的元素被称为矩阵$A$的奇异值。矩阵$U$的列向量被称为<strong>左奇异向量</strong>，矩阵$V$的列向量被称<strong>右奇异向量</strong>。</p><p>$SVD$最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，$SVD$可用于推荐系统中。</p><h3 id="2-8-Moore-Penrose伪逆"><a href="#2-8-Moore-Penrose伪逆" class="headerlink" title="2-8 Moore-Penrose伪逆"></a>2-8 Moore-Penrose伪逆</h3><p>对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵$A$的左逆$B$来求解线性方程：</p><script type="math/tex; mode=display">Ax=y</script><p>等式两边同时左乘左逆$B$后，得到：</p><script type="math/tex; mode=display">x=By</script><p>是否存在唯一的映射将$A$映射到$B$取决于问题的形式。</p><p>如果矩阵$A$的行数大于列数，那么上述方程可能没有解；如果矩阵$A$的行数小于列数，那么上述方程可能有多个解。</p><p>$Moore-Penrose$伪逆使我们能够解决这种情况，矩阵$A$的伪逆定义为：</p><script type="math/tex; mode=display">A^+=\lim_{a\rightarrow{0}}(A^T{A}+\alpha{I}^{-1})A^T</script><p>但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：</p><script type="math/tex; mode=display">A^+=VD^+{U}^T</script><p>其中，矩阵$U$，$D$和$V$是矩阵$A$奇异值分解后得到的矩阵。对角矩阵$D$的伪逆$D^+$是其非零元素取倒之后再转置得到的。</p><h3 id="2-9-几种常用的距离"><a href="#2-9-几种常用的距离" class="headerlink" title="2-9 几种常用的距离"></a>2-9 几种常用的距离</h3><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有$100$个特征，那么他对应的就是一个$100$维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的$KNN$算法和$K-means$算法中很明显。</p><p>设有两个$n$维变量$A=\left[ x<em>{11}, x</em>{12},…,x<em>{1n} \right]$和$B=\left[ x</em>{21} ,x<em>{22} ,…,x</em>{2n} \right]$，则一些常用的距离公式定义如下：</p><h4 id="1、曼哈顿距离"><a href="#1、曼哈顿距离" class="headerlink" title="1、曼哈顿距离"></a>1、曼哈顿距离</h4><p>曼哈顿距离也称为城市街区距离，数学定义如下：</p><script type="math/tex; mode=display">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| }</script><p>曼哈顿距离的$Python$实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *  </span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  </span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])  </span><br><span class="line"><span class="keyword">print</span> sum(abs(vector1-vector2))</span><br></pre></td></tr></table></figure><h4 id="2、欧氏距离"><a href="#2、欧氏距离" class="headerlink" title="2、欧氏距离"></a>2、欧氏距离</h4><p>欧氏距离其实就是$L_2$范数，数学定义如下： </p><script type="math/tex; mode=display">d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } }</script><p>欧氏距离的$Python$实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt((vector1-vector2)*(vector1-vector2).T)</span><br></pre></td></tr></table></figure></p><h4 id="3、闵可夫斯基距离"><a href="#3、闵可夫斯基距离" class="headerlink" title="3、闵可夫斯基距离"></a>3、闵可夫斯基距离</h4><p>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：</p><script type="math/tex; mode=display">d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } }</script><p>实际上，当$p=1$时，就是曼哈顿距离；当$p=2$时，就是欧式距离。</p><h4 id="4、切比雪夫距离"><a href="#4、切比雪夫距离" class="headerlink" title="4、切比雪夫距离"></a>4、切比雪夫距离</h4><p>切比雪夫距离就是$L_{\varpi}$，即无穷范数，数学表达式如下：</p><script type="math/tex; mode=display">d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)</script><p>切比雪夫距离额Python实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt(abs(vector1-vector2).max)</span><br></pre></td></tr></table></figure></p><h4 id="5、夹角余弦"><a href="#5、夹角余弦" class="headerlink" title="5、夹角余弦"></a>5、夹角余弦</h4><p>夹角余弦的取值范围为$[-1,1]$，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值$1$；当两个向量的方向完全相反时，夹角余弦取最小值$-1$。</p><p>机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：</p><script type="math/tex; mode=display">cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } }</script><p>夹角余弦的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))</span><br></pre></td></tr></table></figure></p><h4 id="6、汉明距离"><a href="#6、汉明距离" class="headerlink" title="6、汉明距离"></a>6、汉明距离</h4><p>汉明距离定义的是两个字符串中不相同位数的数目。</p><p>例如：字符串$’1111’$与$’1001’$之间的汉明距离为$2$。</p><p>信息编码中一般应使得编码间的汉明距离尽可能的小。</p><p>汉明距离的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">smstr = nonzero(matV[<span class="number">0</span>]-matV[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> smstr</span><br></pre></td></tr></table></figure></p><h4 id="7、杰卡德相似系数"><a href="#7、杰卡德相似系数" class="headerlink" title="7、杰卡德相似系数"></a>7、杰卡德相似系数</h4><p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号$J(A,B)$表示，数学表达式为：</p><script type="math/tex; mode=display">J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| }</script><p>杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p><h4 id="8、杰卡德距离"><a href="#8、杰卡德距离" class="headerlink" title="8、杰卡德距离"></a>8、杰卡德距离</h4><p>与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：</p><script type="math/tex; mode=display">J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| }</script><p>杰卡德距离的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> scipy.spatial.distance <span class="keyword">as</span> dist</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> dist.pdist(matV,<span class="string">'jaccard'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="三、概率"><a href="#三、概率" class="headerlink" title="三、概率"></a>三、概率</h2><h3 id="3-1-为什么使用概率？"><a href="#3-1-为什么使用概率？" class="headerlink" title="3.1 为什么使用概率？"></a>3.1 为什么使用概率？</h3><p>概率论是用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。</p><p>在人工智能领域，我们主要以两种方式来使用概率论。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。</p><p>计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常。因为机器学习大部分时候处理的都是不确定量或随机量。</p><h4 id="3-2-随机变量"><a href="#3-2-随机变量" class="headerlink" title="3.2 随机变量"></a>3.2 随机变量</h4><p>随机变量可以随机地取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如，x<em>{1}  和x</em>{2}  都是随机变量X可能的取值。</p><p>对于向量值变量，我们会将随机变量写成X，它的一个值为x。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。</p><p>随机变量可以是离散的或者连续的。</p><h4 id="3-3-概率分布"><a href="#3-3-概率分布" class="headerlink" title="3.3 概率分布"></a>3.3 概率分布</h4><p>给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。</p><p>从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。</p><h4 id="3-4-条件概率"><a href="#3-4-条件概率" class="headerlink" title="3.4 条件概率"></a>3.4 条件概率</h4><p>很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。</p><p>我们将给定$X=x$时$Y=y$发生的概率记为$P\left( Y=y|X=x \right)$，这个概率可以通过下面的公式来计算：</p><script type="math/tex; mode=display">P\left( Y=y|X=x \right) =\frac{P\left( Y=y,X=x \right) }{P\left( X=x \right) }</script><h4 id="3-5-贝叶斯公式"><a href="#3-5-贝叶斯公式" class="headerlink" title="3.5 贝叶斯公式"></a>3.5 贝叶斯公式</h4><p>先看看什么是“先验概率”和“后验概率”，以一个例子来说明：</p><p>假设某种病在人群中的发病率是$0.001$，即$1000$人中大概会有$1$个人得病，则有：$P(患病) = 0.1\%$；即：在没有做检验之前，我们预计的患病率为$P(患病)=0.1\%$，这个就叫作”先验概率”。 </p><p>再假设现在有一种该病的检测方法，其检测的准确率为$95\%$；即：如果真的得了这种病，该检测法有$95\%$的概率会检测出阳性，但也有5%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有$95\%$的概率检测出阴性，但也有$5\%$的概率检测为阳性。用概率条件概率表示即为：$P(显示阳性|患病)=95\%$。</p><p>现在我们想知道的是：在做完检测显示为阳性后，某人的患病率$P(患病|显示阳性)$，这个其实就称为”后验概率”。</p><p>而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”。</p><p>这里先了解<strong>条件概率公式</strong>：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}</script><p>由条件概率可以得到<strong>乘法公式</strong>：  </p><script type="math/tex; mode=display">P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)</script><p>将条件概率公式和乘法公式结合可以得到：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}</script><p>再由<strong>全概率公式</strong>：</p><script type="math/tex; mode=display">P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)}</script><p>代入可以得到<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }</script><p>在这个例子里就是：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberp(患病|显示阳性) & =\frac{P(显示阳性|患病)P(患病)}{P(显示阳性)}\\                & =\frac{P(显示阳性|患病)P(患病)}{P(显示阳性|患病)P(患病)+{P(显示阳性|无病)P(无病)}}\\                & =\frac{95\%*0.1\%}{95\%*0.1\%+5\%*99.9\%}=1.86\%\end{aligned}</script><p>贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。</p><p>期望、方差、协方差等主要反映数据的统计特征，机器学习的一个很大应用就是数据挖掘等，因此这些基本的统计概念也是很有必要掌握。另外，像后面的EM算法中，就需要用到期望的相关概念和性质。</p><h4 id="3-6-期望"><a href="#3-6-期望" class="headerlink" title="3.6 期望"></a>3.6 期望</h4><p>在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。</p><p>假设X是一个离散随机变量，其可能的取值有：$\left\{ x<em>{1} ,x</em>{2} ,……,x<em>{n} \right\}$，各个取值对应的概率取值为：$P\left( x</em>{k} \right) , k=1,2,……,n$，则其数学期望被定义为：</p><script type="math/tex; mode=display">E\left(X \right) =\sum_{k=1}^{n}{x_{k} P\left( x_{k} \right) }</script><p>假设$X$是一个连续型随机变量，其概率密度函数为$P\left( x \right)$则其数学期望被定义为：</p><script type="math/tex; mode=display">E\left( x \right) =\int_{-\varpi }^{+\varpi } xf\left( x \right) dx</script><h4 id="3-7-方差"><a href="#3-7-方差" class="headerlink" title="3.7 方差"></a>3.7 方差</h4><p>概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下： </p><script type="math/tex; mode=display">Var\left( x \right) =E\left\{ \left[ x-E\left( x \right) \right] ^{2} \right\} =E\left( x^{2} \right) -\left[ E\left( x \right) \right] ^{2}</script><h4 id="3-8-协方差"><a href="#3-8-协方差" class="headerlink" title="3.8 协方差"></a>3.8 协方差</h4><p>在概率论和统计学中，协方差被用于衡量两个随机变量$X$和$Y$之间的总体误差。数学定义式为：</p><script type="math/tex; mode=display">Cov\left( X,Y \right) =E\left[ \left( X-E\left[ X \right] \right) \left( Y-E\left[ Y \right] \right) \right] =E\left[ XY \right] -E\left[ X \right] E\left[ Y \right]</script><h4 id="3-9-常见分布函数"><a href="#3-9-常见分布函数" class="headerlink" title="3.9 常见分布函数"></a>3.9 常见分布函数</h4><h4 id="1）0-1分布"><a href="#1）0-1分布" class="headerlink" title="1）0-1分布"></a>1）0-1分布</h4><p>$0-1$分布是单个二值型离散随机变量的分布，其概率分布函数为：  </p><script type="math/tex; mode=display">P\left( X=1 \right) =pP\left( X=0 \right) =1-p</script><h4 id="2）几何分布"><a href="#2）几何分布" class="headerlink" title="2）几何分布"></a>2）几何分布</h4><p>几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前$k-1$次皆失败，第k次成功的概率。其概率分布函数为：</p><script type="math/tex; mode=display">P\left( X=k \right) =\left( 1-p \right) ^{k-1} p</script><p>性质：</p><script type="math/tex; mode=display">E\left( X \right) =\frac{1}{p} Var\left( X \right) =\frac{1-p}{p^{2} }</script><h4 id="3）二项分布"><a href="#3）二项分布" class="headerlink" title="3）二项分布"></a>3）二项分布</h4><p>二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：   </p><script type="math/tex; mode=display">P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}</script><p>性质：   </p><script type="math/tex; mode=display">E\left( X \right) =npVar\left( X \right) =np\left( 1-p \right)</script><h4 id="4）高斯分布"><a href="#4）高斯分布" class="headerlink" title="4）高斯分布"></a>4）高斯分布</h4><p>高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：(A这里少高斯分布图像)</p><p><img src="https://pic4.zhimg.com/80/v2-a0811acc8ab121a3ad8f2e37ff6c37cc_hd.jpg" alt=""></p><p>若随机变量$X$服从一个数学期望为$\mu$，方差为$\sigma ^{2}$的正态分布，则我们将其记为：$N\left( \mu ,\sigma^{2} \right)$。其期望值$\mu$决定了正态分布的位置，其标准差$\sigma$（方差的开方）决定了正态分布的幅度。</p><h4 id="5）指数分布"><a href="#5）指数分布" class="headerlink" title="5）指数分布"></a>5）指数分布</h4><p>指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了$t$小时，它总共使用至少$t+s$小时的条件概率，与从开始使用时算起它使用至少$s$小时的概率相等。下面这些都属于指数分布：</p><ul><li>婴儿出生的时间间隔</li><li>网站访问的时间间隔</li><li>奶粉销售的时间间隔    </li></ul><p>指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间$t$，就等同于$t$之内没有任何婴儿出生，即：   </p><script type="math/tex; mode=display">P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t} }{0!}=e^{-\lambda t}</script><p>则：   </p><script type="math/tex; mode=display">P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}</script><p>如：接下来15分钟，会有婴儿出生的概率为：</p><script type="math/tex; mode=display">P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53</script><p>指数分布的图像如下：</p><p><img src="https://pic4.zhimg.com/80/v2-a58c37c481e032bbb53ff17113754ef6_hd.jpg" alt="tu"></p><h4 id="6）泊松分布"><a href="#6）泊松分布" class="headerlink" title="6）泊松分布"></a>6）泊松分布</h4><p>日常生活中，大量事件是有固定频率的，比如：</p><p>某医院平均每小时出生3个婴儿<br>某网站平均每分钟有2次访问<br>某超市平均每小时销售4包奶粉<br>它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。</p><p>泊松分布就是描述某段时间内，事件具体的发生概率。其概率函数为：   </p><script type="math/tex; mode=display">P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t} }{n!}</script><p>其中：</p><p>$P$表示概率，$N$表示某种函数关系，$t$表示时间，$n$表示数量，1小时内出生3个婴儿的概率，就表示为$P(N(1) = 3)$；$λ$表示事件的频率。</p><p>还是以上面医院平均每小时出生3个婴儿为例，则$\lambda =3$；</p><p>那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：   </p><script type="math/tex; mode=display">P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025</script><p>同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：    </p><script type="math/tex; mode=display">P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0 \right) - P\left( N\left( 1 \right)=1 \right)\approx 0.8</script><p>【注】上面的指数分布和泊松分布参考了阮一峰大牛的博客：“泊松分布和指数分布：10分钟教程”，在此说明，也对其表示感谢！</p><h4 id="3-10-Lagrange乘子法"><a href="#3-10-Lagrange乘子法" class="headerlink" title="3.10 Lagrange乘子法"></a>3.10 Lagrange乘子法</h4><p>对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。</p><p>对于一个要求极值的函数$f\left( x,y \right)$，图上的蓝圈就是这个函数的等高图，就是说$f\left( x,y \right) =c<em>{1} ,c</em>{2} ,…,c<em>{n}$分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组$\left( x,y \right)$，使它的$c</em>{i}$值越大越好，但是这点必须满足约束条件$g\left( x,y \right)$（在黄线上）。</p><p><img src="https://pic1.zhimg.com/80/v2-e59fd8c296c7e8c3b804726998610b31_hd.jpg" alt="tu1" title="tu1"></p><p>也就是说$f(x,y)$和$g(x,y)$相切，或者说它们的梯度$\nabla{f}$和$\nabla{g}$平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为$\lambda $倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。</p><p>在支持向量机模型（SVM）的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。</p><h4 id="3-11、最大似然估计"><a href="#3-11、最大似然估计" class="headerlink" title="3-11、最大似然估计"></a>3-11、最大似然估计</h4><p>最大似然也称为最大概似估计，即：在“模型已定，参数$θ$未知”的情况下，通过观测数据估计未知参数θ 的一种思想或方法。</p><p><strong>其基本思想是</strong>：  给定样本取值后，该样本最有可能来自参数$\theta$为何值的总体。即：寻找$\tilde{\theta } _{ML}$使得观测到样本数据的可能性最大。</p><p>举个例子，假设我们要统计全国人口的身高，首先假设这个身高服从服从正态分布，但是该分布的均值与方差未知。由于没有足够的人力和物力去统计全国每个人的身高，但是可以通过采样（所有的采样要求都是独立同分布的），获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布的均值与方差。</p><p>求极大似然函数估计值的一般步骤：</p><ol><li>写出似然函数； <script type="math/tex; mode=display">L(\theta_1,\theta_2,\dots,\theta_n)=\left \{\begin{array}{c}\prod_{i=1}^{n}p(x_i;\theta_1,\theta_2,\dots,\theta_n)\\\prod_{i=1}^{n}f(x_i;\theta_1,\theta_2,\dots,\theta_n)\end{array}\right.</script></li><li>对似然函数取对数；  </li><li>两边同时求导数；  </li><li>令导数为0解出似然方程。  </li></ol><p>在机器学习中也会经常见到极大似然的影子。比如后面的<strong>逻辑斯特回归模型（LR）</strong>，其核心就是构造对数损失函数后运用极大似然估计。</p><h3 id="四、信息论"><a href="#四、信息论" class="headerlink" title="四、信息论"></a>四、信息论</h3><p>信息论本来是通信中的概念，但是其核心思想“熵”在机器学习中也得到了广泛的应用。比如决策树模型$ID3$，$C4.5$中是利用<strong>信息增益</strong>来划分特征而生成一颗决策树的，而信息增益就是基于这里所说的<strong>熵</strong>。所以它的重要性也是可想而知。</p><h4 id="4-1-熵"><a href="#4-1-熵" class="headerlink" title="4.1 熵"></a>4.1 熵</h4><p>如果一个随机变量X的可能取值为$X=\left\{ x<em>{1},x</em>{2} ,…..,x<em>{n} \right\}$，其概率分布为$P\left( X=x</em>{i} \right) =p_{i} ,i=1,2,…..,n$，则随机变量X的熵定义为$H(X)$：</p><script type="math/tex; mode=display">H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i} \right) logP\left( x_{i} \right) } =\sum_{i=1}^{n}{P\left( x_{i} \right) \frac{1}{logP\left( x_{i} \right) } }</script><h4 id="4-2-联合熵"><a href="#4-2-联合熵" class="headerlink" title="4.2 联合熵"></a>4.2 联合熵</h4><p>两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量XY的不确定性的度量，用$H(X,Y)$表示：</p><script type="math/tex; mode=display">H\left( X,Y \right) =-\sum_{i=1}^{n}{\sum_{j=1}^{n}{P\left( x_{i} ,y_{j} \right)} logP\left( x_{i},y_{j} \right) }</script><h4 id="4-3-条件熵"><a href="#4-3-条件熵" class="headerlink" title="4.3 条件熵"></a>4.3 条件熵</h4><p>在随机变量$X$发生的前提下，随机变量$Y$发生新带来的熵，定义为$Y$的条件熵，用$H(Y|X)$表示：</p><script type="math/tex; mode=display">H\left(Y|X \right) =-\sum_{x,y}^{}{P\left( x,y \right) logP\left( y|x \right) }</script><p>条件熵用来衡量在已知随机变量$X$的条件下，随机变量$Y$的不确定性。</p><p>实际上，熵、联合熵和条件熵之间存在以下关系：</p><script type="math/tex; mode=display">H\left( Y|X \right) =H\left( X,Y\right) -H\left( X \right)</script><p>推导过程如下：  </p><script type="math/tex; mode=display">\begin{array}{ll}H\left( X,Y\right) -H\left( X \right)\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(x,y)}+\sum_{x}p(x)log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(x,y)}+\sum_{x}{(\sum_{y}p(x,y))}log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(y|x)}+\sum_{x,y}p(x,y)log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)}log{\frac{p(x,y)}{p(x)}}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)}logp(y|x)\end{array}</script><p>其中：</p><ul><li>第二行推到第三行的依据是边缘分布$P(x)$等于联合分布$P(x,y)$的和；</li><li>第三行推到第四行的依据是把公因子$logP(x)$乘进去，然后把$x$,$y$写在一起；</li><li>第四行推到第五行的依据是：因为两个sigma都有$P(x,y)$，故提取公因子$P(x,y)$放到外边，然后把里边的$-（log P(x,y) - log P(x)）$写成$- log (P(x,y) / P(x) ) $；</li><li>第五行推到第六行的依据是：$P(x,y) = P(x) * P(y|x)$，故$P(x,y) / P(x) =  P(y|x)$。   </li></ul><h4 id="4-4-相对熵"><a href="#4-4-相对熵" class="headerlink" title="4.4 相对熵"></a>4.4 相对熵</h4><p>相对熵又称互熵、交叉熵、$KL$散度、信息增益，是描述两个概率分布$P$和$Q$差异的一种方法，记为<strong>$D(P||Q)$</strong>。在信息论中，$D(P||Q)$表示当用概率分布$Q$来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，$Q$表示$P$的拟合分布。</p><p>对于一个离散随机变量的两个概率分布$P$和$Q$来说，它们的相对熵定义为：</p><script type="math/tex; mode=display">D\left( P||Q \right) =\sum_{i=1}^{n}{P\left( x_{i} \right) log\frac{P\left( x_{i} \right) }{Q\left( x_{i} \right) } }</script><p>注意：$D(P||Q) ≠ D(Q||P)$</p><p>相对熵又称<strong>$KL$散度($Kullback–Leibler divergence$)</strong>，$KL$散度也是一个机器学习中常考的概念。</p><h4 id="4-5-互信息"><a href="#4-5-互信息" class="headerlink" title="4.5 互信息"></a>4.5 互信息</h4><p>两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵称为互信息，用I(X,Y)表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。  </p><script type="math/tex; mode=display">I\left( X,Y \right) =\sum_{x\in X}^{}{\sum_{y\in Y}^{}{P\left( x,y \right) } log\frac{P\left( x,y \right) }{P\left( x \right) P\left( y \right) } }</script><p>互信息、熵和条件熵之间存在以下关系：$ H\left( Y|X \right) =H\left( Y \right) -I\left( X,Y \right) $  </p><p>推导过程如下：</p><p><img src="https://pic3.zhimg.com/v2-6f41bffde009999cbc370f7f38cab092_r.jpg" alt="img"></p><p>通过上面的计算过程发现有：$H(Y|X) = H(Y) - I(X,Y)$，又由前面条件熵的定义有：$H(Y|X) = H(X,Y) - H(X)$，于是有$I(X,Y)= H(X) + H(Y) - H(X,Y)$，此结论被多数文献作为互信息的定义。</p><h3 id="4-6、最大熵模型"><a href="#4-6、最大熵模型" class="headerlink" title="4-6、最大熵模型"></a>4-6、最大熵模型</h3><p>最大熵原理是概率模型学习的一个准则，它认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。通常用约束条件来确定模型的集合，所以，最大熵模型原理也可以表述为：在满足约束条件的模型集合中选取熵最大的模型。 </p><p>前面我们知道，若随机变量X的概率分布是$P\left( x_{i} \right)$ ，则其熵定义如下：    </p><script type="math/tex; mode=display">H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i} \right) logP\left( x_{i} \right) } =\sum_{i=1}^{n}{P\left( x_{i} \right) \frac{1}{logP\left( x_{i} \right) } }</script><p>熵满足下列不等式：</p><script type="math/tex; mode=display">0\leq H\left( X \right) \leq log\left| X \right|</script><p>式中，$|X|$是$X$的取值个数，当且仅当$X$的分布是均匀分布时右边的等号成立。也就是说，当$X$服从均匀分布时，熵最大。</p><p>直观地看，最大熵原理认为：要选择概率模型，首先必须满足已有的事实，即约束条件；在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性；“等可能”不易操作，而熵则是一个可优化的指标。</p><h2 id="五、-数值计算"><a href="#五、-数值计算" class="headerlink" title="五、 数值计算"></a>五、 数值计算</h2><h3 id="5-1-上溢和下溢"><a href="#5-1-上溢和下溢" class="headerlink" title="5.1 上溢和下溢"></a>5.1 上溢和下溢</h3><p>在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。</p><p>一种特别毁灭性的舍入误差是下溢。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除。</p><p>另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为\varpi 或-\varpi 时发生上溢。进一步的运算通常将这些无限值变为非数字。</p><p>必须对上溢和下溢进行数值稳定的一个例子是softmax 函数。softmax 函数经常用于预测与$multinoulli$分布相关联的概率，定义为：</p><p><img src="https://pic2.zhimg.com/80/v2-7283f680255ba0da3a69f2df58b58ae0_hd.jpg" alt=""></p><p>softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 $x<em>{i} $的类别标签$ y</em>{i}$ 属于$K$个类别的概率，最后判别$ y_{i}$ 所属类别时就是将其归为对应概率最大的那一个。</p><p>当式中的$w<em>{k} x</em>{i} +b$都是很小的负数时，$e^{w<em>{k} x</em>{i} +b }$ 就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的$x<em>{w</em>{k} x<em>{i} +b} $是很大的正数时，$e^{w</em>{k} x_{i} +b }$ 就会发生上溢导致结果是未定的。</p><h3 id="5-2-计算复杂性与NP问题"><a href="#5-2-计算复杂性与NP问题" class="headerlink" title="5.2 计算复杂性与NP问题"></a>5.2 计算复杂性与NP问题</h3><h4 id="1-算法复杂性"><a href="#1-算法复杂性" class="headerlink" title="1.算法复杂性"></a>1.算法复杂性</h4><p>现实中大多数问题都是离散的数据集，为了反映统计规律，有时数据量很大，而且多数目标函数都不能简单地求得解析解。这就带来一个问题：算法的复杂性。</p><p>算法理论被认为是解决各类现实问题的方法论。衡量算法有两个重要的指标：时间复杂度和空间复杂度，这是对算法执行所需要的两类资源——时间和空间的估算。</p><p>一般，衡量问题是否可解的重要指标是：该问题能否在多项式时间内求解，还是只能在指数时间内求解？在各类算法理论中，通常使用多项式时间算法即可解决的问题看作是易解问题，需要指数时间算法解决的问题看作是难解问题。</p><p>指数时间算法的计算时间随着问题规模的增长而呈指数化上升，这类问题虽然有解，但并不适用于大规模问题。所以当前算法研究的一个重要任务就是将指数时间算法变换为多项式时间算法。</p><h4 id="2-确定性和非确定性"><a href="#2-确定性和非确定性" class="headerlink" title="2.确定性和非确定性"></a>2.确定性和非确定性</h4><p>除了问题规模与运算时间的比较，衡量一个算法还需要考虑确定性和非确定性的概念。</p><p>这里先介绍一下“自动机”的概念。自动机实际上是指一种基于状态变化进行迭代的算法。在算法领域常把这类算法看作一个机器，比较知名的有图灵机、玻尔兹曼机、支持向量机等。</p><p>所谓确定性，是指针对各种自动机模型，根据当时的状态和输入，若自动机的状态转移是唯一确定的，则称确定性；若在某一时刻自动机有多个状态可供选择，并尝试执行每个可选择的状态，则称为非确定性。</p><p>换个说法就是：确定性是程序每次运行时产生下一步的结果是唯一的，因此返回的结果也是唯一的；非确定性是程序在每个运行时执行的路径是并行且随机的，所有路径都可能返回结果，也可能只有部分返回结果，也可能不返回结果，但是只要有一个路径返回结果，那么算法就结束。</p><p>在求解优化问题时，非确定性算法可能会陷入局部最优。</p><h4 id="3-NP问题"><a href="#3-NP问题" class="headerlink" title="3.NP问题"></a>3.NP问题</h4><p>有了时间上的衡量标准和状态转移的确定性与非确定性的概念，我们来定义一下问题的计算复杂度。</p><p>P类问题就是能够以多项式时间的确定性算法来对问题进行判定或求解，实现它的算法在每个运行状态都是唯一的，最终一定能够确定一个唯一的结果——最优的结果。</p><p>NP问题是指可以用多项式时间的非确定性算法来判定或求解，即这类问题求解的算法大多是非确定性的，但时间复杂度有可能是多项式级别的。</p><p>但是，NP问题还要一个子类称为NP完全问题，它是NP问题中最难的问题，其中任何一个问题至今都没有找到多项式时间的算法。</p><p>机器学习中多数算法都是针对NP问题（包括NP完全问题）的。</p><h3 id="5-3-数值计算"><a href="#5-3-数值计算" class="headerlink" title="5.3 数值计算"></a>5.3 数值计算</h3><p>上面已经分析了，大部分实际情况中，计算机其实都只能做一些近似的数值计算，而不可能找到一个完全精确的值，这其实有一门专门的学科来研究这个问题，这门学科就是——数值分析（有时也叫作“计算方法”）；运用数值分析解决问题的过程为：实际问题→数学模型→数值计算方法→程序设计→上机计算求出结果。</p><p>计算机在做这些数值计算的过程中，经常会涉及到的一个东西就是“迭代运算”，即通过不停的迭代计算，逐渐逼近真实值（当然是要在误差收敛的情况下）。</p><h2 id="六、最优化"><a href="#六、最优化" class="headerlink" title="六、最优化"></a>六、最优化</h2><p>本节介绍机器学习中的一种重要理论——最优化方法。</p><h3 id="6-1-最优化理论"><a href="#6-1-最优化理论" class="headerlink" title="6.1 最优化理论"></a>6.1 最优化理论</h3><p>无论做什么事，人们总希望以最小的代价取得最大的收益。在解决一些工程问题时，人们常会遇到多种因素交织在一起与决策目标相互影响的情况；这就促使人们创造一种新的数学理论来应对这一挑战，也因此，最早的优化方法——线性规划诞生了。</p><p>在李航博士的《统计学习方法》中，其将机器学习总结为如下表达式：</p><p>机器学习 = 模型 + 策略 + 算法</p><p>可以看得出，算法在机器学习中的 重要性。实际上，这里的算法指的就是优化算法。在面试机器学习的岗位时，优化算法也是一个特别高频的问题，大家如果真的想学好机器学习，那还是需要重视起来的。</p><h3 id="6-2-最优化问题的数学描述"><a href="#6-2-最优化问题的数学描述" class="headerlink" title="6.2 最优化问题的数学描述"></a>6.2 最优化问题的数学描述</h3><p>最优化的基本数学模型如下：</p><p><img src="https://pic3.zhimg.com/80/v2-f35226b3e0fa018db6a4b233c51eccbe_hd.jpg" alt=""></p><p>它有三个基本要素，即：</p><p>设计变量：$x$是一个实数域范围内的$n$维向量，被称为决策变量或问题的解；<br>目标函数：$f(x)$为目标函数；<br>约束条件：$h<em>{i} \left( x \right) =0$称为等式约束，$g</em>{i} \left( x \right) \leq 0$为不等式约束，$i=0,1,2,……$</p><h3 id="6-3-凸集与凸集分离定理"><a href="#6-3-凸集与凸集分离定理" class="headerlink" title="6.3 凸集与凸集分离定理"></a>6.3 凸集与凸集分离定理</h3><h4 id="1-凸集"><a href="#1-凸集" class="headerlink" title="1.凸集"></a>1.凸集</h4><p>实数域R上（或复数C上）的向量空间中，如果集合S中任两点的连线上的点都在S内，则称集合S为凸集，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-608f89f47688c41e4c3f83cfad095c84_hd.jpg" alt=""></p><p>数学定义为：</p><p>设集合$D\subset R^{n} $，若对于任意两点$x,y\in D$，及实数$\lambda \left( 0\leq \lambda \leq 1 \right)$ 都有：   </p><script type="math/tex; mode=display">\lambda x+\left( 1-\lambda \right) y\in D</script><p>则称集合$D$为凸集。</p><h4 id="2-超平面和半空间"><a href="#2-超平面和半空间" class="headerlink" title="2.超平面和半空间"></a>2.超平面和半空间</h4><p>实际上，二维空间的超平面就是一条线（可以使曲线），三维空间的超平面就是一个面（可以是曲面）。其数学表达式如下：</p><p>超平面：$H=\left\{ x\in R^{n} |a<em>{1} +a</em>{2}+…+a_{n} =b \right\} $</p><p>半空间：$H^{+} =\left\{ x\in R^{n} |a<em>{1} +a</em>{2}+…+a_{n} \geq b \right\} $ </p><h4 id="3-凸集分离定理"><a href="#3-凸集分离定理" class="headerlink" title="3.凸集分离定理"></a>3.凸集分离定理</h4><p>所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用一张超平面将两者隔在两边，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-4116a3bda12faa5e2421ce27efb7fb71_hd.jpg" alt=""></p><h4 id="4-凸函数"><a href="#4-凸函数" class="headerlink" title="4.凸函数"></a>4.凸函数</h4><p>凸函数就是一个定义域在某个向量空间的凸子集C上的实值函数。</p><p><img src="https://pic2.zhimg.com/80/v2-f1b39d0aad4388433158679221f813d2_hd.jpg" alt=""></p><p>数学定义为： </p><p>对于函数$f(x)$，如果其定义域$C$是凸的，且对于$∀x,y∈C，0\leq \alpha \leq 1$，<br> 有：  </p><script type="math/tex; mode=display">f\left( \theta x+\left( 1-\theta \right) y \right) \leq \theta f\left( x \right) +\left( 1-\theta \right) f\left( y \right)</script><p>则$f(x)$是凸函数。</p><p>注：如果一个函数是凸函数，则其局部最优点就是它的全局最优点。这个性质在机器学习算法优化中有很重要的应用，因为机器学习模型最后就是在求某个函数的全局最优点，一旦证明该函数（机器学习里面叫“损失函数”）是凸函数，那相当于我们只用求它的局部最优点了。</p><h3 id="6-4-梯度下降算法"><a href="#6-4-梯度下降算法" class="headerlink" title="6.4 梯度下降算法"></a>6.4 梯度下降算法</h3><h4 id="1-引入"><a href="#1-引入" class="headerlink" title="1.引入"></a>1.引入</h4><p>前面讲数值计算的时候提到过，计算机在运用迭代法做数值计算（比如求解某个方程组的解）时，只要误差能够收敛，计算机最后经过一定次数的迭代后是可以给出一个跟真实解很接近的结果的。</p><p>这里进一步提出一个问题，如果我们得到的目标函数是非线性的情况下，按照哪个方向迭代求解误差的收敛速度会最快呢？</p><p>答案就是沿梯度方向。这就引入了我们的梯度下降法。</p><h4 id="2-梯度下降法"><a href="#2-梯度下降法" class="headerlink" title="2.梯度下降法"></a>2.梯度下降法</h4><p>在多元微分学中，梯度就是函数的导数方向。</p><p>梯度法是求解无约束多元函数极值最早的数值方法，很多机器学习的常用算法都是以它作为算法框架，进行改进而导出更为复杂的优化方法。</p><p>在求解目标函数f\left( x \right) 的最小值时，为求得目标函数的一个凸函数，在最优化方法中被表示为：</p><script type="math/tex; mode=display">minf\left( x \right)</script><p>根据导数的定义，函数$f\left( x \right) $的导函数就是目标函数在$x$上的变化率。在多元的情况下，目标函数$f\left( x,y,z \right) $在某点的梯度$grad f\left( x,y,z \right) =\left( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z} \right)$ 是一个由各个分量的偏导数构成的向量，负梯度方向是$f\left( x,y,z \right) $减小最快的方向。</p><p><img src="https://pic4.zhimg.com/80/v2-e61c38f10e34badf5b2c1f3b9c9bcfa0_hd.jpg" alt=""></p><p>如上图所示，当需要求$f\left( x \right) $的最小值时（机器学习中的$f\left( x \right) $一般就是损失函数，而我们的目标就是希望损失函数最小化），我们就可以先任意选取一个函数的初始点$x<em>{0} $（三维情况就是$\left( x</em>{0} ,y<em>{0} ,z</em>{0} \right)$ ），让其沿着途中红色箭头（负梯度方向）走，依次到$x<em>{1} ，x</em>{2} ，…，x_{n} $（迭代n次）这样可最快达到极小值点。</p><p>梯度下降法过程如下：</p><p>输入：目标函数$f\left( x \right) $，梯度函数$g\left( x \right) =grad f\left( x \right) $，计算精度$\varepsilon $</p><p>输出：$f\left( x \right) 的极小值点x^{*} $</p><ol><li>任取取初始值$x_{0}$ ，置$k=0$；</li><li>计算$f\left( x_{k} \right) $；</li><li>计算梯度$g<em>{k} =grad f\left( x</em>{k} \right) $，当$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $时停止迭代，令$x^{*} =x</em>{k}$ ；</li><li>否则令$P<em>{k} =-g</em>{k}$ ，求$\lambda <em>{k} 使f\left( x</em>{k+1} \right) =minf\left( x<em>{k} +\lambda </em>{k} P_{k} \right) $；</li><li>置$x<em>{k+1} =x</em>{k} +\lambda <em>{k} P</em>{k} $，计算$f\left( x<em>{k+1}\right) $，当$\left| \left| f\left( x</em>{k+1}\right) -f\left( x<em>{k}\right) \right| \right| &lt;\varepsilon 或\left| \left| x</em>{k+1} -x<em>{k} \right| \right| &lt;\varepsilon $时，停止迭代，令$x^{*} =x</em>{k+1}  $；</li><li>否则，置$k=k+1$，转$3$。</li></ol><h3 id="6-5-随机梯度下降算法"><a href="#6-5-随机梯度下降算法" class="headerlink" title="6.5 随机梯度下降算法"></a>6.5 随机梯度下降算法</h3><p>上面可以看到，在梯度下降法的迭代中，除了梯度值本身的影响外，还有每一次取的步长\lambda _{k} 也很关键：步长值取得越大，收敛速度就会越快，但是带来的可能后果就是容易越过函数的最优点，导致发散；步长取太小，算法的收敛速度又会明显降低。因此我们希望找到一种比较好的方法能够平衡步长。</p><p>随机梯度下降法并没有新的算法理论，仅仅是引进了随机样本抽取方式，并提供了一种动态步长取值策略。目的就是又要优化精度，又要满足收敛速度。</p><p>也就是说，上面的批量梯度下降法每次迭代时都会计算训练集中所有的数据，而随机梯度下降法每次迭代只是随机取了训练集中的一部分样本数据进行梯度计算，这样做最大的好处是可以避免有时候陷入局部极小值的情况（因为批量梯度下降法每次都使用全部数据，一旦到了某个局部极小值点可能就停止更新了；而随机梯度法由于每次都是随机取部分数据，所以就算局部极小值点，在下一步也还是可以跳出）</p><p>两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p><h3 id="6-6-牛顿法"><a href="#6-6-牛顿法" class="headerlink" title="6.6 牛顿法"></a>6.6 牛顿法</h3><h4 id="1-牛顿法介绍"><a href="#1-牛顿法介绍" class="headerlink" title="1.牛顿法介绍"></a>1.牛顿法介绍</h4><p>牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。</p><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。</p><p><img src="https://pic3.zhimg.com/80/v2-e22ea8c565434e945a17a80bec5630b6_hd.jpg" alt=""><br>或者从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p><h4 id="2-牛顿法的推导"><a href="#2-牛顿法的推导" class="headerlink" title="2.牛顿法的推导"></a>2.牛顿法的推导</h4><p>将目标函数$f\left( x \right)  在x_{k}$ 处进行二阶泰勒展开，可得：   </p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k} \right) +f^{'} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{''}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}</script><p>因为目标函数$f\left( x \right)$ 有极值的必要条件是在极值点处一阶导数为0，即：$f^{‘} \left( x \right) =0 $</p><p>所以对上面的展开式两边同时求导（注意$x$才是变量，$x<em>{k}$ 是常量$\Rightarrow f^{‘} \left( x</em>{k} \right) ,f^{‘’} \left( x_{k} \right)$ 都是常量），并令$f^{‘} \left( x \right) =0$可得：  </p><script type="math/tex; mode=display">f^{'} \left( x_{k} \right) +f^{''} \left( x_{k} \right) \left( x-x_{k} \right) =0</script><p>即：   </p><script type="math/tex; mode=display">x=x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>于是可以构造如下的迭代公式：</p><script type="math/tex; mode=display">x_{k+1} =x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>这样，我们就可以利用该迭代式依次产生的序列$\left\{x<em>{1},x</em>{2},…., x_{k} \right\} $才逐渐逼近$f\left( x \right)$ 的极小值点了。</p><p>牛顿法的迭代示意图如下：</p><p><img src="https://pic1.zhimg.com/80/v2-e908f9721cc82415fa7e70c763351f3a_hd.jpg" alt=""></p><p>上面讨论的是2维情况，高维情况的牛顿迭代公式是：</p><p>式中，$ ▽f是f\left( x \right)$ 的梯度，即：</p><p><img src="https://pic4.zhimg.com/80/v2-71df54a8e32e172596dcaa07e6b31899_hd.jpg" alt=""></p><p>H是Hessen矩阵，即：</p><p><img src="https://pic4.zhimg.com/80/v2-2891044fd02769c3148649e2a1a01fd5_hd.jpg" alt=""></p><h4 id="3-牛顿法的过程"><a href="#3-牛顿法的过程" class="headerlink" title="3.牛顿法的过程"></a>3.牛顿法的过程</h4><ol><li>给定初值$x_{0} $和精度阈值$\varepsilon $，并令$k=0$；</li><li>计算$x<em>{k} $和$H</em>{k}$ ；</li><li>若$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $则停止迭代；否则确定搜索方向：$d</em>{k} =-H<em>{k}^{-1} \cdot g</em>{k}$ ；</li><li>计算新的迭代点：$x<em>{k+1} =x</em>{k} +d_{k} $；</li><li>令$k=k+1$，转至$2$。</li></ol><h3 id="6-7-阻尼牛顿法"><a href="#6-7-阻尼牛顿法" class="headerlink" title="6.7 阻尼牛顿法"></a>6.7 阻尼牛顿法</h3><h4 id="1-引入-1"><a href="#1-引入-1" class="headerlink" title="1.引入"></a>1.引入</h4><p>注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现$f\left( x<em>{k+1} \right) &gt;f\left( x</em>{k} \right) $的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。</p><p>为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是$x<em>{k} $，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子$\lambda </em>{k}$ ，即：</p><p>$\lambda <em>{k} = minf\left( x</em>{k} +\lambda d_{k} \right) $</p><h4 id="2-算法过程"><a href="#2-算法过程" class="headerlink" title="2.算法过程"></a>2.算法过程</h4><ol><li>给定初值$x_{0}$ 和精度阈值$\varepsilon$ ，并令$k=0$；</li><li>计算$g<em>{k} （f\left( x \right) $在$x</em>{k}$ 处的梯度值）和$H_{k} $；</li><li>若$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $则停止迭代；否则确定搜索方向：$d</em>{k} =-H<em>{k}^{-1} \cdot g</em>{k}$ ；</li><li>利用$d<em>{k} =-H</em>{k}^{-1} \cdot g<em>{k} 得到步长\lambda </em>{k} $，并令$x<em>{k+1} =x</em>{k} +\lambda <em>{k} d</em>{k}$  </li><li>令$k=k+1$，转至$2$。</li></ol><h3 id="6-8-拟牛顿法"><a href="#6-8-拟牛顿法" class="headerlink" title="6.8 拟牛顿法"></a>6.8 拟牛顿法</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h4><p>由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵，计算量比较大（求矩阵的逆运算量比较大），因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。</p><h4 id="2-拟牛顿法的推导"><a href="#2-拟牛顿法的推导" class="headerlink" title="2.拟牛顿法的推导"></a>2.拟牛顿法的推导</h4><p>先将目标函数在$x_{k+1} $处展开，得到：  </p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k+1} \right) +f^{'} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{''}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}</script><p>两边同时取梯度，得：  </p><script type="math/tex; mode=display">f^{'}\left( x \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>取上式中的$x=x_{k}$ ，得：   </p><script type="math/tex; mode=display">f^{'}\left( x_{k} \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>即：   </p><script type="math/tex; mode=display">g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)</script><p>可得：  </p><script type="math/tex; mode=display">H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}</script><p>上面这个式子称为“拟牛顿条件”，由它来对$Hessen$矩阵做约束。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2-1创建图 启动图</title>
      <link href="/2017/12/26/2-1%E5%88%9B%E5%BB%BA%E5%9B%BE%20%E5%90%AF%E5%8A%A8%E5%9B%BE/"/>
      <url>/2017/12/26/2-1%E5%88%9B%E5%BB%BA%E5%9B%BE%20%E5%90%AF%E5%8A%A8%E5%9B%BE/</url>
      <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m1=tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">m2=tf.constant([[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">product=tf.matmul(m1,m2)</span><br><span class="line">print(product)</span><br></pre></td></tr></table></figure><pre><code>Tensor(&quot;MatMul_1:0&quot;, shape=(1, 1), dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.Session()</span><br><span class="line">result=sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result=sess.run(product)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Python学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>攻壳机动队</title>
      <link href="/2017/12/12/%E6%94%BB%E5%A3%B3%E6%9C%BA%E5%8A%A8%E9%98%9F/"/>
      <url>/2017/12/12/%E6%94%BB%E5%A3%B3%E6%9C%BA%E5%8A%A8%E9%98%9F/</url>
      <content type="html"><![CDATA[<p><img src="/2017/12/12/攻壳机动队/img12.png" alt=""><br>生死去来<br>棚头傀儡<br>一线断时<br>落落磊磊</p><p>若吾起舞时，<br>丽人亦沉醉。<br>若吾起舞时，<br>皓月亦鸣响。<br>神降合婚夜，<br>破晓虎鸫啼。<br>远神惠赐。</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>万万没想到：用理工科思维理解世界</title>
      <link href="/2017/07/30/%E4%B8%87%E4%B8%87%E6%B2%A1%E6%83%B3%E5%88%B0%EF%BC%9A%E7%94%A8%E7%90%86%E5%B7%A5%E7%A7%91%E6%80%9D%E7%BB%B4%E7%90%86%E8%A7%A3%E4%B8%96%E7%95%8C/"/>
      <url>/2017/07/30/%E4%B8%87%E4%B8%87%E6%B2%A1%E6%83%B3%E5%88%B0%EF%BC%9A%E7%94%A8%E7%90%86%E5%B7%A5%E7%A7%91%E6%80%9D%E7%BB%B4%E7%90%86%E8%A7%A3%E4%B8%96%E7%95%8C/</url>
      <content type="html"><![CDATA[<p>这是一篇读书笔记。<br><a id="more"></a></p><h2 id="《万万没想到：用理工科思维理解世界》"><a href="#《万万没想到：用理工科思维理解世界》" class="headerlink" title="《万万没想到：用理工科思维理解世界》"></a>《万万没想到：用理工科思维理解世界》</h2><ul><li>作者：万维钢（同人于野）<br><em>Unconventional wisdom</em></li></ul><h3 id="Part-one-反常识思维"><a href="#Part-one-反常识思维" class="headerlink" title="Part one 反常识思维"></a><strong>Part one 反常识思维</strong></h3><h4 id="1．“反常识”思维"><a href="#1．“反常识”思维" class="headerlink" title="1．“反常识”思维"></a>1．“反常识”思维</h4><p>当我们需要做决定的时候，我们考虑的是具体的事、具体的人和他们具体的表情。在这些具体例子的训练下，我们的潜意识早就学会了快速判断人的真诚度和事件的紧急程度。<br>这种“具体思维”做各种选择的首要标准，是<strong>道德</strong>。<br>我们首要学会的是分辨美丑。这也就是文人思维的起源，针对每个特定动作的美学评价。有时候他们管这种评价叫“价值观”，但所谓的价值观无非就是给人和事贴或好或坏的标签。<br>低端文人研究道德，高端文人研究美感。可是只有刚接触艺术的人才喜欢令人愉悦的东西，审美观成熟到一定程度后我们就觉得快乐是一种肤浅的感觉，改为欣赏愁苦了。<br>有时候文人把自己的价值观判断称为“常识”，因为这些判断本来就是从人的原始思维而来的，然而现代社会却产生了另一种思维，却是“反常识”的。<br>现代社会与古代最大的不同，是人们的生活变得越来越复杂。除了工作学习，我们还要娱乐。参加社交活动、学习和发展以及随时对遥远的公众事务发表意见。我们的每一个决定都可能以一种不直截了当的方式影响他人，然后再影响自己。面对这种复杂的局面，最基本的一个结果是好东西虽然多，你却不能都要。<br>取舍思维，英文有一个形神兼备的词“tradeoff”。它是“理工科思维”的起源。讨价还价一番后达成交易，这对文人来说是一个非常无语的情境！既不美也不丑，既不值得歌颂也不值得唾弃。斤斤计较地得到一个既谈不上实现了梦想也谈不上是悲剧的结果。完全不文艺。<br>“tradeoff”要求我们知道每一个事物的利弊。世界上并没有多少事情是“在没有使任何人境况没有变坏的前提下使得至少一个人变得更好”的所谓“帕累托改进”。<br>理工科思维要求妥协，而文人总爱不管不顾，喜欢说不惜一切代价，喜欢动不动就把全部筹码都押上去的剧情。理工科思维要求随时根据新情况调整策略。<br>《思考，快与慢》将人脑的两套思维系统称为“系统1”和“系统2”。前者自动起作用，能迅速对一个事物给出一个的很难改变的第一印象；而后者费力而缓慢，需要我们集中注意力进行复杂的计算甚至我们在系统2工作时连瞳孔都放大了。系统2根本不是计算机的对手，然而系统1却比计算机强大多了。文人思维显然是系统1的集大成者，而理工科思维则是系统2的产物。<br>“tradeoff”要求量化输入和预计输出，这也是理工思维的根本方法。但人脑天生不适应抽象数字。<br>在大多数公共问题上，常识是不好使的。资源调配即使做不到完全依赖市场，也不应该谁声音大就听谁的。<br>现在到了用理工科思维取代文人思维的时候，传统的文人腔已经越来越少出现在主流媒体上，一篇正经讨论现实的文章总要做点计算才说的过去。</p><h4 id="2-别想说服我"><a href="#2-别想说服我" class="headerlink" title="2.别想说服我"></a>2.别想说服我</h4><p>人做判断的时候有两种机制：一种是“科学家机制”，先有证据再下结论；另一种是“律师机制”，先有结论再去找证据。每个人都爱看能印证自己已有观念的东西，我们不但不爱看，而且还会直接忽略，那些不符合我们已有观念的证据。<br>这个毛病叫“确认偏误”（confirmation bias）。如果你已经开始相信一个东西了，那么你就会主动寻找能够增强这种相信的信息，乃至不顾事实。<br>在确认偏误的作用下，任何新的证据都有可能被忽略，甚至被对立的双方都用来加强自己的观点。可能有人认为只有文化程比较低的人才会陷入确认偏误，文化程度越高就越能客观判断。事实并非如此。在某些问题上，甚至是文化程度越高的人群，思想越容易两极化。<br>人们为了付出的沉没成本而不得不死命拥护自己的派别，也许就是为了表明自己的身份，也许是为了寻找一份归属感。<br>如果是两个理性而真诚的真理追求者讨论问题，争论的结果必然是这两个人达成一致。“why do humans reason?”甚至认为人的逻辑推理能力本来就不是用来追求真理的，而是用来说服别人的。也就是说我们天生就都是律师思维，我们的大脑本来就是个争论设备。这也就是因为进化总是奖励那些能说服别人的人，而不是那些能发现真理的人。<br>人人都只接收符合自己观点的信息，甚至只跟与自己志同道合的人交流，那么就会形成“回音室效应（echo chamber effect）”人们的观点将会变得越来越极端。<br>有鉴于此，约翰逊提出的核心建议是：要主动刻意地消费，吸收有可能修正我们观点的新信息，而不是吸收对我们现有观念的肯定。</p><h4 id="3-真理追求者"><a href="#3-真理追求者" class="headerlink" title="3.真理追求者"></a>3.真理追求者</h4><p>诺贝尔奖获得者罗伯特•奥曼指出，如果是两个理性而真诚的真理追求者争论问题，争论的结果必然是两人达成一致。换句话说如果争论不欢而散，那么必然有一方是虚伪的。</p><h4 id="4-坏比好重要"><a href="#4-坏比好重要" class="headerlink" title="4.坏比好重要"></a>4.坏比好重要</h4><p>损失厌恶：人们对负面感情的重视程度总是超过正面感情。心理学对这个更一般的现象也有个名词，叫“negativity bias”，姑且称之为负面偏见。<br>用核磁共振观察他们的大脑，当试验受试者说“损失”这个词的时候，他们大脑中的信任和（amygdala）兴奋了。这个区域会产生负面情绪。<br>恐惧和冒险是人的两种非常基本的感情。进化心理学认为恐惧来自人的自我保护本能，而冒险来自人的求偶本能。进化是的我们大脑中的恐惧优先级高于浪漫。<br>本能归本能，有人可以超越自己的本能。</p><h4 id="5-最简单概率论的五个智慧"><a href="#5-最简单概率论的五个智慧" class="headerlink" title="5.最简单概率论的五个智慧"></a>5.最简单概率论的五个智慧</h4><h5 id="1-随机"><a href="#1-随机" class="headerlink" title="1.随机"></a>1.随机</h5><p>概率论最基础的思想是，有些思想是无缘无故发生的。<br>大多数事情并不是完全的随机事件，却都有一定的随机因素。人们经常错误地理解偶然事件，总想用必然去解释偶然。<br>理解了随机性，我们就知道有些事情发生就发生了，没有太大可供解读的意义。<br>偶然的错误是不值得深究的，成绩也不值得深究。现代概率论的奠基人之一雅各布•伯努利，甚至认为我们根本就不应该基于一个人的成就去赞美他。用成绩评估一个人的能力，来决定是否让他入学、是否给他升职加薪，是社会的普遍做法，对此人人都服气，童叟无欺非常公平。这还有什么可说的。这还有什么可说的？问题在于，成绩很有可能有很大的偶然因素。失败者没必要要妄自菲薄，成功者也应该明白自己的成功中是有侥幸的。</p><h5 id="2-误差"><a href="#2-误差" class="headerlink" title="2.误差"></a>2.误差</h5><p>后来人们认识到偶然因素永远存在，即使实验条件再精确也无法完全避免随机干扰的影响。</p><h6 id="3-赌徒谬误"><a href="#3-赌徒谬误" class="headerlink" title="3.赌徒谬误"></a>3.赌徒谬误</h6><p>概率论中确实有一个“大数定律”说如果进行足够多次的抽奖，那么各种不同结果出现的频率就会等于它们的概率。<br>但是人们常常错误地理解随机性和大数定律——以为随机就意味着均匀。但大数定律的工作机制不是跟过去平均，它的真实意思是如果未来你再进行非常多次的抽奖，你就会得到非常多的“2”和“6”，以至于它们此前的一点点差异会变得微不足道。<br>“比如号码2已经出现了3期，而号码6已经出现了5期，则再下一次号码2再出现的概率明显大于6”，这完全错误，下一次出现2和6的概率是相等的。这就是一个著名错误“赌徒谬误”。</p><h5 id="4-在没有规律的地方发现规律"><a href="#4-在没有规律的地方发现规律" class="headerlink" title="4.在没有规律的地方发现规律"></a>4.在没有规律的地方发现规律</h5><p>理解随机性和独立随机事件，得出一个结论：独立随机事件的发生是没有规律和不可预测的。<br>未来是不可被精确预测的。这个世界并不像钟表那样运行。</p><h5 id="5-小数定律"><a href="#5-小数定律" class="headerlink" title="5.小数定律"></a>5.小数定律</h5><p>问题的关键是随机分布不等于均匀分布。人们往往认为如果是随机的，那就应该是均匀的，殊不知这一点仅在样本总数非常大的时候才有效。<br>如果统计数字很少，就很容易出现不均匀的情况。这个被戏称为“小数定律”。<br>大数定律是我们从统计数字中推测真相的理论基础。大数定律说如果统计样本足够大，那么事物出现的频率就能无限接近它的理论概率<br>小数定律说如果样本不够大，那么它就会变现为各种极端情况，而这种情况可能跟本性一点关系都没有。</p><h5 id="6-一颗阴谋论的心"><a href="#6-一颗阴谋论的心" class="headerlink" title="6.一颗阴谋论的心"></a>6.一颗阴谋论的心</h5><p>1.美国的阴谋<br>可以解释这些看似“自然”其实“不自然”的事件，我发现其背后有一个不可告人的目的。这种解读是阴谋论。<br>所有阴谋论都有一个共同的思维模式，就是不承认巧合，不承认有些事情是自然而然发生的，认为这一切都有联系、有目的。<br>2.合理性和可能性<br>想要对任何事情的真伪都给出正确的判断是不可能的，我们只能在有限的条件下合理地评估每件事的可能性。阴谋论之所以不足信，是因为其成立的可能性很低。<br>3.目的与科学<br>科学的标志，是对世界的运行给出一套纯机械的机制。<br>很多人研究为什么自然科学没有在中国发生。莫里斯（Ian Morris）在《西方将主宰多久》（Why The West Rules:For Now）说：有一个重要原因是在于中国的传统认为天道是有目的的。我们认为上天有道德观，它降下自然灾害是对皇帝的警告，或者对坏人的惩罚。<br>自然是没有目的，人类社会的很多现象往往也没有目的。<br>人在复杂的现代社会中运动，很大程度上类似于原子在电磁场中的运动，个人意愿能改变的事情很少，绝大多数人都是在随波逐流，复杂的系统也会出现非常激烈的“事件”。有人用计算机模拟发现，哪怕没有任何消息输入，仅仅是交易者之间的简单互动，也可能让股价产生很大波动。这些波动的发生并没有什么目的。每一次金融危机都会有阴谋论者站出来说这是谁谁为了某个目的故意制造的，但事实上，美联储对金融市场的控制手段非常有限。在正经的经济学家看来，把1997年亚洲金融危机归罪于索罗斯是非常可笑的事情。</p><h5 id="7-桥段会毁了你的生活"><a href="#7-桥段会毁了你的生活" class="headerlink" title="7.桥段会毁了你的生活"></a>7.桥段会毁了你的生活</h5><p>《连线》杂志的布朗（Scott Brown）在谈到“TV Tropes”时发出感慨，认为原创剧情已经消失了。其实也不至于。真正的原创剧情是高雅文学和文艺片的事情，流行文学和商业片只需要“好用的”剧情。评价严肃的作品，往往要看它是不是发明了独一无二的人物和剧情。所以严肃文学作家是科学家，通俗文学作家是工程师。</p><p>可是最好的程序员仍然可以把编程从技术上升到艺术的境界。</p><h5 id="8-健康的经济学"><a href="#8-健康的经济学" class="headerlink" title="8.健康的经济学"></a>8.健康的经济学</h5><p>如果没每加班1小时都一定能使寿命减少5分钟，恐怕就不会有这么多人加班了。但工作时间与健康并不是一个确定的关系，而是一个概率关系。<br>而且统计表明那些工作很轻松，生活无压力的人反而不如努力工作的人长寿。<br>一个选择了高风险高回报的人在健康出问题后应该愿赌服输——再给他们一次机会很可能还是这样选。</p><h5 id="9-核电站能出什么大事"><a href="#9-核电站能出什么大事" class="headerlink" title="9.核电站能出什么大事"></a>9.核电站能出什么大事</h5><p>1.核爆<br>要自爆核电站以报复人类，核电站也不会像原子弹一样爆炸。因为原材料纯度远远不够。事实上，维持核电站反应堆中的链式反应是很不容易的，以至于如果失控，链式反应会立即停止。燃料会继续变热，像日本这样需要灌水冷却，但这种变热不是链式反应，也就是说哪怕你不管了，让燃料自己变冷它也不会发生核爆。<br>核电事故的有害性在于辐射。核电站泄漏对于公众的危害是癌症。<br>2.癌症<br>3.哲学</p><h3 id="Part2-成功学的解药"><a href="#Part2-成功学的解药" class="headerlink" title="Part2 成功学的解药"></a><strong>Part2 成功学的解药</strong></h3><p>我们需要的是科学的励志，只有你的理论有意义，你的成功才可以复制。</p><h4 id="1-科学的励志和励志的科学"><a href="#1-科学的励志和励志的科学" class="headerlink" title="1.科学的励志和励志的科学"></a>1.科学的励志和励志的科学</h4><p>如今有了一些励志书，它们不再依赖名人轶事，而是借助试验和统计。这些书的理论背后都有严肃的学术论文作为依据，它们是几十年来心理学和认知科学的进步的结果。在科学家看来，乔布斯的个性管理也许不根本不值得推广，而扎克伯格的所谓天才霸业，远远比不上一群普通学生在几个月内的整体进步有研究价值。科学家，是励志领域一股拨乱反正的势力。<br>想知道什么品质对成功最重要，科学的办法不是看名人传记，而是进行大规模的统计。<br>真正能左右成绩的品质只有一个：自控。<br> 研究者普遍认为，排除智力因素，不管你心目中的成功是个人成就、家庭幸福还是人际关系，最能决定成功的只有自控。<br>自控需要意志力。意志力其实是一种生理机能。它就好像人的肌肉一样每次使用都需要消耗能量，而且用多了会疲倦。<br>意志力是一种有限资源。<br>如何提高意志力？研究者推断：人的意志力能量来自血液中的葡萄糖。 我们可以想办法合理支配这种资源，甚至像在锻炼肌肉一样增加意志力的容量。<br>除了好习惯可以减少意志力的消耗外，作者提到另外一种重要自控手段是自我监视。<br>真正有效的办法是“常立志”。意志力是一种通用资源，则意味着你可以通过做一些日常的小事来提高意志力，然后把它用在其他事情上。本书提出一个有效的练习办法是做自己不习惯做的事。<br>意志力显然不是人们喜欢自夸的能力。<br>尽管亚裔只占美国人口的4%，亚裔学生却占到斯坦福等顶级名校的25%。但统计表明同样是进入一个高智商的行业，白人需要智商110，而亚裔只需要103。<br>亚裔靠的是意志力。有实验发现，中国的小孩从两岁开始就比美国的小孩有更强的自控能力。不管什么中国文化虽然不怎么擅长科学思维，也不太明白意志力到底是什么，它却在意志力的实践上遥遥领先。</p><h4 id="2-匹夫怎样逆袭"><a href="#2-匹夫怎样逆袭" class="headerlink" title="2.匹夫怎样逆袭"></a>2.匹夫怎样逆袭</h4><p>关键在于两点：<br>第一，    你要知道你的不利条件，在某些情况下可能是你的有利条件；而巨人的所谓有利条件，在某些情况下可能是他的不利条件。<br>第二，    你绝对不能按照对手的打法去跟他玩，你有时候得使用非常规手段。<br>尼采说过，“凡不能使我毁灭的，必使我强大。”<br>既然优势和劣势可以互相转化，我们就不应该一味追求加强某一方面的优势，正所谓过犹不及。维护现有社会格局和强调遵守游戏规则，那是高富帅的事。而改变规则则是匹夫的特权。</p><h4 id="3-练习一万小时成天才？"><a href="#3-练习一万小时成天才？" class="headerlink" title="3.练习一万小时成天才？"></a>3.练习一万小时成天才？</h4><p>强调练习的同时绝对不能否定天赋的重要性。真正的关键根本不是训练时间的长短，而是训练方法。<br>练习，讲究的并不是谁练的最苦，或者谁的心最“诚”。<br>坏消息是高水平训练的成本很高。你需要一位掌握这个领域的先进知识的最好的教练，你需要一个有助于你提高能力的外部环境——这通常意味着加入一所好大学或者入选一个好的俱乐部，你要忍受一点都不舒服的训练方法，而且你需要投入非常多的时间。<br>好消息是各个领域的不同训练方法也都存在着一些共同特征。<br>科学的练习方法并不是从天而降的神秘招式，它一定程度上已经存在于我们的生活之中。它不是科学家的发明，而是科学家对各个领域高手训练方法的总结。人们一直在各个领域中不自觉地使用这些方法。<br>这一套练习方法，就是“刻意练习（deliberate practice）”。<br>去除一些不重要的，总结如下：</p><ol><li>只在“学习区”练习；</li><li>把要训练的内容分成有针对性的小块，对每一小块进行重复训练；</li><li>在整个练习过程中，随时能获得有效反馈；</li><li>练习时注意力必须高度集中。</li></ol><p>几点说明：许多人把困难事情干成认为是靠干事业人某种“内在”品质。比如归于拼搏精神，可是如果是怎么也干不成的事业，归于中国人的素质，作者不能赞同这种凡是往特别简单或者特别复杂了说的思维。首先，干事业不是靠拼命就行，其次，干事业就是干事业，没必要先把官场文化和春秋以来的儒家思想都研究、批判和改造一遍。<br>过分强调“功夫在诗外”这句陆游名言也是不对的。<br>想要成为某一领域的顶尖高手，关键在于“刻意”地在这个领域内，练习。</p><ol><li><p>只在“学习区”学习<br>心理学家把人的知识和技能分为层层嵌套的三个圆形区域：最内一层是“舒适区”，使我们已经熟练掌握的各种技能；最外一层是“恐慌区”，是我们暂时无法学会的技能，二者中间则是“学习区”。<br>有效的练习任务必须在受训者的学习区内进行，它具有高度的针对性。训练者必须随时了解自己最需要改进的地方。一旦学会某个东西，就不应该继续在上面花费时间，应该立即转入下一个困难点。<br>在舒适区做事，叫生活；在学习区做事，才叫练习。<br>脱离舒适区，需要强大的意志力，甚至是一种修炼。<br>每天都给自己定一个更远的目标。</p></li><li><p>掌握套路<br>刻意练习的基础部分：基础训练。也就是套路。<br>人所掌握的知识和技能绝非是零散的信息和随意的动作，它们大多数具有某种“结构”，这些“结构”就是套路。下棋用的定式，编程用的固定算法，这些都是套路。<br>心理学认为人的工作能力主要依靠两种记忆了：“短期记忆力”（short term working memory）和长期记忆力（long term working memory）短期工作记忆类似于电脑内存，是指人脑在同一时刻能够处理的事情的个数——一般来说四个，它与逻辑推理能力、创造性思维有关，也就是智商很有关系，它很难通过训练得到。<br>长期工作记忆储存了我们的知识和技能。它有点类似于计算机硬盘，但比硬盘高级多了。关键在于，长期工作记忆并非是杂乱无章。随便储存的，它是以神经网络的形式运作，必须通过训练才能储存，而且具有高度的结构性。心理学家把这种结构称为“块”（chunk）。<br>人的技能，取决于这两种工作记忆。专家做的事，就是使用有限的短期工作记忆，去调动自己几乎无限的长期工作记忆。而刻意练习，就是在大脑中建立长期工作记忆的过程。</p></li></ol><p>两种套路<br>对于脑力工作者，水平的高低关键要看掌握的套路有多少。<br>以量取胜的套路是容易掌握的。<br>但有些套路，比如那些非纯脑力劳动的非专业技能，想要掌握就没那么容易了。<br>人脑是如何掌握一个技能的，一个比较主流的结论是说这是神经元的作用。完成一个动作需要激发很多神经元，如果这个动作被反复做，那么这些神经元会被反复地一起激发。而神经元有个特点，就是如果经常被一起激发，它们最终会连在一起！！因为每个特定技能需要调动的神经元不同，不同技能在人的大脑就形成了不同的网络结构。另有一个理论则认为神经元的连接必然重要，但更重要的则是包裹在神经元伸出去的神经纤维（轴突）外面的一层髓磷脂组成的膜：髓鞘。如果我们把神经元想象成元器件，那么神经纤维就是连接元器件的导向，而髓鞘则相当与包在导线外面的胶皮、这样用胶皮把电线包起来防止电脉冲外泄，能够使得信号被传输地更强，更快，更准确。当我们正确地练习时，髓鞘就会越包越厚，每多一层就意味着更高的准确度和更快的速度。髓鞘，把小道变成告诉公路。<br>不论是哪种理论，最后我们都可以得出这样的结论：技能是人脑中的一种硬件结构，是“长”在人脑中的。这意味着如果你能打开大脑，你就会发现每个人脑中的神经网络结构都不一样。技能很不容易获得，一旦获得了也很难抹掉。<br>如此一来，高手与普通人就有了本质的区别。高手拥有长期训练获得的特殊神经脑结构，他的一举一动可能都带着不一般气质，连眼神都与众不同，简直是用特殊材料制成的人。练习，是对人体的改造。<br>用什么方法才能迅速把技能套路“长”在身上呢？关键在于两点：</p><ol><li>必须进行大量的重复训练；</li><li>训练必须有高度针对性。</li></ol><ul><li>基本功<br>磨刀不误砍柴工，基本功就是这么重要。不但体育和音乐需要练基本功，就连那些人们认为不存在基本功的领域，也要练基本功。</li><li>重复！重复！再重复！<br>  想把一个动作套路，一个技能，哪怕仅仅是一个生活习惯，甚至是一种心态，“长”在大脑之中，唯一的办法是不断重复。<br>  这种把不常见的高难度事件重复化也是MBA课程的精髓</li><li>高度针对性<br>  想要掌握一项技能，要像运动员一样，需要不停地练习实战动作，不停地比赛，而不是不停地看录像。</li><li>随时获得反馈<br>  从刻意练习角度，这就是即时的反馈（immediate feedback）在有即时反馈的情况下，一个人的进步速度非常之快，而且是实实在在的。</li><li>一定要有反馈<br>  想要真正的理解，唯一的办法是考试和测验。这就是反馈！没有测验，你的知识只是幻觉。</li><li>立即反馈<br>  老师的作用<br>  现代老师的最大作用是什么？及时反馈。一个动作好不好，最好有教练随时指出，本人必须能够随时了解练习结果。看不到结果的练习等于没有练习。在某种程度上，刻意练习是以错误为中心的练习。练习者必须要对错误及其敏感，一旦发现自己错了就会感到非常不舒服，一直练习到改正为止。</li><li>学徒制<br>  作者认为真正的人才不是靠课程、院系、考试大纲培养出来的。培养人才的有效办法只有一个，那就是学徒制。</li><li>刻意练习不好玩<br>  2005年，“刻意练习”概念的提出者埃里克森（K.Anders Ericeeon）领导的小组表明，决定性因素不是学习时间，而是学习环境。<br>  研究人员发现，排除以往成绩的话，只有一个因素能预测他成绩的变化，就是学习环境。</li><li>单独练习<br>  刻意练习需要练习者调动大量的身体和精神资源，全力投入。<br>  研究人员发现，所有学生都了解一个道理：真正决定你水平的不是全班一起上的音乐课，而是单独练习。<br>  所以我们再次发现所谓的“一万小时”实在是个误导人的概念。练习时间长短并不是最重要的，真正关键是你“刻意练习”——哪怕仅仅是“单独练习”——的时间。可见要想成为世界级高手，一定要尽早投入训练，这就是为什么天才音乐学家都是从很小的时候就开始苦练。</li><li>练习与娱乐<br>  在刻意练习中没有“寓教于乐”这个概念。<br>  如果你想学点知识，最好的办法是找本书——最好是正规的教科书或者专业著作——然后老老实实地找个没有人的地方坐下反复读，而且还有自己整理笔记，甚至做习题获得反馈。</li><li>练习需要重复，而重复一定不好玩。<br>  谁愿意练习一万小时？<br>  刻意练习需要学习者精神高度集中，是一种非常艰苦的练习，人的精力只能做这么多。<br>  世界就是属于这极少数人的。世界并不需要一千个钢琴大师或者一万个足球明星，这些少数的幸运儿已经把所有位置都占满了。如果你想要享受快乐童年，你的位置在观众席。<br>  刻意练习不好玩。伟大的成就需要放弃很多很多东西，而这种放弃并不是没有争议的。<br>  我不知道虎妈的育儿法是否对整个社会有力，但我相信虎妈一定明白一个道理：如果你想出类拔萃，那么你要参与的这场竞争很大程度上是个零和博弈——你想赢就意味着有人要输，你拿到这个位置就意味着有人拿不到这个位置。像这种博弈对社会有没有好处对你不重要，你关心的是怎么做对自己有好处。这个博弈没有双赢。<br>  这不是一般人玩的起的游戏。</li><li>孤注一掷<br>  体育、音乐和表演，都是高投入高风险的事情，明星的背后是无数个失败的垫背。想要成功，就得练习一万小时，但考虑到机遇因素，即使你练了这一万小时也未必成功，这其实是一场赌博。<br>  下这么大赌注练习，绝对不仅仅是为了博女朋友一笑，与之对等的回报是整个世界的认可。高水平的运动员有一个共同特点：他们非常，非常，非常非常想赢得比赛。<br>  真正使得乔丹成为巨星的“素质”，是对失败的痛恨。</li><li>奖励机制<br>  一般人当然用不着孤注一掷地刻意练习，但还是需要一点刺激才能练下去，因为只要是有用的练习都好玩。<br>  一个可能的结论似乎是奖励学习过程，比只看学习的结果效果更好。</li><li>兴趣和基因<br>  学习一个技能的初期，智商可能是决定性因素。但是随着学习的深入，兴趣的作用可能就越来越大了，因为兴趣可以相当的大的程度上决定谁能坚持下来。<br>  决定一个学生进步的幅度的不是智商，而是内在动力和学习方法。<br>  科学界的共识是，先天因素远远大于后天因素。<br>  寻找适合自己兴趣的环境，把自己的基因发扬光大——这难道不就是进化论告诉我们的人生的意义吗？</li></ul><h4 id="4-最高级的想象力是不自由的"><a href="#4-最高级的想象力是不自由的" class="headerlink" title="4.最高级的想象力是不自由的"></a>4.最高级的想象力是不自由的</h4><p>所以，最高级的想象力其实是不自由的。正是因为不自由，它的难度才大。自由的“what if”思维，只是高级想象力活动的第一步，其背后不自由的东西才是关键。</p><h4 id="5-思维密集度与牛人的反击"><a href="#5-思维密集度与牛人的反击" class="headerlink" title="5.思维密集度与牛人的反击"></a>5.思维密集度与牛人的反击</h4><p>“思维密集度”=准备这个读物需要的总时间/阅读这个读物需要的时间</p><h4 id="5-上网能避免浅薄么？"><a href="#5-上网能避免浅薄么？" class="headerlink" title="5.上网能避免浅薄么？"></a>5.上网能避免浅薄么？</h4><p>只有有意识的短期记忆，称为工作记忆，才有可能被转化为长期记忆。过去心理学家曾经认为人的工作记忆只能同时容纳7条信息，而最新的研究成果是最多只有2~4条。这样有限的容量非常容易被无关信息干扰导致过载。上网时分散的注意力，不停地为点还是不点做决定，都在阻碍我们把短期记忆升级为知识。<br>上网的关键态度是要成为网络的主人，而不做各种超链接的奴隶。高效率的上网应该像自闭症患者一样具有很强的目的性，以我为主，不被无关信息左右。就算是纯粹为了娱乐上网也无可厚非，这时候读的快就是优点。</p><h4 id="6-高效“冲浪”的办法"><a href="#6-高效“冲浪”的办法" class="headerlink" title="6.高效“冲浪”的办法"></a>6.高效“冲浪”的办法</h4><p>第一步，随便翻翻（toss）<br>效率的首要关键是集中。我们先做的不是读新闻，而是挑选新闻。<br>第二步，略读（skim）<br>第三步，精读（read）<br>这种先集中选择再采取行动的办法并不仅限于新闻阅读，其实有更为广泛的应用。诺贝尔经济学奖得主丹尼尔•卡尼曼在《思考，快与慢》一书中介绍了两个著名的心理学概念：“窄框架”（narrow framing）和“宽框架”（broad framing）。所谓窄框架，就是遇到一个东西做一次决策，一事一议；而宽框架则是把所有东西都摆在桌面上集中选择。</p><h4 id="7-笔记本就是力量"><a href="#7-笔记本就是力量" class="headerlink" title="7.笔记本就是力量"></a>7.笔记本就是力量</h4><p>记笔记，是一个被动的行为。你现在甚至不用自己记，软件的工具可以让你在几次点击之内记录下任何需要记下的信息。但真正的好笔记确实主动的，它不仅仅是对客观事物的记录，更是对自己思想的记录。<br>哪怕有一天维基百科、百度知道再加上人工智能可以向我们提供所有问题的答案，它们仍然不能取代人脑中的真正知识。这是因为真正的知识是分层的。你必须完全理解基础的一层，才能谈得上去看懂上面的一层。如果你没学过微积分，就算有人把人类历史上广义相对论的全部文本摆在你面前也没用。知识，不能仅仅机械地“存”在你脑子里，而必须以一种个性化的结构“长”在你的脑子里。通过个人笔记本来不断总结自己个性化的理解。恰恰可以帮助我们“长”知识。<br>理解知识需要笔记，使用知识也需要笔记。<br>真正的专家，都有自己一整套知识体系。这套体系就如同长在他们心中的一颗不断生枝长叶的树，又如同一张随时变大变复杂的网。每当有新的知识进来，他们都知道该把这个知识放到体系的什么位置上去。有人管这套体系叫做心智模式（mental model），有人管它叫矩阵（matrix）。有了这套体系，你才可能对相关事务作出出神入化的“眨眼判断”，而不是靠什么“灵感”或者“直觉”。<br>所以，记笔记的最直接的目的是为了形成自己的知识体系，改变自己看事物的眼光。<br>笔记系统的一个附带好处是它可以帮助我们把新的知识跟自己已有的知识联系起来。一般人善于发现新事物的不同点，而真正的高手则善于发现共同点。一旦发现新的知识和已有知识的共同点，这个知识就彻底“长”在我们身上了。而且这样带来的类比和联想，特别能刺激创造性思维。只有你把它们全部拆开、撕碎，再重新组合成你的东西，它们才真正属于你。<br>我们要做的就是“吃进”很多信息，然后生产笔记本。</p><p><strong>用强力读书</strong></p><p>读书却可以及大幅度地提升人的思想内力，这种内力是对世界的理解和见识。<br>《如何阅读一本书》这本书提出一种精神：为了娱乐而读，为了信息而读和为了理解而读。首先，只有为了理解某个我们原来不懂的东西而读书，才值得认真对待。其次，读书应该以我为主，而不是以书为主。<br>强力阅读<br>强力阅读更像是一种态度和心法。作者更想起名叫Deep Reading。<br>“强力阅读”并不是为了读《广义相对论》之类的专业著作，它面向的对象是《卧底经济学》之类写给非专业读者的非小说类书籍。称为“强力”，是因为它追求阅读的深度和效率，力图能在一本书中挖掘到最大限度的收获。<br>强力阅读跟“刻意练习”有三个共同点：<br>第一，    不好玩。用非常严肃认真的态度，非得把一本书融会贯通以至于“长”在自己的大脑里不可。<br>第二，    用时少。就如同在那种专门培养天才的最好的音乐学校里，孩子们每天真正练琴的时间绝对不超过2个小时一样。没人能长时间坚持那样的强度，而没有强度•的训练还不如不练。要把精力充沛而又不受打扰的时间段留给最好的书。<br>第三，    不追求块。读书的一个关键技术在于对于不同的读物采取不同的阅读速度。</p><p>以下是强力阅读的具体做法，它的核心技术是读书笔记。<br>新书要读两遍<br>一本书应该被读两遍，而且只读两遍，我们说的思想类书籍，不是什么学术著作。而且最优效率的办法是读完一遍马上再读一遍。<br>第一遍是正常通读，只要放松欣赏作者的精妙思想和有趣故事即可。<br>在读第二遍的同时写下读书笔记。要专注于思想脉络。读一章，记一章笔记。<br>什么是好的读书笔记？<br>强力研读要求读书笔记必须包括四方面内容：</p><ol><li>清晰地表现每一章的逻辑脉络</li><li>带走书中所有的亮点</li><li>有大量的自己的看法心得</li><li>发现这本书和以前读过的其他书或文章的联系<br>重要的是一定要看出作者的逻辑脉络。<br>读书笔记的第一作用就是抛开故事记住文章。让一本书从厚变薄，从具体的山川景色变成抽象的地图。只有当你跳出字里行间，以居高临下的姿态俯视全书，它的脉络才能变得清晰。<br>读书，在某种程度上就是在寻找能够刺激自己思维的那些亮点。强力研读是一种主动的读书方法。要在笔记中写下自己对此书的评论，好像跟作者对话一样。<br>你不可能对说的好的一段话无动于衷。你可以写下自己对这件事的理解，你还可以写下对作者的质疑或肯定。更高级的批注则是写下自己因为这段文字而产生的灵感。一本好书的每一章都能让人迸发出十个以上的灵感。也许它就解决了你之前一直关注的问题——尽管这个问题看似与此书无关；也许你会想把作者的理论往前推一步。这些想法未必有用，但是都非常宝贵，因为如果你不马上记下来，它们很快就会忘记。也许多年以后翻阅笔记的时候你会觉得自己的心得灵感比原书更有价值。</li></ol><p>如果你读的足够多，你会获得一种更难得的经历：感受人类知识的进步。<br>好书之所以读两遍，最重要的目的就是为了获得这些心得、灵感和联系。第一遍是为了陷进去，第二遍是为了跳出来。<br>记笔记是对一本书最大的敬意。读书笔记是一种非常个性化的写作，是个人知识的延伸。它不是书评，它完全是写给自己而不是为了公开发表——可以完全专注于意思，而不必关心文章。<br>所以“眼过千遍不如手过一遍”【心过一遍】，而且用思维导图做笔记是真的没用。</p><p>电子书<br>读书人的武功<br>强力研读要求慢读，但是我们知道很多著名的读书人的读书速度却都很快，这是为什么呢？这就是武功。他们读的快，是因为对他们来说一般的书里，新的东西已经非常有限。<br>比尔盖茨、查理芒格、沃伦巴菲特</p><p>【下面的文章有时间再写，缓慢一段时间】<br>创新是落后者的特权：三个竞争故事<br>————————————————</p>]]></content>
      
      <categories>
          
          <category> Essay </category>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>星际穿越出现的诗</title>
      <link href="/2017/03/22/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E5%B0%8F%E8%AF%97/"/>
      <url>/2017/03/22/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E5%B0%8F%E8%AF%97/</url>
      <content type="html"><![CDATA[<p><strong>Do not go gentle into that good night</strong><br>-Dylan Thomas </p><p><em>Do not go gentle into that good night,<br>Old age should burn and rave at close of day;  　　<br>Rage, rage against the dying of the light.  　　  　　<br>Though wise men at their end know dark is right,  　　<br>Because their words had forked no lightning they  　　<br>Do not go gentle into that good night.  　　  　　<br>Good men, the last wave by, crying how bright  　　<br>Their frail deeds might have danced in a green bay,  　　<br>Rage, rage against the dying of the light.  　　  　　<br>Wild men who caught and sang the sun in flight,  　　<br>And learn, too late, they grieved it on its way,  　　<br>Do not go gentle into that good night.  　　  　　<br>Grave men, near death, who see with blinding sight  　　<br>Blind eyes could blaze like meteors and be gay,  　　<br>Rage, rage against the dying of the light.  　　  　　<br>And you, my father, there on the sad height,  　　<br>Curse, bless, me now with your fierce tears.,I pray.<br>Do not go gentle into that good night,    　<br>Rage, rage against the dying of the light. </em></p><p>《不要温和地走进那个良夜》</p><p>狄兰·托马斯</p><p>不要温和地走进那个良夜，<br>老年应当在日暮时燃烧咆哮；<br>怒斥，怒斥光明的消逝。</p><p>虽然智慧的人临终时懂得黑暗有理，<br>因为他们的话没有迸发出闪电，他们<br>也并不温和地走进那个良夜。</p><p>善良的人，当最后一浪过去，高呼他们脆弱的善行<br>可能曾会多么光辉地在绿色的海湾里舞蹈，<br>怒斥，怒斥光明的消逝。</p><p>狂暴的人抓住并歌唱过翱翔的太阳，<br>懂得，但为时太晚，他们使太阳在途中悲伤，<br>也并不温和地走进那个良夜。</p><p>严肃的人，接近死亡，用炫目的视觉看出<br>失明的眼睛可以像流星一样闪耀欢欣，<br>怒斥，怒斥光明的消逝。</p><p>你啊，我的父亲。在那悲哀的高处。<br>现在用您的热泪诅咒我，祝福我吧。我求您<br>不要温和地走进那个良夜。<br>怒斥，怒斥光明的消逝。</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>New test</title>
      <link href="/2016/04/25/New%20test/"/>
      <url>/2016/04/25/New%20test/</url>
      <content type="html"><![CDATA[<p>fefefefe<br>abcd这也是一篇测试文章</p><p>假设X是一个离散随机变量，其可能的取值有：$\left\{ x<em>1 ,x_2 ,……,x</em>{n} \right\}$，各个取值对应的概率取值为：$P\left( x_k \right) , k=1,2,……,n $，则其数学期望被定义为：</p><p>$\sum=er_dew11$</p><p>$\{ x_1 ,x_2 ,……,x_n \}$<br>efefewfwefwefwefweefe<br>$P( x_k) , k=1,2,……,n $</p><script type="math/tex; mode=display">efewfw\frac{1}{trhtr7}</script><p><img src="/2016/04/25/New test/img8.jpg" alt=""></p>]]></content>
      
      
        <tags>
            
            <tag> 主题测试 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
