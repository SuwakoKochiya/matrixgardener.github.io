<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>欢迎来到我的博客</title>
      <link href="/2018/07/30/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/07/30/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>这里写摘要。test<br><a id="more"></a></p><p>这是我的第一篇博文，测试用。</p>]]></content>
      
      <categories>
          
          <category> Sports </category>
          
          <category> Baseball </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 主题测试 </tag>
            
            <tag> Injury </tag>
            
            <tag> Fight </tag>
            
            <tag> Shocking </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/07/30/hello-world/"/>
      <url>/2018/07/30/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>《七堂极简物理课》</title>
      <link href="/2018/02/16/%E4%B8%83%E5%A0%82%E6%9E%81%E7%AE%80%E7%89%A9%E7%90%86%E8%AF%BE/"/>
      <url>/2018/02/16/%E4%B8%83%E5%A0%82%E6%9E%81%E7%AE%80%E7%89%A9%E7%90%86%E8%AF%BE/</url>
      <content type="html"><![CDATA[<p>《七堂极简物理课》</p><p>第一课：最美的理论</p><p>爱因斯坦 广义相对论 </p><p>“牛顿试图解释物体下落和行星运转的原因。他假设在万物之间存在一种相互吸引的“力量”，他称之为“引力”。那么这个力是如何牵引两个相距甚远，中间又空无一物的物体的呢？这位伟大的现代科学之父对此显得谨慎小心，未敢大胆提出假设。牛顿想象物体是在空间中运动的，他认为空间是一个巨大的空容器，一个能装下宇宙的大盒子，也是一个硕大无朋的框架，所有物体都在其中做直线运动，直到有一个力使它们的轨道发生弯曲。至于“空间”，或者说牛顿想象的这个可以容纳世界的容器是由什么做成的，牛顿也没有给出答案。就在爱因斯坦出生前的几年，英国的两位大物理学家——法拉第（ Michael Faraday）和麦克斯韦（ James Maxwell）——为牛顿冰冷的世界添加了新鲜的内容：电磁场。所谓“电磁场”，是一种无处不在的真实存在，它可以传递无线电波，可以布满整个空间；它可以振动，也可以波动，就像起伏的湖面一样；它还可以将电力“四处传播”。爱因斯坦……很快他想到，就像电力一样，引力一定也是由一种场来传播的，一定存在一种类似于“电场”的“引力场”。他想弄明白这个“引力场”是如何运作的，以及怎样用方程对其进行描述。就在这时，他灵光一闪，想到了一个非同凡响的点子，一个百分百天才的想法：引力场不“弥漫”于空间，因为它本身就是空间。这就是广义相对论的思想。其实，牛顿的那个承载物体运动的“空间”与“引力场”是同一个东西。这是一个惊世骇俗的理论，对宇宙做了惊人的简化：空间不再是一种有别于物质的东西，而是构成世界的“物质”成分之一，一种可以波动、弯曲、变形的实体。我们不再身处一个看不见的坚硬框架里，而更像是深陷在一个巨大的容易形变的软体动物中。太阳会使其周围的空间发生弯曲，所以地球并不是在某种神秘力量的牵引下绕着太阳旋转，而是在一个倾斜的空间中行进，就好像弹珠在漏斗中滚动一样：漏斗中心并不会产生什么神秘的“力量”，是弯曲的漏斗壁使弹珠滚动的。所以无论是行星绕着太阳转，还是物体下落，都是因为空间发生了弯曲。那么我们该如何描述这种空间的弯曲呢？ 19世纪最伟大的数学家、“数学王子”卡尔·弗里德里希·高斯（ Carl Friedrich Gauss）已经写出了描述二维曲面（比如小山丘的表面）的公式。他还让自己的得意门生将这一理论推广到三维乃至更高维的曲面。这位学生就是波恩哈德·黎曼（ Bernhard Riemann），他就此问题写了一篇重量级的博士论文，但当时看起来全然无用。黎曼论文的结论是，任何一个弯曲空间的特征都可以用一个数学量来描述，如今我们称之为“黎曼曲率”，用大写的“ R”来表示。后来爱因斯坦也写了一个方程，将这个 R与物质的能量等价起来，也就是说：空间在有物质的地方会发生弯曲。就这么简单。这个方程只有半行的长度，仅此而已。空间弯曲这个观点，现在变成了一个方程。然而，这个方程中却蕴含着一个光彩夺目的宇宙。首先，这个方程描述了空间如何在恒星周围发生弯曲。由于这个弯曲，不仅行星要在轨道上绕着恒星转，就连光也发生了偏折，不再走直线。爱因斯坦预测，太阳会使光线偏折。在 1919年，这个偏折被测量出来，从而证实了他的这一预测。其实不仅是空间，时间也同样会发生弯曲。爱因斯坦曾预言，在高空中，在离太阳更近的地方，时间会过得比较快，而在低的地方，离地球近的地方时间则过得比较慢。这一预测后来也经测量得到了证实。</p><p>当一个大恒星燃烧完自己所有的燃料（氢）时，它就会熄灭。残留的部分因为没有燃烧产生的热量的支撑，会因为自身的重量而坍塌，导致空间强烈弯曲，最终塌陷成一个真真正正的洞。这就是著名的“黑洞”。……整个宇宙空间可以膨胀和收缩。爱因斯坦的方程还指出，空间不可能一直保持静止，它一定是在不断膨胀的。……这个方程还预测，这个膨胀是由一个极小、极热的年轻宇宙的爆炸引发的：这就是我们所说的“宇宙大爆炸”。……但大量证据纷纷出现在我们眼前，直至在太空中观测到了“宇宙背景辐射”，也就是原始爆炸的余热里弥漫的光。事实证明，爱因斯坦方程的预言是正确的。此外，这个理论还说，空间会像海平面一样起伏，目前人们已经在宇宙中的双星上观测到了“引力波”的这种效应，与爱因斯坦理论的预言惊人一致，精确到了千亿分之一。……所有这一切都源自一个朴素的直觉，那就是，空间和引力场本是一回事。这一切也可以归结为一个简洁的方程，……”</p><p>第二课 量子</p><p>量子力学诞生于1900年，……德国物理学家马克思·普朗克（Max Planck）计算了一个“热匣子”内处于平衡态的电磁场。为此他用了一个巧妙的办法：假设电磁场的能量都分布在一个个“量子”上，也就是说能量是一包一包或一块一块的。用这个方法算出的结果与测量得到的数据完全吻合（所以应该算是正确的），但却与当时人们的认知背道而驰，因为人们认为能量是连续变动的，硬把它说是由一堆“碎砖块”构成的，简直是无稽之谈。</p><p>对于普朗克来说，把能量视为一个个能量包块的集合只是计算上使用的一个特殊策略，就连他自己也不明白为什么这种方法会奏效。然而五年后，有事爱因斯坦，终于认识到这些“能量包”是真是存在的。</p><p>爱因斯坦指出光是由成包的光粒子构成的，今天我们称之为“光子”。他在那篇文章的引言中写道：“在我看来，如果我们假设光的能量在空间中的分布是不连续的，我们就能更好地理解有关黑体辐射、荧光、紫外线产生的阴极射线，以及有关其他有关光的发射和转化的现象。依据这个假设，点光源发射出的一束光线的能量，并不会在越来越广的空间中连续分布，而是由有限数目的‘能量量子’组成，它们在空间中点状分布，作为能量发射和吸收的最小单元，能量量子不可再分。”</p><p>20世纪10-20年代，丹麦人尼尔斯·波尔（Dane Niels Bohr）引领了这一理论的发展，他了解到原子核内电子的能量跟光能一样，只是特定值，而更重要的是，电子只有在特定的能量之下才能从一个原子轨道“跳跃”到另一个原子轨道上，并同时释放或吸收一个光子，这就是著名的“量子跃迁”。 </p><p>1925年，量子理论的方程终于出现了，取代整个牛顿力学。</p><p>率先为这个新理论列出方程的是一个非常年轻的德国天才——维尔纳·海森堡（Werner Heisenberg），他所依据的理念简直让人晕头转向。</p><p>海森堡想象电子并非一只存在，只有人看到它们的时，或者更确切的说，只有和其他东西相互作用时它们才会存在。当它们与其他东西相撞时，就会以一个可计算的概率在某个地方出现。从一个轨道到另一个轨道的“量子跃迁”是它们现身的唯一方式：一个电子就是相互作用下的一连串跳跃。如果么有受到打扰，电子就没有固定的栖身之所，它甚至不会存在于一个所谓的“地方”。</p><p>在量子力学中，没有一样东西拥有确定的位置，除非它撞上了别的东西。为了描述电子从一种相互作用到另一个相互作用的飞跃，就要借助一个抽象的公式，它只存在于抽象的数学空间，而不存在于真实空间。</p><p>更糟的是，这些从一处到另一处的飞跃大多是随机的，不可预测的。我们无法预知一个电子再次出现会是在哪儿，只能计算它出现在这里或那里的“概率”。这个概率问题直捣物理的核心，可原本物理学的一切问题都是被那些普遍且不可改变的铁律所控制的。</p><p>一个世纪过去了，我们还停在原点。量子力学的方程以及用它们得出的结果每天都被应用于物理、工程、化学、生物乃至更广阔的领域中。量子力学对于当代科技的整体发展有着至关重要的意义。没有量子力学就不会出现晶体管。然而这些方程仍然十分神秘，因为它们并不描述在一个物理系统内发生了什么，而只说明一个物理系统是如何影响另外一个物理系统的。</p><p>第三课 宇宙的构造</p><p>20世纪上半叶，爱因斯坦用相对论描述了空间和时间的运作方式，而波尔和他年轻的门徒们则用一系列方程捕捉到了物质奇怪的量子特性。20世纪下半叶，物理学家们在此基础上，把这两个新理论广泛应用在了自然界的各个领域：从宏观世界的宇宙构造，到微观世界的基本粒子。</p><p>哥白尼的日心说</p><p>太阳系只是不计其数的星系中的一个，而我们的太阳也只是众多恒星中普普通通的一颗，是浩瀚银河系星云中的沧海一粟。</p><p>但是在20世纪30年代，天文学家对星云（恒星之间近乎白色的云团）进行精确的测量后发现，银河系本身也只是众多星系间浩瀚星云中的一粒尘埃。这些星系一只蔓延到我们最强大的天文望远镜也看不到的地方。</p><p>这片均匀无边的宇宙并不像看上去那么简单。就像我在第一节课中解释过的那样，空间不是一马平川，而是弯曲的。宇宙布满了星系，所以我们想象它的纹理会像海浪一样起伏，激烈处还会产生黑洞空穴。</p><p>我们今天终于知道，这个布满星系、富有弹性的浩瀚宇宙是大约150亿年前由一个极热极密的小星云演化来的。</p><p>宇宙的诞生的时候就像一个小球，大爆炸后一直膨胀到它现在的规模。这就是我们现在对宇宙最大程度的了解了。</p><p>第四课 粒子</p><p>我们身边所有物体都是由电子、夸克、光子和胶子组成的。它们就是粒子物理学中所讲的“基本粒子”。除此之外还有几种粒子，例如中微子（neutrino）——它布满整个宇宙，但并不跟我们发生交互作用，还有希格斯玻色子（Higgs boson）——不久前日内瓦欧洲核子研究中心的大型强子对撞机发现的粒子。但这些粒子并不多，只有不到十种。这些少量的基本原料，如同大型乐高玩具中的小积木，靠它们建造出了我们身边的整个物质世界。</p><p>量子力学描述了这些粒子的性质和运动方式。这些粒子当然并不像小石子那般真实可感，而是相应的场的“量子”，比方说光子是电磁场的“量子”。就跟在法拉第和麦克斯韦的电磁场中一样，它们是这些变化的基底场的元激发，是极小的移动的波包。它们的消失和重现遵循量子力学的奇特定律：存在的每样东西都是不稳定的，永远都在从一种相互作用跃迁到另一种相互作用。</p><p>即使我们观察的是空间中一块没有原子的区域，还是可以探测到粒子的微小涌动。彻底的虚空是不存在，就像最平静的海面，我们凑近看还是会发现细微的波动和振荡。构成世界的各种场也会轻微的波动起伏，我们可以想象，组成世界的基本粒子在这样的波动中不断产生、消失。</p><p>这就是量子力学和粒子理论描述的世界。这同牛顿和拉普拉斯（Laplace）的世界相去甚远：在那里，冰冷的小石子在不变的集合空间里沿着精确而漫长的轨迹永恒不变地运动着。量子力学和粒子试验告诉我们，世界是物体连续、永不停歇的涌动，是稍纵即逝的实体不断出现和消失，是一系列的振荡，就像20世界60年代时髦的嬉皮时代，一个由事件而非物体构成的世界。</p><p>第五课 空间的颗粒</p><p>“圈量子引力”试图将广义相对论和量子力学统一起来。</p><p>它的中心思想很简单。广义相对论告诉我们空间不是一个静止的盒子，而是在不断运动，像一个移动中的巨大软体动物，可以压缩和扭曲，而我们被包在里面。另一方面，量子力学告诉我们，所有这样的场都“由量子构成”，具有精细的颗粒状结构。于是物理空间当然也是“由量子构成的”。</p><p>这正是圈量子引力的核心结论：空间是不连续的，不可被无限分割，而是由细小的颗粒，或者说“空间原子”构成。这些颗粒极其微小，比最小的原子核还要小几亿亿倍。圈量子引力用数学形式描述了这些“空间原子”，也给出了它们演化的方程。它们被称为“圈”或环，因为它们环环相扣，形成了一个相互关联的网络，从而编制出空间的纹理，就像细密织成的巨大锁子甲上的小铁圈一样。</p><p>第六课 概率、时间和黑洞的热</p><p>英国物理学家麦克斯韦和奥地利物理学家玻尔兹曼（Ludwig Boltzmann）发现了热的本质。</p><p>玻尔兹曼发现其中的原因惊人的简单：这完全是随机的。玻尔兹曼的解释非常精妙，用到了概率的概念。热量从热的物体跑到冷的物体并非遵循什么绝对的定律，只是这种情况发生的概率比较大而已。原因在于：从统计学的角度看，一个快速运动的热物体的原子更有可能撞上一个冷物体的原子，传递给它一部分能量；而相反过程发生的概率则很小。在碰撞的过程中能量是是守恒的，但当发生大量偶然碰撞时，能量倾向于平均分布。就这样，相互接触的物体温度趋向于相同。热的物体和冷的物体接触后温度不降反升的情况并非不可能，只是概率小的可怜罢了。</p><p>尾声 我们</p>]]></content>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>03机器学习实战-第3章 决策树</title>
      <link href="/2018/01/28/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC3%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2018/01/28/03%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC3%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><h1 id="第3章-决策树"><a href="#第3章-决策树" class="headerlink" title="第3章 决策树"></a>第3章 决策树</h1><h2 id="本章内容"><a href="#本章内容" class="headerlink" title="本章内容"></a>本章内容</h2><blockquote><ul><li>决策树简介  </li><li>在数据集中度量一致性  </li><li>使用递归构造决策树     </li><li>使用Matplotlib绘制树形图   </li></ul></blockquote><h2 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h2><blockquote><p>优点：计算复杂度不高，输出易于理解，对中间值得确实不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题<br>使用数据类型：数值型和标称型  </p></blockquote><p><strong>创建分支伪代码函数createBranch()如下：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">检测数据集中的每个指向是否属于同一个分类：</span><br><span class="line">    IF so return 类标签</span><br><span class="line">    Else</span><br><span class="line">        寻找划分数据集的最好特征</span><br><span class="line">        划分数据集</span><br><span class="line">        创建分支节点</span><br><span class="line">            for 每个划分的子集</span><br><span class="line">                调用函数createBranch并增加返回结果到分支节点中</span><br><span class="line">        return 分支节点</span><br></pre></td></tr></table></figure></p><p>上述是一个递归函数</p><h2 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h2><blockquote><p>(1) 收集数据：可以使用任何方法。<br>(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>(4) 训练算法：构造树的数据结构。<br>(5) 测试算法：使用经验树计算错误率。<br>(6) 使用算法：词步骤可以使用于任何监督学习算法，而使用决策树可能更好地理解数据的内在含义。  </p></blockquote><p><strong> 摘要</strong></p><ol><li>信息论相关知识</li><li>决策树算法原理</li><li>代码实现与解释   </li></ol><p>今天总结决策树算法，目前建立决策树有三种主要算法：ID3、C4.5以及CART。由于算法知识点比较琐碎，我分成两节来总结。</p><p>第一节主要是梳理决策树算法中ID3和C4.5的知识点；第二节主要梳理剪枝技术、CART算法和随机森林算法的知识。</p><h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><h3 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1.信息熵"></a>1.信息熵</h3><p>在决策树算法中，熵是一个非常非常重要的概念。</p><p>一件事发生的概率越小，我们说它所蕴含的信息量越大。</p><p>比如：我们听女人能怀孕不奇怪，如果某天听到哪个男人怀孕了，我们就会觉得emmm…信息量很大了。</p><p>所以我们这样衡量信息量：</p><script type="math/tex; mode=display">i(y)=-log{P(y)}</script><p>其中，$P(y)$是事件发生的概率。</p><p>信息熵就是所有可能发生事件的信息量的期望：  </p><script type="math/tex; mode=display">H(Y)=-\sum_{i=1}^{n}P(y_i)log{P(y_i)}</script><p>表达了$Y$事件发生的不确定度。  </p><h3 id="2-条件熵"><a href="#2-条件熵" class="headerlink" title="2.条件熵"></a>2.条件熵</h3><p>条件熵：表示在X给定条件下，$Y$的条件概率分布的熵对$X$的数学期望。其数学推导如下：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y|X) & =\sum_{x\in{X}}P{(x)}H(Y|X=x) \\   & =-\sum_{x\in{X}}P(x)\sum_{y\in{Y}}P(y|x)log{P(y|x)}\\   & =-\sum_{x\in{X}}\sum_{y\in{Y}}P(x,y)log{P(y|x)}\end{aligned}</script><p>条件熵$H（Y|X）$表示在已知随机变量$X$的条件下随机变量Y的不确定性。注意一下，条件熵中X也是一个变量，意思是在一个变量$X$的条件下（变量$X$的每个值都会取到），另一个变量$Y$的熵对$X$的期望。</p><p>举个例子</p><p>例：女生决定主不主动追一个男生的标准有两个：颜值和身高，如下表所示：</p><div class="table-container"><table><thead><tr><th></th><th>颜值</th><th>身高</th><th>追不追</th></tr></thead><tbody><tr><td>1</td><td>帅</td><td>高</td><td>追</td></tr><tr><td>2</td><td>帅</td><td>不高</td><td>追</td></tr><tr><td>3</td><td>不帅</td><td>高</td><td>不追</td></tr></tbody></table></div><p>上表中随机变量$Y=\{追，不追\}$，$P(Y=追)=2/3$，$P(Y=不追)=1/3$，得到$Y$的熵：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y) & =-\frac{2}{3}log\frac{2}{3}-\frac{1}{3}log\frac{1}{3} \\   & =0.918\end{aligned}</script><p>这里还有一个特征变量$X$，$X=｛高，不高｝$。当$X=高$时，追的个数为1，占1/2，不追的个数为1，占1/2，此时：</p><script type="math/tex; mode=display">H(Y|X=高)=-\frac{1}{2}log\frac{1}{2}-\frac{1}{2}log\frac{1}{2}</script><p>同理：</p><script type="math/tex; mode=display">H(Y|X=不高)=-{1}log{1}-{1}log{1}</script><p>（注意：我们一般约定，当$p=0$时，$plogp=0$）</p><p>所以我们得到条件熵的计算公式：  </p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberH(Y|X=身高) & =P(X=不高)*H(Y|X=不高)+P(X=高)*H(Y|X=高)\\            & =0.67\end{aligned}</script><h3 id="3-信息增益"><a href="#3-信息增益" class="headerlink" title="3.信息增益"></a>3.信息增益</h3><p>当我们用另一个变量$X$对原变量$Y$分类后，原变量$Y$的不确定性就会减小了（即熵值减小）。而熵就是不确定性，不确定程度减少了多少其实就是信息增益。这就是信息增益的由来，所以信息增益定义如下：</p><script type="math/tex; mode=display">Gain(Y,X)=H(Y)-H(Y|X)</script><p>此外，信息论中还有互信息、交叉熵等概念，它们与本算法关系不大，这里不展开。 </p><h2 id="代码实现与解读"><a href="#代码实现与解读" class="headerlink" title="代码实现与解读"></a>代码实现与解读</h2><p><strong>1.计算给定数据的香浓熵 </strong>    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算给定数据集的香农熵</span></span><br><span class="line"><span class="comment">#从math中导入log函数</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)   <span class="comment">#计算实例中的个数</span></span><br><span class="line">    </span><br><span class="line">    labelCounts = &#123;&#125;    <span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:    <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">        </span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]    <span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():     <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">            </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>       <span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">          </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>     <span class="comment">#标签为currentLabel的个数加1       </span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries    <span class="comment">#计算每一个标签的概率p</span></span><br><span class="line">        </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)    <span class="comment">#log base 2利用公式计算香农熵</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="comment">#change to discrete values</span></span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line">myDat,labels=createDataSet()</span><br><span class="line">myDat,labels</span><br></pre></td></tr></table></figure><pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]], [&#39;no surfacing&#39;, &#39;flippers&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calcShannonEnt(myDat)</span><br></pre></td></tr></table></figure><pre><code>0.9709505944546686</code></pre><p><strong>2.创建选取的数据特征属性划分数据集</strong></p><p>程序清单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照给定特征划分数据集</span></span><br><span class="line"><span class="comment">#参数解释：dataSet待划分数据集</span></span><br><span class="line"><span class="comment">#axis：划分数据集的特征，这个函数里指函数第几列</span></span><br><span class="line"><span class="comment">#value：特征返回值，指的是特征划分的标准</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []     <span class="comment">#创建一个新列表</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:   <span class="comment">#如果这组数据特征值等于特征返回值的话</span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec = featVec[:axis]       <span class="comment">#这两行是把原来的数据除掉划分数据的特征那一列 </span></span><br><span class="line">            </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)   <span class="comment">#把列表reduceFeatVect放入retDataSet中</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">1</span>) </span><br><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line"><span class="comment"># 将myDat的第1列按照取出所有等于1的方式划分</span></span><br></pre></td></tr></table></figure><pre><code>[[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">splitDataSet(myDat,<span class="number">0</span>,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>[[1, &#39;no&#39;], [1, &#39;no&#39;]]</code></pre><p><strong>3.根据信息增益准则，选取最好的划分特征</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#找到最好的数据集划分方式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>    <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)   <span class="comment">#计算数据原始香农熵</span></span><br><span class="line">   </span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>   <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):       </span><br><span class="line">        </span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">        </span><br><span class="line">        uniqueVals = set(featList)    <span class="comment">#set()，生成一个集合数据类型，其和列表类型一样，不同之处在于</span></span><br><span class="line">                                      <span class="comment">#集合数据类型里面的值不重复，是唯一的</span></span><br><span class="line">        </span><br><span class="line">        newEntropy = <span class="number">0.0</span>    <span class="comment">#初始化新熵为0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:    <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">           </span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)    <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">            </span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))   <span class="comment">#计算概率</span></span><br><span class="line">            </span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)    <span class="comment">#计算新熵 </span></span><br><span class="line">       </span><br><span class="line">        infoGain = baseEntropy - newEntropy     <span class="comment">#计算信息增益calculate the info gain; ie reduction in entropy</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):      <span class="comment">#得到最大的信息增益和选取特征 #compare this to the best gain so far</span></span><br><span class="line">            bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature                      <span class="comment">#returns an integer</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myDat=[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#       [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#       [0, 1, 'no']</span></span><br><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span> <span class="comment">#得到特征个数，减1是因为类别栏     #the last column is used for the labels</span></span><br><span class="line">    <span class="comment">#计算数据原始香农熵</span></span><br><span class="line"><span class="comment"># numFeatures</span></span><br><span class="line">baseEntropy = calcShannonEnt(myDat)</span><br><span class="line">print(<span class="string">"numFeatures=%d"</span> %numFeatures) </span><br><span class="line">print(<span class="string">"原始熵是："</span>,baseEntropy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化信息增益和初始化最优特征</span></span><br><span class="line">bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span>     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):        <span class="comment">#iterate over all the features</span></span><br><span class="line">        <span class="comment">#熟悉这种写法，括号里面是取了第i个特征的所有值</span></span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(<span class="string">"第%d个特征的所有取值"</span> %i,featList)</span><br><span class="line">    </span><br><span class="line">    uniqueVals = set(featList) </span><br><span class="line">    <span class="comment">#初始化新熵为0#get a set of unique values</span></span><br><span class="line">    newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment">#下面这个for函数主要为了计算按第i个特征划分的新熵</span></span><br><span class="line">    print(<span class="string">"-简化取值:"</span>,uniqueVals)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            <span class="comment">#生成按value值划分的数据集</span></span><br><span class="line">        subDataSet = splitDataSet(myDat, i, value)</span><br><span class="line">        print(<span class="string">"--按照%d划分取值"</span>% value,subDataSet)</span><br><span class="line">            <span class="comment">#计算概率</span></span><br><span class="line">        prob = len(subDataSet)/float(len(myDat))</span><br><span class="line">            <span class="comment">#计算新熵</span></span><br><span class="line">        print(<span class="string">"---去此值的概率是："</span>,prob)</span><br><span class="line">        newEntropy += prob * calcShannonEnt(subDataSet)   </span><br><span class="line">            <span class="comment">#计算信息增益</span></span><br><span class="line">        print(<span class="string">"---新熵是"</span>,newEntropy)</span><br><span class="line">    infoGain = baseEntropy - newEntropy     <span class="comment">#calculate the info gain; ie reduction in entropy</span></span><br><span class="line">    print(<span class="string">"-----信息增益"</span>,infoGain)</span><br><span class="line">        <span class="comment">#得到最大的信息增益和选取特征</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (infoGain &gt; bestInfoGain):       <span class="comment">#compare this to the best gain so far</span></span><br><span class="line">        bestInfoGain = infoGain         <span class="comment">#if better than current best, set to best</span></span><br><span class="line">        bestFeature = i</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"此时最好的熵是"</span>,bestInfoGain,<span class="string">"此时最佳特征值是"</span>,bestFeature)</span><br></pre></td></tr></table></figure><pre><code>numFeatures=2原始熵是： 0.9709505944546686第0个特征的所有取值 [1, 1, 1, 0, 0]-简化取值: {0, 1}--按照0划分取值 [[1, &#39;no&#39;], [1, &#39;no&#39;]]---去此值的概率是： 0.4---新熵是 0.0--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;]]---去此值的概率是： 0.6---新熵是 0.5509775004326937-----信息增益 0.4199730940219749此时最好的熵是 0.4199730940219749 此时最佳特征值是 0第1个特征的所有取值 [1, 1, 0, 1, 1]-简化取值: {0, 1}--按照0划分取值 [[1, &#39;no&#39;]]---去此值的概率是： 0.2---新熵是 0.0--按照1划分取值 [[1, &#39;yes&#39;], [1, &#39;yes&#39;], [0, &#39;no&#39;], [0, &#39;no&#39;]]---去此值的概率是： 0.8---新熵是 0.8-----信息增益 0.17095059445466854此时最好的熵是 0.4199730940219749 此时最佳特征值是 0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chooseBestFeatureToSplit(myDat)</span><br></pre></td></tr></table></figure><pre><code>0</code></pre><p><strong>从数据集构造决策树算法：其工作原理如下：</strong></p><ol><li>得到原始数据集  </li><li>基于最好的属性值划分数据集（可能存在大于两个分支的数据集划分）    </li><li>第一次划分后，数据被向下传递到树分支的下一个节点（可以用递归的思想）</li></ol><p><strong>递归的条件： </strong><br>程序遍历完所有划分数据集属性，或者每个分支下的所有实例都具有相同的分支。</p><p><strong>4.多数表决器</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多数表决器</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    </span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="comment">#for程序用来计数</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): </span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#排序函数</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>5.创建决策树</strong>  </p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建树</span></span><br><span class="line"><span class="comment">#     myDat  = [[1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 1, 'yes'],</span></span><br><span class="line"><span class="comment">#               [1, 0, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no'],</span></span><br><span class="line"><span class="comment">#               [0, 1, 'no']]</span></span><br><span class="line"><span class="comment">#     labels = ['no surfacing','flippers']</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#classLsit里面是dataSet里面的标签</span></span><br><span class="line">    <span class="comment"># 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行</span></span><br><span class="line">    <span class="comment"># 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。</span></span><br><span class="line">    <span class="comment"># count() 函数是统计括号中的值在list中出现的次数</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList): <span class="comment">#第一个终止条件：所有类标签都相同，country（）函数用来计数0</span></span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]<span class="comment">#stop splitting when all of the classes are equal</span></span><br><span class="line">    <span class="comment"># 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果</span></span><br><span class="line">    <span class="comment"># 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: <span class="comment">#第二个终止条件：用完了所有的特征#stop splitting when there are no more features in dataSet</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]<span class="comment">#得到标签里的所有属性值</span></span><br><span class="line">    uniqueVals = set(featValues)<span class="comment">#得到属性值集合</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]       <span class="comment">#copy all of labels, so trees don't mess up existing labels</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br></pre></td></tr></table></figure><pre><code>{&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}}</code></pre><p><strong>6.使用决策树进行分类</strong></p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用决策树分类函数</span></span><br><span class="line"><span class="comment">#三个参数意义：input：决策树；featLabels：特征标签；testVec：测试向量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure><p><strong>7.决策树在磁盘中的存储与导入</strong>  </p><p>程序清单：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将决策树分类器存储在磁盘中，filename一般保存为txt格式</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#将磁盘中的对象加载出来，这里filename就是上面函数中的txt文件</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">createTree(myDat,labels)</span><br><span class="line"><span class="comment"># storeTree(myTree,'classifierStorage.txt')</span></span><br><span class="line"><span class="comment"># grabTree('classifierStorage.txt')</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------IndexError                                Traceback (most recent call last)&lt;ipython-input-16-33c9af9c39fa&gt; in &lt;module&gt;()----&gt; 1 createTree(myDat,labels)      2 # storeTree(myTree,&#39;classifierStorage.txt&#39;)      3 # grabTree(&#39;classifierStorage.txt&#39;)&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)     21     for value in uniqueVals:     22         subLabels = labels[:]       #copy all of labels, so trees don&#39;t mess up existing labels---&gt; 23         myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)     24     return myTree&lt;ipython-input-12-854ee28d5c1d&gt; in createTree(dataSet, labels)     14         return majorityCnt(classList)     15     bestFeat = chooseBestFeatureToSplit(dataSet)---&gt; 16     bestFeatLabel = labels[bestFeat]     17     myTree = {bestFeatLabel:{}}     18     del(labels[bestFeat])IndexError: list index out of range</code></pre><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li>CART决策树</li><li>决策树的剪枝技术</li><li>Bagging与随机森林</li><li>决策树中缺失值的处理</li><li>决策树代码</li></ol><p>注：本节代码对应第九章“树回归”</p><h2 id="CART决策树"><a href="#CART决策树" class="headerlink" title="CART决策树"></a>CART决策树</h2><p>为什么同样作为建立决策树的三种算法之一，我们要将CART算法单独拿出来讲。</p><p>因为ID3算法和C4.5算法采用了较为复杂的熵来度量，所以它们只能处理分类问题。而CART算法既能处理分类问题，又能处理回归问题。</p><p>对于分类树，CART采用基尼指数最小化准则；对于回归树，CART采用平方误差最小化准则</p><h3 id="1-CART分类树"><a href="#1-CART分类树" class="headerlink" title="1.CART分类树"></a>1.CART分类树</h3><p>CART分类树与上一节讲述的ID3算法和C4.5算法在原理部分差别不大，唯一的区别在于划分属性的原则。CART选择“基尼指数”作为划分属性的选择。</p><p>Gini指数作为一种做特征选择的方式，其表征了特征的不纯度。</p><p>在具体的分类问题中，对于数据集D，我们假设有K个类别，每个类别出现的概率为$P_k$，则数据集$D$的基尼指数的表达式为：</p><script type="math/tex; mode=display">Gini=1-\sum_{k=1}^{K}{P_k}^2</script><p>我们取一个极端情况，如果数据集合中的类别只有一类，那么：</p><script type="math/tex; mode=display">Gini(D)=0</script><p>我们发现，当只有一类时，数据的不纯度是最低的，所以Gini指数等于零。Gini(D)越小，则数据集D的纯度越高。</p><p>特别地，对于样本D，如果我们选择特征A的某个值a，把D分成$D_1$和$D_2$两部分，则此时，Gini指数为：  </p><script type="math/tex; mode=display">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>与信息增益类似，我们可以计算如下表达式：  </p><script type="math/tex; mode=display">\Delta{Gini(A)}=Gini(D)-Gini(D,A)</script><p>即以特征A划分后，数据不纯度减少的程度。显然，我们在做特征选取时，应该选择最大的一个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat,labels</span><br></pre></td></tr></table></figure><pre><code>([[1, 1, &#39;yes&#39;], [1, 1, &#39;yes&#39;], [1, 0, &#39;no&#39;], [0, 1, &#39;no&#39;], [0, 1, &#39;no&#39;]], [&#39;no surfacing&#39;, &#39;flippers&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">currentLabel</span><br></pre></td></tr></table></figure><pre><code>&#39;no&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">labelCounts = &#123;&#125;<span class="comment">#创建字典，键为标签，值为个数</span></span><br><span class="line"><span class="keyword">for</span> featVec <span class="keyword">in</span> myDat: <span class="comment">#the the number of unique elements and their occurance</span></span><br><span class="line">    currentLabel = featVec[<span class="number">-1</span>]<span class="comment">#得到标签，注意是最后一个标签</span></span><br><span class="line">    <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): <span class="comment">#如果标签不在字典已经存在的键中</span></span><br><span class="line">        labelCounts[currentLabel] = <span class="number">0</span><span class="comment">#创建名为currentLabel的键，赋值为0</span></span><br><span class="line">    labelCounts[currentLabel] += <span class="number">1</span><span class="comment">#标签为currentLabel的个数加1</span></span><br><span class="line">labelCounts</span><br></pre></td></tr></table></figure><pre><code>{&#39;yes&#39;: 2, &#39;no&#39;: 3}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">    print(featList)</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 0, 0][1, 1, 0, 1, 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myDat[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>[1, 1, &#39;yes&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numFeatures = len(myDat[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">numFeatures</span><br></pre></td></tr></table></figure><pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><pre><code>01</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">classList</span><br></pre></td></tr></table></figure><pre><code>[&#39;yes&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;no&#39;, &#39;no&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bestFeat = chooseBestFeatureToSplit(myDat)</span><br><span class="line">bestFeatLabel = labels[bestFeat]</span><br><span class="line">myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">myTree</span><br></pre></td></tr></table></figure><pre><code>{&#39;no surfacing&#39;: {}}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> myDat]</span><br><span class="line">featValues</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 1, 0, 0]</code></pre><p>至此，我们完成了决策树算法原理和主要代码的学习。</p><p>下一节我们将学习CART算法、随机森林算法以及剪枝技术。</p><p>以上原理部分主要来自于《机器学习》—周志华，《统计学习方法》—李航，《机器学习实战》—Peter Harrington。代码部分主要来自于《机器学习实战》，我对代码进行了版本的改进，文中代码用Python3实现，这是机器学习主流语言，本人也会尽力对代码做出较为详尽的注释。</p><h2 id="决策树-原理"><a href="#决策树-原理" class="headerlink" title="决策树 原理"></a>决策树 原理</h2><h3 id="决策树-须知概念"><a href="#决策树-须知概念" class="headerlink" title="决策树 须知概念"></a>决策树 须知概念</h3><h4 id="信息熵-amp-信息增益"><a href="#信息熵-amp-信息增益" class="headerlink" title="信息熵 &amp; 信息增益"></a>信息熵 &amp; 信息增益</h4><p><strong>熵</strong>： 熵（entropy）指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。</p><p><strong>信息熵（香农熵）</strong>： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。</p><p>信息增益： 在划分数据集前后信息发生的变化称为信息增益。</p><h3 id="决策树-工作原理"><a href="#决策树-工作原理" class="headerlink" title="决策树 工作原理"></a>决策树 工作原理</h3><p>如何构造一个决策树?<br>我们使用 createBranch() 方法，如下所示：</p><blockquote><p>检测数据集中的所有数据的分类标签是否相同:<br>         If so return 类标签<br>            Else:<br>                寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）<br>                划分数据集<br>                创建分支节点<br>                        for 每个划分的子集<br>                                调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中<br>                return 分支节点    </p></blockquote><p>​        </p><h4 id="决策树-开发流程"><a href="#决策树-开发流程" class="headerlink" title="决策树 开发流程"></a>决策树 开发流程</h4><blockquote><p>收集数据：可以使用任何方法。<br>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。<br>分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。<br>训练算法：构造树的数据结构。<br>测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充）<br>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。     </p></blockquote><p><strong>决策树 算法特点</strong></p><blockquote><p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。<br>缺点：可能会产生过度匹配问题。<br>适用数据类型：数值型和标称型。</p></blockquote><h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>1.算法简介</strong></p><p>决策树算法是一类常见的分类和回归算法，顾名思义，决策树是基于树的结构来进行决策的。</p><p>以二分类为例，我们希望从给定训练集中学得一个模型来对新的样例进行分类。</p><p><strong>举个例子</strong></p><p>有一个划分是不是鸟类的数据集合，如下：</p><div class="table-container"><table><thead><tr><th></th><th>是否会飞</th><th>是否会跑</th><th>属于鸟类</th></tr></thead><tbody><tr><td>1</td><td>是</td><td>是</td><td>否</td></tr><tr><td>2</td><td>是</td><td>是</td><td>是</td></tr><tr><td>3</td><td>是</td><td>否</td><td>否</td></tr><tr><td>4</td><td>否</td><td>是</td><td>否</td></tr><tr><td>5</td><td>否</td><td>是</td><td>否</td></tr></tbody></table></div><p>这时候我们建立这样一个决策树：  </p><p><img src="https://pic4.zhimg.com/80/v2-4a601bdc74abb553c0873fbd61597035_hd.jpg" ,width="400,height=400"></p><p>当我们有了一组新的数据时，我们就可以根据这个决策树判断出是不是鸟类。创建决策树的伪代码如下：  </p><p><img src="https://pic4.zhimg.com/80/v2-c226901dc50538bd40410e7aae938f47_hd.jpg" ,width="400,eight=400"></p><p>生成决策树是一个递归的过程，在决策树算法中，当出现下列三种情况时，导致递归返回： </p><p>(1)当前节点包含的样本属于同一种类，无需划分；</p><p>(2)当前属性集合为空，或者所有样本在所有属性上取值相同，无法划分；</p><p>(3)当前节点包含的样本集合为空，无法划分。</p><p><strong>2.属性选择</strong></p><p>在决策树算法中，最重要的就是划分属性的选择，即我们选择哪一个属性来进行划分。三种划分属性的主要算法是：ID3、C4.5以及CART。</p><p><strong>2.1 ID3算法</strong></p><p>ID3算法所采用的度量标准就是我们前面所提到的“信息增益”。当属性a的信息增益最大时，则意味着用a属性划分，其所获得的“纯度”提升最大。我们所要做的，就是找到信息增益最大的属性。由于前面已经强调了信息增益的概念，这里不再赘述。</p><p><strong>2.2 C4.5算法</strong></p><p>实际上，信息增益准则对于可取值数目较多的属性会有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：  </p><script type="math/tex; mode=display">Gain\_ratio(Y,X)=\frac{Gain(Y,X)}{H(X)}</script><p>其中，分子为信息增益，分母为属性$X$的熵。</p><p>需要注意的是，增益率准则对可取值数目较少的属性有所偏好。</p><p>所以一般这样选取划分属性：<strong>先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</strong>。</p><p><strong>2.3 CART算法</strong></p><p>ID3算法和C4.5算法主要存在三个问题：</p><p>(1)每次选取最佳特征来分割数据，并按照该特征的所有取值来进行划分。也就是说，如果一个特征有4种取值，那么数据就将被切成4份，一旦特征被切分后，该特征就不会再起作用，有观点认为这种切分方式过于迅速。</p><p>(2)它们不能处理连续型特征。只有事先将连续型特征转换为离散型，才能在上述算法中使用。</p><p>(3)会产生过拟合问题。</p><p>为了解决上述(1)、(2)问题，产生了CART算法，它主要的衡量指标是基尼系数。为了解决问题(3)，主要采用剪枝技术和随机森林算法，这部分内容，下一次再详细讲述。</p><p>上述就是决策树算法的原理部分，下面展示完整代码和注释。代码中主要采用的是ID3算法。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>01机器学习实战-机器学习基础</title>
      <link href="/2018/01/27/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC1%E7%AB%A0%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/27/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC1%E7%AB%A0%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><ul><li>主要来源自《机器学习实战》《机器学习》《利用Python进行数据分析》，还有一些网站资料</li></ul><h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><h2 id="第1章-机器学习基础"><a href="#第1章-机器学习基础" class="headerlink" title="第1章 机器学习基础"></a>第1章 机器学习基础</h2><p><strong>机器学习概述</strong></p><p>机器学习就是无序的数据转换成有用信息。</p><blockquote><ol><li><p>获取海量的数据   </p></li><li><p>从海量数据中获取有用信息</p></li></ol></blockquote><h3 id="机器学习场景"><a href="#机器学习场景" class="headerlink" title="机器学习场景"></a>机器学习场景</h3><blockquote><p>例如：识别动物猫<br>模式识别（官方标准）：人们通过大量的经验，得到结论，从而判断它就是猫。<br>机器学习（数据学习）：人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。<br>深度学习（深入数据）：人们通过深入了解它，发现它会’喵喵’的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域：语音识别、图像识别）   </p></blockquote><p>模式识别（pattern recognition）：模式识别是最古老的（作为一个术语而言，可以说是很过时的）。<br>    我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。<br>    通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。    </p><p>机器学习（machine learning）：机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。<br>    在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。<br>    “机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。<br>    机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。    </p><p>深度学习（deep learning）：深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。<br>    深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。</p><p>参考地址： </p><p><a href="http://www.csdn.net/article/2015-03-24/2824301" target="_blank" rel="noopener">http://www.csdn.net/article/2015-03-24/2824301</a></p><p><a href="http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q" target="_blank" rel="noopener">http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q</a></p><blockquote><p>机器学习已应用于多个领域，远远超出大多数人的想象，横跨：计算机科学、工程技术和统计学等多个学科。</p></blockquote><ul><li>搜索引擎：根据你的搜索点击，优化你下次的搜索结果。   </li><li>垃圾邮件：会自动的过滤垃圾广告邮件到垃圾箱内。   </li><li>超市优惠券：你会发现，你在购买小孩子尿布时候，售货员会赠送给你一张优惠券可以兑换免费啤酒。  </li><li>邮件邮寄：手写软件自动识别寄送贺卡的地址。  </li><li>申请贷款：通过你最近的金融活动信息进行综合评定，决定你是否合格。  </li></ul><h3 id="机器学习组成"><a href="#机器学习组成" class="headerlink" title="机器学习组成"></a>机器学习组成</h3><p><strong>主要任务</strong></p><ul><li>分类：将实例数据划分到合适的类别中。  </li><li>回归：主要用于预测数值型数据（示例：数据通过给定数据点耐力和最优曲线）   </li></ul><p><strong>监督学习</strong></p><ul><li>必须确定目标变量的值，以便机器学习可以发现特征值和目标变量之间的关系。（包括分类和回归）  </li><li>样本集：训练数据+测试数据<ul><li>训练样本=特征（feature）+目标变量（label：分类-离散值/回归-连续值）  </li><li>特征通常是训练样本集的列，它们是独立测量得到的。 </li><li>目标变量：目标变量是机器学习预测算法的测试结果。<ul><li>在分类算法中目标变量的类型通常是标称型（如：真与假），而在回归算法中通常是连续型（如：1~100）</li></ul></li></ul></li></ul><ul><li>知识表示：<br>1.可以采用规则集的形式【例如：数学成绩大于90分为优秀】<br>2.可以采用概率分布的形式【例如：通过统计分布，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】<br>3.可以使用训练样本集中的一个实例【例如：通过样本集合，我们训练处一个模型实例，得出年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】</li></ul><p><strong>非监督学习</strong>  </p><ul><li>数据没有类别，也不会给定目标值。  </li><li>聚类：在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。  </li><li>此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。</li></ul><p><strong>训练过程</strong>  </p><p><img src="https://pic4.zhimg.com/80/v2-711466f238bbb969618e6fe669a16a4f_hd.jpg" ,width="400,height=400"></p><p><strong>算法汇总</strong>   </p><center>**用于执行分类、回归、聚类和密度估计的机器学习算法**</center><div class="table-container"><table><thead><tr><th><p align="center">监督学习的用途</p></th></tr></thead><tbody><tr><td><p align="left">k-近邻算法</p></td><td><p align="left">线性回归</p></td></tr><tr><td><p align="left">朴素贝叶斯算法</p></td><td><p align="left">局部加权线性回归</p></td></tr><tr><td><p align="left">支持向量机</p></td><td><p align="left">Ridge回归</p></td></tr><tr><td><p align="left">决策树</p></td><td><p align="left">Lasso最小回归系数估计</p></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th><p align="center">无监督学习的用途</p></th></tr></thead><tbody><tr><td><p align="left">K-均值</p></td><td><p align="left">最大期望算法</p></td></tr><tr><td><p align="left">DBSCAN</p></td><td><p align="left">Parzen窗设计</p></td></tr></tbody></table></div><p>机器学习使用</p><blockquote><p>选择算法需要考虑的两个问题</p></blockquote><p>1.算法场景  </p><ul><li>预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法   </li><li>给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。    </li></ul><p>2.需要收集或分析的数据是什么  </p><blockquote><p>举例</p></blockquote><p><img src="https://pic3.zhimg.com/80/v2-88ee740c5e4a2a2bdab7e08043321e08_hd.jpg" ,width="400,height=400"></p><blockquote><p>机器学习开发流程</p></blockquote><ul><li>收集数据: 收集样本数据</li><li>准备数据: 注意数据的格式</li><li>分析数据: 为了确保数据集中没有垃圾数据；<br>  如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；<br>  另外该步骤需要人工干预，会降低自动化系统的价值。</li><li>训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤</li><li>测试算法: [机器学习算法核心]评估算法效果</li><li>使用算法: 将机器学习算法转为应用程序</li></ul><p><strong>Python语言优势</strong></p><ol><li>可执行伪代码</li><li>Python比较流行：使用广泛、代码范例多、丰富模块库，开发周期短</li><li>Python语言的特色：清晰简练、易于理解</li><li>Python语言的缺点：唯一不足的是性能问题</li><li>Python相关的库    </li></ol><ul><li>科学函数库：SciPy、NumPy（底层语言：C和Fortran）</li><li>绘图工具库：Matplotlib</li></ul><h3 id="科普"><a href="#科普" class="headerlink" title="科普"></a>科普</h3><p>　　机器学习（Machine Learning）专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合。  ——来自 百度百科</p><p>简单来讲，机器学习就是一门让机器能够进行自我学习并不断优化功能的学科。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>00机器学习实战-机器学习的数学基础</title>
      <link href="/2018/01/26/00%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/26/00%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<p>[TOC]</p><h1 id="机器学习：机器学习的数学基础"><a href="#机器学习：机器学习的数学基础" class="headerlink" title="机器学习：机器学习的数学基础"></a>机器学习：机器学习的数学基础</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>机器学习的特点就是：以计算机为工具和平台，以数据为研究对象，以学习方法为中心；<br>是概率论、线性代数、数值计算、信息论、最优化理论和计算机科学等多个领域的交叉学科。</p><h2 id="二、线性代数"><a href="#二、线性代数" class="headerlink" title="二、线性代数"></a>二、线性代数</h2><h3 id="2-1-标量"><a href="#2-1-标量" class="headerlink" title="2.1 标量"></a>2.1 标量</h3><p>一个标量就是一个单独的数，一般用小写的的变量名称表示。</p><h3 id="2-2-向量"><a href="#2-2-向量" class="headerlink" title="2.2 向量"></a>2.2 向量</h3><p>一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱：</p><script type="math/tex; mode=display">X=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\quad</script><p>我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。 </p><h3 id="2-3-矩阵"><a href="#2-3-矩阵" class="headerlink" title="2.3 矩阵"></a>2.3 矩阵</h3><p>矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如$A$。如果一个实数矩阵高度为$m$，宽度为$n$，那么我们说$A\epsilon R^{m\times n}$。</p><script type="math/tex; mode=display">A=\begin{bmatrix}a_{11} & a_{12} & \dots & a_{1n}\\a_{21} & a_{22} & \dots & a_{2n}\\\vdots & \vdots &  & \dots\\a_{m1} & a_{m2} & \dots & a_{mn}\end{bmatrix}\quad</script><p>矩阵这东西在机器学习中就不要太重要了！实际上，如果我们现在有$N$个用户的数据，每条数据含有$M$个特征，那其实它对应的就是一个$N<em>M$的矩阵呀；再比如，一张图由$16</em>16$的像素点组成，那这就是一个$16*16$的矩阵了。</p><h3 id="2-4-张量"><a href="#2-4-张量" class="headerlink" title="2.4 张量"></a>2.4 张量</h3><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。</p><p>例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：</p><p><img src="https://pic4.zhimg.com/v2-c0c16793d4662bfcdd7e112030096f94_r.jpg" alt=""></p><p>其中表的横轴表示图片的宽度值，这里只截取$0-319$；表的纵轴表示图片的高度值，这里只截取$0-4$；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为$[1.0,1.0,1.0]$，代表的就是$RGB$三原色在图片的这个位置的取值情况（即$R=1.0$，$G=1.0$，$B=1.0$）。</p><p>当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。</p><p>张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。</p><h3 id="2-5-范数"><a href="#2-5-范数" class="headerlink" title="2-5 范数"></a>2-5 范数</h3><p>有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数$(norm)$的函数衡量矩阵大小。$L_p$范数如下：</p><script type="math/tex; mode=display">\left| \left| x \right| \right| _{p}^{} =\left( \sum_{i}^{}{\left| x_{i} \right| ^{p} } \right) _{}^{\frac{1}{p} }</script><p>所以：</p><p>$L_1$范数$ \left| \left| x \right| \right|$：为$x$向量各个元素绝对值之和；</p><p>$L<em>2$范数$ \left| \left| x \right| \right| </em>{2}$：为$x$向量各个元素平方和的开方。</p><p>这里先说明一下，在机器学习中，$ L_1$范数和$ L_2$范数很常见，主要用在损失函数中起到一个限制模型参数复杂度的作用，至于为什么要限制模型的复杂度，这又涉及到机器学习中常见的过拟合问题。具体的概念在后续文章中会有详细的说明和推导，大家先记住：这个东西很重要，实际中经常会涉及到，面试中也常会被问到！！！</p><h3 id="2-6-特征分解"><a href="#2-6-特征分解" class="headerlink" title="2.6 特征分解"></a>2.6 特征分解</h3><p>许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。</p><p>方阵$ A$的特征向量是指与$ A$相乘后相当于对该向量进行缩放的非零向量$ \nu$ ：</p><script type="math/tex; mode=display">A\nu =\lambda \nu</script><p>标量$\lambda$被称为这个特征向量对应的特征值。 </p><p>使用特征分解去分析矩阵$ A$时，得到特征向量构成的矩阵$ V$和特征值构成的向量$\lambda$，我们可以重新将A写作：</p><script type="math/tex; mode=display">A=Vdiag\left( \lambda \right) V^{-1}</script><h3 id="2-7-奇异值分解（Singular-Value-Decomposition，SVD）"><a href="#2-7-奇异值分解（Singular-Value-Decomposition，SVD）" class="headerlink" title="2-7 奇异值分解（Singular Value Decomposition，SVD）"></a>2-7 奇异值分解（Singular Value Decomposition，SVD）</h3><p>矩阵的特征分解是有前提条件的，那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称$ SVD$。通过奇异分解，我们会得到一些类似于特征分解的信息。</p><p>它的具体做法是将一个普通矩阵分解为奇异向量和奇异值。比如将矩阵$ A$分解成三个矩阵的乘积：</p><script type="math/tex; mode=display">A=UDV^{T}</script><p>假设<strong>$ A$是一个$m\times n$矩阵，那么$ U$是一个$m\times m$矩阵，$D$是一个$m\times n$矩阵，$V$是一个$n\times n$矩阵</strong>。</p><p>这些矩阵每一个都拥有特殊的结构，其中$U$和$V$都是正交矩阵，$D$是对角矩阵（注意，$D$不一定是方阵）。对角矩阵$D$对角线上的元素被称为矩阵$A$的奇异值。矩阵$U$的列向量被称为<strong>左奇异向量</strong>，矩阵$V$的列向量被称<strong>右奇异向量</strong>。</p><p>$SVD$最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，$SVD$可用于推荐系统中。</p><h3 id="2-8-Moore-Penrose伪逆"><a href="#2-8-Moore-Penrose伪逆" class="headerlink" title="2-8 Moore-Penrose伪逆"></a>2-8 Moore-Penrose伪逆</h3><p>对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵$A$的左逆$B$来求解线性方程：</p><script type="math/tex; mode=display">Ax=y</script><p>等式两边同时左乘左逆$B$后，得到：</p><script type="math/tex; mode=display">x=By</script><p>是否存在唯一的映射将$A$映射到$B$取决于问题的形式。</p><p>如果矩阵$A$的行数大于列数，那么上述方程可能没有解；如果矩阵$A$的行数小于列数，那么上述方程可能有多个解。</p><p>$Moore-Penrose$伪逆使我们能够解决这种情况，矩阵$A$的伪逆定义为：</p><script type="math/tex; mode=display">A^+=\lim_{a\rightarrow{0}}(A^T{A}+\alpha{I}^{-1})A^T</script><p>但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：</p><script type="math/tex; mode=display">A^+=VD^+{U}^T</script><p>其中，矩阵$U$，$D$和$V$是矩阵$A$奇异值分解后得到的矩阵。对角矩阵$D$的伪逆$D^+$是其非零元素取倒之后再转置得到的。</p><h3 id="2-9-几种常用的距离"><a href="#2-9-几种常用的距离" class="headerlink" title="2-9 几种常用的距离"></a>2-9 几种常用的距离</h3><p>上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有$100$个特征，那么他对应的就是一个$100$维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的$KNN$算法和$K-means$算法中很明显。</p><p>设有两个$n$维变量$A=\left[ x<em>{11}, x</em>{12},…,x<em>{1n} \right]$和$B=\left[ x</em>{21} ,x<em>{22} ,…,x</em>{2n} \right]$，则一些常用的距离公式定义如下：</p><h4 id="1、曼哈顿距离"><a href="#1、曼哈顿距离" class="headerlink" title="1、曼哈顿距离"></a>1、曼哈顿距离</h4><p>曼哈顿距离也称为城市街区距离，数学定义如下：</p><script type="math/tex; mode=display">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| }</script><p>曼哈顿距离的$Python$实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *  </span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  </span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])  </span><br><span class="line"><span class="keyword">print</span> sum(abs(vector1-vector2))</span><br></pre></td></tr></table></figure><h4 id="2、欧氏距离"><a href="#2、欧氏距离" class="headerlink" title="2、欧氏距离"></a>2、欧氏距离</h4><p>欧氏距离其实就是$L_2$范数，数学定义如下： </p><script type="math/tex; mode=display">d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } }</script><p>欧氏距离的$Python$实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt((vector1-vector2)*(vector1-vector2).T)</span><br></pre></td></tr></table></figure></p><h4 id="3、闵可夫斯基距离"><a href="#3、闵可夫斯基距离" class="headerlink" title="3、闵可夫斯基距离"></a>3、闵可夫斯基距离</h4><p>从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：</p><script type="math/tex; mode=display">d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } }</script><p>实际上，当$p=1$时，就是曼哈顿距离；当$p=2$时，就是欧式距离。</p><h4 id="4、切比雪夫距离"><a href="#4、切比雪夫距离" class="headerlink" title="4、切比雪夫距离"></a>4、切比雪夫距离</h4><p>切比雪夫距离就是$L_{\varpi}$，即无穷范数，数学表达式如下：</p><script type="math/tex; mode=display">d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)</script><p>切比雪夫距离额Python实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> sqrt(abs(vector1-vector2).max)</span><br></pre></td></tr></table></figure></p><h4 id="5、夹角余弦"><a href="#5、夹角余弦" class="headerlink" title="5、夹角余弦"></a>5、夹角余弦</h4><p>夹角余弦的取值范围为$[-1,1]$，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值$1$；当两个向量的方向完全相反时，夹角余弦取最小值$-1$。</p><p>机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：</p><script type="math/tex; mode=display">cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } }</script><p>夹角余弦的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">vector1 = mat([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">vector2 = mat([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="keyword">print</span> dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))</span><br></pre></td></tr></table></figure></p><h4 id="6、汉明距离"><a href="#6、汉明距离" class="headerlink" title="6、汉明距离"></a>6、汉明距离</h4><p>汉明距离定义的是两个字符串中不相同位数的数目。</p><p>例如：字符串$’1111’$与$’1001’$之间的汉明距离为$2$。</p><p>信息编码中一般应使得编码间的汉明距离尽可能的小。</p><p>汉明距离的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">smstr = nonzero(matV[<span class="number">0</span>]-matV[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> smstr</span><br></pre></td></tr></table></figure></p><h4 id="7、杰卡德相似系数"><a href="#7、杰卡德相似系数" class="headerlink" title="7、杰卡德相似系数"></a>7、杰卡德相似系数</h4><p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号$J(A,B)$表示，数学表达式为：</p><script type="math/tex; mode=display">J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| }</script><p>杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p><h4 id="8、杰卡德距离"><a href="#8、杰卡德距离" class="headerlink" title="8、杰卡德距离"></a>8、杰卡德距离</h4><p>与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：</p><script type="math/tex; mode=display">J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| }</script><p>杰卡德距离的Python实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> scipy.spatial.distance <span class="keyword">as</span> dist</span><br><span class="line">matV = mat([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> dist.pdist(matV,<span class="string">'jaccard'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="三、概率"><a href="#三、概率" class="headerlink" title="三、概率"></a>三、概率</h2><h3 id="3-1-为什么使用概率？"><a href="#3-1-为什么使用概率？" class="headerlink" title="3.1 为什么使用概率？"></a>3.1 为什么使用概率？</h3><p>概率论是用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。</p><p>在人工智能领域，我们主要以两种方式来使用概率论。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。</p><p>计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常。因为机器学习大部分时候处理的都是不确定量或随机量。</p><h4 id="3-2-随机变量"><a href="#3-2-随机变量" class="headerlink" title="3.2 随机变量"></a>3.2 随机变量</h4><p>随机变量可以随机地取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如，x<em>{1}  和x</em>{2}  都是随机变量X可能的取值。</p><p>对于向量值变量，我们会将随机变量写成X，它的一个值为x。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。</p><p>随机变量可以是离散的或者连续的。</p><h4 id="3-3-概率分布"><a href="#3-3-概率分布" class="headerlink" title="3.3 概率分布"></a>3.3 概率分布</h4><p>给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。</p><p>从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。</p><h4 id="3-4-条件概率"><a href="#3-4-条件概率" class="headerlink" title="3.4 条件概率"></a>3.4 条件概率</h4><p>很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。</p><p>我们将给定$X=x$时$Y=y$发生的概率记为$P\left( Y=y|X=x \right)$，这个概率可以通过下面的公式来计算：</p><script type="math/tex; mode=display">P\left( Y=y|X=x \right) =\frac{P\left( Y=y,X=x \right) }{P\left( X=x \right) }</script><h4 id="3-5-贝叶斯公式"><a href="#3-5-贝叶斯公式" class="headerlink" title="3.5 贝叶斯公式"></a>3.5 贝叶斯公式</h4><p>先看看什么是“先验概率”和“后验概率”，以一个例子来说明：</p><p>假设某种病在人群中的发病率是$0.001$，即$1000$人中大概会有$1$个人得病，则有：$P(患病) = 0.1\%$；即：在没有做检验之前，我们预计的患病率为$P(患病)=0.1\%$，这个就叫作”先验概率”。 </p><p>再假设现在有一种该病的检测方法，其检测的准确率为$95\%$；即：如果真的得了这种病，该检测法有$95\%$的概率会检测出阳性，但也有5%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有$95\%$的概率检测出阴性，但也有$5\%$的概率检测为阳性。用概率条件概率表示即为：$P(显示阳性|患病)=95\%$。</p><p>现在我们想知道的是：在做完检测显示为阳性后，某人的患病率$P(患病|显示阳性)$，这个其实就称为”后验概率”。</p><p>而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”。</p><p>这里先了解<strong>条件概率公式</strong>：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}</script><p>由条件概率可以得到<strong>乘法公式</strong>：  </p><script type="math/tex; mode=display">P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)</script><p>将条件概率公式和乘法公式结合可以得到：</p><script type="math/tex; mode=display">P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}</script><p>再由<strong>全概率公式</strong>：</p><script type="math/tex; mode=display">P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)}</script><p>代入可以得到<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }</script><p>在这个例子里就是：</p><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. numberp(患病|显示阳性) & =\frac{P(显示阳性|患病)P(患病)}{P(显示阳性)}\\                & =\frac{P(显示阳性|患病)P(患病)}{P(显示阳性|患病)P(患病)+{P(显示阳性|无病)P(无病)}}\\                & =\frac{95\%*0.1\%}{95\%*0.1\%+5\%*99.9\%}=1.86\%\end{aligned}</script><p>贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。</p><p>期望、方差、协方差等主要反映数据的统计特征，机器学习的一个很大应用就是数据挖掘等，因此这些基本的统计概念也是很有必要掌握。另外，像后面的EM算法中，就需要用到期望的相关概念和性质。</p><h4 id="3-6-期望"><a href="#3-6-期望" class="headerlink" title="3.6 期望"></a>3.6 期望</h4><p>在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。</p><p>假设X是一个离散随机变量，其可能的取值有：$\left\{ x<em>{1} ,x</em>{2} ,……,x<em>{n} \right\}$，各个取值对应的概率取值为：$P\left( x</em>{k} \right) , k=1,2,……,n$，则其数学期望被定义为：</p><script type="math/tex; mode=display">E\left(X \right) =\sum_{k=1}^{n}{x_{k} P\left( x_{k} \right) }</script><p>假设$X$是一个连续型随机变量，其概率密度函数为$P\left( x \right)$则其数学期望被定义为：</p><script type="math/tex; mode=display">E\left( x \right) =\int_{-\varpi }^{+\varpi } xf\left( x \right) dx</script><h4 id="3-7-方差"><a href="#3-7-方差" class="headerlink" title="3.7 方差"></a>3.7 方差</h4><p>概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下： </p><script type="math/tex; mode=display">Var\left( x \right) =E\left\{ \left[ x-E\left( x \right) \right] ^{2} \right\} =E\left( x^{2} \right) -\left[ E\left( x \right) \right] ^{2}</script><h4 id="3-8-协方差"><a href="#3-8-协方差" class="headerlink" title="3.8 协方差"></a>3.8 协方差</h4><p>在概率论和统计学中，协方差被用于衡量两个随机变量$X$和$Y$之间的总体误差。数学定义式为：</p><script type="math/tex; mode=display">Cov\left( X,Y \right) =E\left[ \left( X-E\left[ X \right] \right) \left( Y-E\left[ Y \right] \right) \right] =E\left[ XY \right] -E\left[ X \right] E\left[ Y \right]</script><h4 id="3-9-常见分布函数"><a href="#3-9-常见分布函数" class="headerlink" title="3.9 常见分布函数"></a>3.9 常见分布函数</h4><h4 id="1）0-1分布"><a href="#1）0-1分布" class="headerlink" title="1）0-1分布"></a>1）0-1分布</h4><p>$0-1$分布是单个二值型离散随机变量的分布，其概率分布函数为：  </p><script type="math/tex; mode=display">P\left( X=1 \right) =pP\left( X=0 \right) =1-p</script><h4 id="2）几何分布"><a href="#2）几何分布" class="headerlink" title="2）几何分布"></a>2）几何分布</h4><p>几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前$k-1$次皆失败，第k次成功的概率。其概率分布函数为：</p><script type="math/tex; mode=display">P\left( X=k \right) =\left( 1-p \right) ^{k-1} p</script><p>性质：</p><script type="math/tex; mode=display">E\left( X \right) =\frac{1}{p} Var\left( X \right) =\frac{1-p}{p^{2} }</script><h4 id="3）二项分布"><a href="#3）二项分布" class="headerlink" title="3）二项分布"></a>3）二项分布</h4><p>二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：   </p><script type="math/tex; mode=display">P\left( X=k \right) =C_{n}^{k} p^{k} \left( 1-p \right) ^{n-k}</script><p>性质：   </p><script type="math/tex; mode=display">E\left( X \right) =npVar\left( X \right) =np\left( 1-p \right)</script><h4 id="4）高斯分布"><a href="#4）高斯分布" class="headerlink" title="4）高斯分布"></a>4）高斯分布</h4><p>高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：(A这里少高斯分布图像)</p><p><img src="https://pic4.zhimg.com/80/v2-a0811acc8ab121a3ad8f2e37ff6c37cc_hd.jpg" alt=""></p><p>若随机变量$X$服从一个数学期望为$\mu$，方差为$\sigma ^{2}$的正态分布，则我们将其记为：$N\left( \mu ,\sigma^{2} \right)$。其期望值$\mu$决定了正态分布的位置，其标准差$\sigma$（方差的开方）决定了正态分布的幅度。</p><h4 id="5）指数分布"><a href="#5）指数分布" class="headerlink" title="5）指数分布"></a>5）指数分布</h4><p>指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了$t$小时，它总共使用至少$t+s$小时的条件概率，与从开始使用时算起它使用至少$s$小时的概率相等。下面这些都属于指数分布：</p><ul><li>婴儿出生的时间间隔</li><li>网站访问的时间间隔</li><li>奶粉销售的时间间隔    </li></ul><p>指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间$t$，就等同于$t$之内没有任何婴儿出生，即：   </p><script type="math/tex; mode=display">P\left( X\geq t \right) =P\left( N\left( t \right) =0 \right) =\frac{\left( \lambda t \right) ^{0}\cdot e^{-\lambda t} }{0!}=e^{-\lambda t}</script><p>则：   </p><script type="math/tex; mode=display">P\left( X\leq t \right) =1-P\left( X\geq t \right) =1-e^{-\lambda t}</script><p>如：接下来15分钟，会有婴儿出生的概率为：</p><script type="math/tex; mode=display">P\left( X\leq \frac{1}{4} \right) =1-e^{-3\cdot \frac{1}{4} } \approx 0.53</script><p>指数分布的图像如下：</p><p><img src="https://pic4.zhimg.com/80/v2-a58c37c481e032bbb53ff17113754ef6_hd.jpg" alt="tu"></p><h4 id="6）泊松分布"><a href="#6）泊松分布" class="headerlink" title="6）泊松分布"></a>6）泊松分布</h4><p>日常生活中，大量事件是有固定频率的，比如：</p><p>某医院平均每小时出生3个婴儿<br>某网站平均每分钟有2次访问<br>某超市平均每小时销售4包奶粉<br>它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。</p><p>泊松分布就是描述某段时间内，事件具体的发生概率。其概率函数为：   </p><script type="math/tex; mode=display">P\left( N\left( t \right) =n \right) =\frac{\left( \lambda t \right) ^{n}e^{-\lambda t} }{n!}</script><p>其中：</p><p>$P$表示概率，$N$表示某种函数关系，$t$表示时间，$n$表示数量，1小时内出生3个婴儿的概率，就表示为$P(N(1) = 3)$；$λ$表示事件的频率。</p><p>还是以上面医院平均每小时出生3个婴儿为例，则$\lambda =3$；</p><p>那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：   </p><script type="math/tex; mode=display">P\left( N\left(2 \right) =0 \right) =\frac{\left( 3\cdot 2 \right) ^{o} \cdot e^{-3\cdot 2} }{0!} \approx 0.0025</script><p>同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：    </p><script type="math/tex; mode=display">P\left( N\left( 1 \right) \geq 2 \right) =1-P\left( N\left( 1 \right)=0 \right) - P\left( N\left( 1 \right)=1 \right)\approx 0.8</script><p>【注】上面的指数分布和泊松分布参考了阮一峰大牛的博客：“泊松分布和指数分布：10分钟教程”，在此说明，也对其表示感谢！</p><h4 id="3-10-Lagrange乘子法"><a href="#3-10-Lagrange乘子法" class="headerlink" title="3.10 Lagrange乘子法"></a>3.10 Lagrange乘子法</h4><p>对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。</p><p>对于一个要求极值的函数$f\left( x,y \right)$，图上的蓝圈就是这个函数的等高图，就是说$f\left( x,y \right) =c<em>{1} ,c</em>{2} ,…,c<em>{n}$分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组$\left( x,y \right)$，使它的$c</em>{i}$值越大越好，但是这点必须满足约束条件$g\left( x,y \right)$（在黄线上）。</p><p><img src="https://pic1.zhimg.com/80/v2-e59fd8c296c7e8c3b804726998610b31_hd.jpg" alt="tu1" title="tu1"></p><p>也就是说$f(x,y)$和$g(x,y)$相切，或者说它们的梯度$\nabla{f}$和$\nabla{g}$平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为$\lambda $倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。</p><p>在支持向量机模型（SVM）的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。</p><h4 id="3-11、最大似然估计"><a href="#3-11、最大似然估计" class="headerlink" title="3-11、最大似然估计"></a>3-11、最大似然估计</h4><p>最大似然也称为最大概似估计，即：在“模型已定，参数$θ$未知”的情况下，通过观测数据估计未知参数θ 的一种思想或方法。</p><p><strong>其基本思想是</strong>：  给定样本取值后，该样本最有可能来自参数$\theta$为何值的总体。即：寻找$\tilde{\theta } _{ML}$使得观测到样本数据的可能性最大。</p><p>举个例子，假设我们要统计全国人口的身高，首先假设这个身高服从服从正态分布，但是该分布的均值与方差未知。由于没有足够的人力和物力去统计全国每个人的身高，但是可以通过采样（所有的采样要求都是独立同分布的），获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布的均值与方差。</p><p>求极大似然函数估计值的一般步骤：</p><ol><li>写出似然函数； <script type="math/tex; mode=display">L(\theta_1,\theta_2,\dots,\theta_n)=\left \{\begin{array}{c}\prod_{i=1}^{n}p(x_i;\theta_1,\theta_2,\dots,\theta_n)\\\prod_{i=1}^{n}f(x_i;\theta_1,\theta_2,\dots,\theta_n)\end{array}\right.</script></li><li>对似然函数取对数；  </li><li>两边同时求导数；  </li><li>令导数为0解出似然方程。  </li></ol><p>在机器学习中也会经常见到极大似然的影子。比如后面的<strong>逻辑斯特回归模型（LR）</strong>，其核心就是构造对数损失函数后运用极大似然估计。</p><h3 id="四、信息论"><a href="#四、信息论" class="headerlink" title="四、信息论"></a>四、信息论</h3><p>信息论本来是通信中的概念，但是其核心思想“熵”在机器学习中也得到了广泛的应用。比如决策树模型$ID3$，$C4.5$中是利用<strong>信息增益</strong>来划分特征而生成一颗决策树的，而信息增益就是基于这里所说的<strong>熵</strong>。所以它的重要性也是可想而知。</p><h4 id="4-1-熵"><a href="#4-1-熵" class="headerlink" title="4.1 熵"></a>4.1 熵</h4><p>如果一个随机变量X的可能取值为$X=\left\{ x<em>{1},x</em>{2} ,…..,x<em>{n} \right\}$，其概率分布为$P\left( X=x</em>{i} \right) =p_{i} ,i=1,2,…..,n$，则随机变量X的熵定义为$H(X)$：</p><script type="math/tex; mode=display">H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i} \right) logP\left( x_{i} \right) } =\sum_{i=1}^{n}{P\left( x_{i} \right) \frac{1}{logP\left( x_{i} \right) } }</script><h4 id="4-2-联合熵"><a href="#4-2-联合熵" class="headerlink" title="4.2 联合熵"></a>4.2 联合熵</h4><p>两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量XY的不确定性的度量，用$H(X,Y)$表示：</p><script type="math/tex; mode=display">H\left( X,Y \right) =-\sum_{i=1}^{n}{\sum_{j=1}^{n}{P\left( x_{i} ,y_{j} \right)} logP\left( x_{i},y_{j} \right) }</script><h4 id="4-3-条件熵"><a href="#4-3-条件熵" class="headerlink" title="4.3 条件熵"></a>4.3 条件熵</h4><p>在随机变量$X$发生的前提下，随机变量$Y$发生新带来的熵，定义为$Y$的条件熵，用$H(Y|X)$表示：</p><script type="math/tex; mode=display">H\left(Y|X \right) =-\sum_{x,y}^{}{P\left( x,y \right) logP\left( y|x \right) }</script><p>条件熵用来衡量在已知随机变量$X$的条件下，随机变量$Y$的不确定性。</p><p>实际上，熵、联合熵和条件熵之间存在以下关系：</p><script type="math/tex; mode=display">H\left( Y|X \right) =H\left( X,Y\right) -H\left( X \right)</script><p>推导过程如下：  </p><script type="math/tex; mode=display">\begin{array}{ll}H\left( X,Y\right) -H\left( X \right)\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(x,y)}+\sum_{x}p(x)log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(x,y)}+\sum_{x}{(\sum_{y}p(x,y))}log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)logP(y|x)}+\sum_{x,y}p(x,y)log{p(x)}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)}log{\frac{p(x,y)}{p(x)}}\\\ \ \ =-\sum_{x,y}^{}{P(x,y)}logp(y|x)\end{array}</script><p>其中：</p><ul><li>第二行推到第三行的依据是边缘分布$P(x)$等于联合分布$P(x,y)$的和；</li><li>第三行推到第四行的依据是把公因子$logP(x)$乘进去，然后把$x$,$y$写在一起；</li><li>第四行推到第五行的依据是：因为两个sigma都有$P(x,y)$，故提取公因子$P(x,y)$放到外边，然后把里边的$-（log P(x,y) - log P(x)）$写成$- log (P(x,y) / P(x) ) $；</li><li>第五行推到第六行的依据是：$P(x,y) = P(x) * P(y|x)$，故$P(x,y) / P(x) =  P(y|x)$。   </li></ul><h4 id="4-4-相对熵"><a href="#4-4-相对熵" class="headerlink" title="4.4 相对熵"></a>4.4 相对熵</h4><p>相对熵又称互熵、交叉熵、$KL$散度、信息增益，是描述两个概率分布$P$和$Q$差异的一种方法，记为<strong>$D(P||Q)$</strong>。在信息论中，$D(P||Q)$表示当用概率分布$Q$来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，$Q$表示$P$的拟合分布。</p><p>对于一个离散随机变量的两个概率分布$P$和$Q$来说，它们的相对熵定义为：</p><script type="math/tex; mode=display">D\left( P||Q \right) =\sum_{i=1}^{n}{P\left( x_{i} \right) log\frac{P\left( x_{i} \right) }{Q\left( x_{i} \right) } }</script><p>注意：$D(P||Q) ≠ D(Q||P)$</p><p>相对熵又称<strong>$KL$散度($Kullback–Leibler divergence$)</strong>，$KL$散度也是一个机器学习中常考的概念。</p><h4 id="4-5-互信息"><a href="#4-5-互信息" class="headerlink" title="4.5 互信息"></a>4.5 互信息</h4><p>两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵称为互信息，用I(X,Y)表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。  </p><script type="math/tex; mode=display">I\left( X,Y \right) =\sum_{x\in X}^{}{\sum_{y\in Y}^{}{P\left( x,y \right) } log\frac{P\left( x,y \right) }{P\left( x \right) P\left( y \right) } }</script><p>互信息、熵和条件熵之间存在以下关系：$ H\left( Y|X \right) =H\left( Y \right) -I\left( X,Y \right) $  </p><p>推导过程如下：</p><p><img src="https://pic3.zhimg.com/v2-6f41bffde009999cbc370f7f38cab092_r.jpg" alt="img"></p><p>通过上面的计算过程发现有：$H(Y|X) = H(Y) - I(X,Y)$，又由前面条件熵的定义有：$H(Y|X) = H(X,Y) - H(X)$，于是有$I(X,Y)= H(X) + H(Y) - H(X,Y)$，此结论被多数文献作为互信息的定义。</p><h3 id="4-6、最大熵模型"><a href="#4-6、最大熵模型" class="headerlink" title="4-6、最大熵模型"></a>4-6、最大熵模型</h3><p>最大熵原理是概率模型学习的一个准则，它认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。通常用约束条件来确定模型的集合，所以，最大熵模型原理也可以表述为：在满足约束条件的模型集合中选取熵最大的模型。 </p><p>前面我们知道，若随机变量X的概率分布是$P\left( x_{i} \right)$ ，则其熵定义如下：    </p><script type="math/tex; mode=display">H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i} \right) logP\left( x_{i} \right) } =\sum_{i=1}^{n}{P\left( x_{i} \right) \frac{1}{logP\left( x_{i} \right) } }</script><p>熵满足下列不等式：</p><script type="math/tex; mode=display">0\leq H\left( X \right) \leq log\left| X \right|</script><p>式中，$|X|$是$X$的取值个数，当且仅当$X$的分布是均匀分布时右边的等号成立。也就是说，当$X$服从均匀分布时，熵最大。</p><p>直观地看，最大熵原理认为：要选择概率模型，首先必须满足已有的事实，即约束条件；在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性；“等可能”不易操作，而熵则是一个可优化的指标。</p><h2 id="五、-数值计算"><a href="#五、-数值计算" class="headerlink" title="五、 数值计算"></a>五、 数值计算</h2><h3 id="5-1-上溢和下溢"><a href="#5-1-上溢和下溢" class="headerlink" title="5.1 上溢和下溢"></a>5.1 上溢和下溢</h3><p>在数字计算机上实现连续数学的基本困难是：我们需要通过有限数量的位模式来表示无限多的实数，这意味着我们在计算机中表示实数时几乎都会引入一些近似误差。在许多情况下，这仅仅是舍入误差。如果在理论上可行的算法没有被设计为最小化舍入误差的累积，可能会在实践中失效，因此舍入误差是有问题的，特别是在某些操作复合时。</p><p>一种特别毁灭性的舍入误差是下溢。当接近零的数被四舍五入为零时发生下溢。许多函数会在其参数为零而不是一个很小的正数时才会表现出质的不同。例如，我们通常要避免被零除。</p><p>另一个极具破坏力的数值错误形式是上溢(overflow)。当大量级的数被近似为\varpi 或-\varpi 时发生上溢。进一步的运算通常将这些无限值变为非数字。</p><p>必须对上溢和下溢进行数值稳定的一个例子是softmax 函数。softmax 函数经常用于预测与$multinoulli$分布相关联的概率，定义为：</p><p><img src="https://pic2.zhimg.com/80/v2-7283f680255ba0da3a69f2df58b58ae0_hd.jpg" alt=""></p><p>softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本 $x<em>{i} $的类别标签$ y</em>{i}$ 属于$K$个类别的概率，最后判别$ y_{i}$ 所属类别时就是将其归为对应概率最大的那一个。</p><p>当式中的$w<em>{k} x</em>{i} +b$都是很小的负数时，$e^{w<em>{k} x</em>{i} +b }$ 就会发生下溢，这意味着上面函数的分母会变成0，导致结果是未定的；同理，当式中的$x<em>{w</em>{k} x<em>{i} +b} $是很大的正数时，$e^{w</em>{k} x_{i} +b }$ 就会发生上溢导致结果是未定的。</p><h3 id="5-2-计算复杂性与NP问题"><a href="#5-2-计算复杂性与NP问题" class="headerlink" title="5.2 计算复杂性与NP问题"></a>5.2 计算复杂性与NP问题</h3><h4 id="1-算法复杂性"><a href="#1-算法复杂性" class="headerlink" title="1.算法复杂性"></a>1.算法复杂性</h4><p>现实中大多数问题都是离散的数据集，为了反映统计规律，有时数据量很大，而且多数目标函数都不能简单地求得解析解。这就带来一个问题：算法的复杂性。</p><p>算法理论被认为是解决各类现实问题的方法论。衡量算法有两个重要的指标：时间复杂度和空间复杂度，这是对算法执行所需要的两类资源——时间和空间的估算。</p><p>一般，衡量问题是否可解的重要指标是：该问题能否在多项式时间内求解，还是只能在指数时间内求解？在各类算法理论中，通常使用多项式时间算法即可解决的问题看作是易解问题，需要指数时间算法解决的问题看作是难解问题。</p><p>指数时间算法的计算时间随着问题规模的增长而呈指数化上升，这类问题虽然有解，但并不适用于大规模问题。所以当前算法研究的一个重要任务就是将指数时间算法变换为多项式时间算法。</p><h4 id="2-确定性和非确定性"><a href="#2-确定性和非确定性" class="headerlink" title="2.确定性和非确定性"></a>2.确定性和非确定性</h4><p>除了问题规模与运算时间的比较，衡量一个算法还需要考虑确定性和非确定性的概念。</p><p>这里先介绍一下“自动机”的概念。自动机实际上是指一种基于状态变化进行迭代的算法。在算法领域常把这类算法看作一个机器，比较知名的有图灵机、玻尔兹曼机、支持向量机等。</p><p>所谓确定性，是指针对各种自动机模型，根据当时的状态和输入，若自动机的状态转移是唯一确定的，则称确定性；若在某一时刻自动机有多个状态可供选择，并尝试执行每个可选择的状态，则称为非确定性。</p><p>换个说法就是：确定性是程序每次运行时产生下一步的结果是唯一的，因此返回的结果也是唯一的；非确定性是程序在每个运行时执行的路径是并行且随机的，所有路径都可能返回结果，也可能只有部分返回结果，也可能不返回结果，但是只要有一个路径返回结果，那么算法就结束。</p><p>在求解优化问题时，非确定性算法可能会陷入局部最优。</p><h4 id="3-NP问题"><a href="#3-NP问题" class="headerlink" title="3.NP问题"></a>3.NP问题</h4><p>有了时间上的衡量标准和状态转移的确定性与非确定性的概念，我们来定义一下问题的计算复杂度。</p><p>P类问题就是能够以多项式时间的确定性算法来对问题进行判定或求解，实现它的算法在每个运行状态都是唯一的，最终一定能够确定一个唯一的结果——最优的结果。</p><p>NP问题是指可以用多项式时间的非确定性算法来判定或求解，即这类问题求解的算法大多是非确定性的，但时间复杂度有可能是多项式级别的。</p><p>但是，NP问题还要一个子类称为NP完全问题，它是NP问题中最难的问题，其中任何一个问题至今都没有找到多项式时间的算法。</p><p>机器学习中多数算法都是针对NP问题（包括NP完全问题）的。</p><h3 id="5-3-数值计算"><a href="#5-3-数值计算" class="headerlink" title="5.3 数值计算"></a>5.3 数值计算</h3><p>上面已经分析了，大部分实际情况中，计算机其实都只能做一些近似的数值计算，而不可能找到一个完全精确的值，这其实有一门专门的学科来研究这个问题，这门学科就是——数值分析（有时也叫作“计算方法”）；运用数值分析解决问题的过程为：实际问题→数学模型→数值计算方法→程序设计→上机计算求出结果。</p><p>计算机在做这些数值计算的过程中，经常会涉及到的一个东西就是“迭代运算”，即通过不停的迭代计算，逐渐逼近真实值（当然是要在误差收敛的情况下）。</p><h2 id="六、最优化"><a href="#六、最优化" class="headerlink" title="六、最优化"></a>六、最优化</h2><p>本节介绍机器学习中的一种重要理论——最优化方法。</p><h3 id="6-1-最优化理论"><a href="#6-1-最优化理论" class="headerlink" title="6.1 最优化理论"></a>6.1 最优化理论</h3><p>无论做什么事，人们总希望以最小的代价取得最大的收益。在解决一些工程问题时，人们常会遇到多种因素交织在一起与决策目标相互影响的情况；这就促使人们创造一种新的数学理论来应对这一挑战，也因此，最早的优化方法——线性规划诞生了。</p><p>在李航博士的《统计学习方法》中，其将机器学习总结为如下表达式：</p><p>机器学习 = 模型 + 策略 + 算法</p><p>可以看得出，算法在机器学习中的 重要性。实际上，这里的算法指的就是优化算法。在面试机器学习的岗位时，优化算法也是一个特别高频的问题，大家如果真的想学好机器学习，那还是需要重视起来的。</p><h3 id="6-2-最优化问题的数学描述"><a href="#6-2-最优化问题的数学描述" class="headerlink" title="6.2 最优化问题的数学描述"></a>6.2 最优化问题的数学描述</h3><p>最优化的基本数学模型如下：</p><p><img src="https://pic3.zhimg.com/80/v2-f35226b3e0fa018db6a4b233c51eccbe_hd.jpg" alt=""></p><p>它有三个基本要素，即：</p><p>设计变量：$x$是一个实数域范围内的$n$维向量，被称为决策变量或问题的解；<br>目标函数：$f(x)$为目标函数；<br>约束条件：$h<em>{i} \left( x \right) =0$称为等式约束，$g</em>{i} \left( x \right) \leq 0$为不等式约束，$i=0,1,2,……$</p><h3 id="6-3-凸集与凸集分离定理"><a href="#6-3-凸集与凸集分离定理" class="headerlink" title="6.3 凸集与凸集分离定理"></a>6.3 凸集与凸集分离定理</h3><h4 id="1-凸集"><a href="#1-凸集" class="headerlink" title="1.凸集"></a>1.凸集</h4><p>实数域R上（或复数C上）的向量空间中，如果集合S中任两点的连线上的点都在S内，则称集合S为凸集，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-608f89f47688c41e4c3f83cfad095c84_hd.jpg" alt=""></p><p>数学定义为：</p><p>设集合$D\subset R^{n} $，若对于任意两点$x,y\in D$，及实数$\lambda \left( 0\leq \lambda \leq 1 \right)$ 都有：   </p><script type="math/tex; mode=display">\lambda x+\left( 1-\lambda \right) y\in D</script><p>则称集合$D$为凸集。</p><h4 id="2-超平面和半空间"><a href="#2-超平面和半空间" class="headerlink" title="2.超平面和半空间"></a>2.超平面和半空间</h4><p>实际上，二维空间的超平面就是一条线（可以使曲线），三维空间的超平面就是一个面（可以是曲面）。其数学表达式如下：</p><p>超平面：$H=\left\{ x\in R^{n} |a<em>{1} +a</em>{2}+…+a_{n} =b \right\} $</p><p>半空间：$H^{+} =\left\{ x\in R^{n} |a<em>{1} +a</em>{2}+…+a_{n} \geq b \right\} $ </p><h4 id="3-凸集分离定理"><a href="#3-凸集分离定理" class="headerlink" title="3.凸集分离定理"></a>3.凸集分离定理</h4><p>所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用一张超平面将两者隔在两边，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-4116a3bda12faa5e2421ce27efb7fb71_hd.jpg" alt=""></p><h4 id="4-凸函数"><a href="#4-凸函数" class="headerlink" title="4.凸函数"></a>4.凸函数</h4><p>凸函数就是一个定义域在某个向量空间的凸子集C上的实值函数。</p><p><img src="https://pic2.zhimg.com/80/v2-f1b39d0aad4388433158679221f813d2_hd.jpg" alt=""></p><p>数学定义为： </p><p>对于函数$f(x)$，如果其定义域$C$是凸的，且对于$∀x,y∈C，0\leq \alpha \leq 1$，<br> 有：  </p><script type="math/tex; mode=display">f\left( \theta x+\left( 1-\theta \right) y \right) \leq \theta f\left( x \right) +\left( 1-\theta \right) f\left( y \right)</script><p>则$f(x)$是凸函数。</p><p>注：如果一个函数是凸函数，则其局部最优点就是它的全局最优点。这个性质在机器学习算法优化中有很重要的应用，因为机器学习模型最后就是在求某个函数的全局最优点，一旦证明该函数（机器学习里面叫“损失函数”）是凸函数，那相当于我们只用求它的局部最优点了。</p><h3 id="6-4-梯度下降算法"><a href="#6-4-梯度下降算法" class="headerlink" title="6.4 梯度下降算法"></a>6.4 梯度下降算法</h3><h4 id="1-引入"><a href="#1-引入" class="headerlink" title="1.引入"></a>1.引入</h4><p>前面讲数值计算的时候提到过，计算机在运用迭代法做数值计算（比如求解某个方程组的解）时，只要误差能够收敛，计算机最后经过一定次数的迭代后是可以给出一个跟真实解很接近的结果的。</p><p>这里进一步提出一个问题，如果我们得到的目标函数是非线性的情况下，按照哪个方向迭代求解误差的收敛速度会最快呢？</p><p>答案就是沿梯度方向。这就引入了我们的梯度下降法。</p><h4 id="2-梯度下降法"><a href="#2-梯度下降法" class="headerlink" title="2.梯度下降法"></a>2.梯度下降法</h4><p>在多元微分学中，梯度就是函数的导数方向。</p><p>梯度法是求解无约束多元函数极值最早的数值方法，很多机器学习的常用算法都是以它作为算法框架，进行改进而导出更为复杂的优化方法。</p><p>在求解目标函数f\left( x \right) 的最小值时，为求得目标函数的一个凸函数，在最优化方法中被表示为：</p><script type="math/tex; mode=display">minf\left( x \right)</script><p>根据导数的定义，函数$f\left( x \right) $的导函数就是目标函数在$x$上的变化率。在多元的情况下，目标函数$f\left( x,y,z \right) $在某点的梯度$grad f\left( x,y,z \right) =\left( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z} \right)$ 是一个由各个分量的偏导数构成的向量，负梯度方向是$f\left( x,y,z \right) $减小最快的方向。</p><p><img src="https://pic4.zhimg.com/80/v2-e61c38f10e34badf5b2c1f3b9c9bcfa0_hd.jpg" alt=""></p><p>如上图所示，当需要求$f\left( x \right) $的最小值时（机器学习中的$f\left( x \right) $一般就是损失函数，而我们的目标就是希望损失函数最小化），我们就可以先任意选取一个函数的初始点$x<em>{0} $（三维情况就是$\left( x</em>{0} ,y<em>{0} ,z</em>{0} \right)$ ），让其沿着途中红色箭头（负梯度方向）走，依次到$x<em>{1} ，x</em>{2} ，…，x_{n} $（迭代n次）这样可最快达到极小值点。</p><p>梯度下降法过程如下：</p><p>输入：目标函数$f\left( x \right) $，梯度函数$g\left( x \right) =grad f\left( x \right) $，计算精度$\varepsilon $</p><p>输出：$f\left( x \right) 的极小值点x^{*} $</p><ol><li>任取取初始值$x_{0}$ ，置$k=0$；</li><li>计算$f\left( x_{k} \right) $；</li><li>计算梯度$g<em>{k} =grad f\left( x</em>{k} \right) $，当$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $时停止迭代，令$x^{*} =x</em>{k}$ ；</li><li>否则令$P<em>{k} =-g</em>{k}$ ，求$\lambda <em>{k} 使f\left( x</em>{k+1} \right) =minf\left( x<em>{k} +\lambda </em>{k} P_{k} \right) $；</li><li>置$x<em>{k+1} =x</em>{k} +\lambda <em>{k} P</em>{k} $，计算$f\left( x<em>{k+1}\right) $，当$\left| \left| f\left( x</em>{k+1}\right) -f\left( x<em>{k}\right) \right| \right| &lt;\varepsilon 或\left| \left| x</em>{k+1} -x<em>{k} \right| \right| &lt;\varepsilon $时，停止迭代，令$x^{*} =x</em>{k+1}  $；</li><li>否则，置$k=k+1$，转$3$。</li></ol><h3 id="6-5-随机梯度下降算法"><a href="#6-5-随机梯度下降算法" class="headerlink" title="6.5 随机梯度下降算法"></a>6.5 随机梯度下降算法</h3><p>上面可以看到，在梯度下降法的迭代中，除了梯度值本身的影响外，还有每一次取的步长\lambda _{k} 也很关键：步长值取得越大，收敛速度就会越快，但是带来的可能后果就是容易越过函数的最优点，导致发散；步长取太小，算法的收敛速度又会明显降低。因此我们希望找到一种比较好的方法能够平衡步长。</p><p>随机梯度下降法并没有新的算法理论，仅仅是引进了随机样本抽取方式，并提供了一种动态步长取值策略。目的就是又要优化精度，又要满足收敛速度。</p><p>也就是说，上面的批量梯度下降法每次迭代时都会计算训练集中所有的数据，而随机梯度下降法每次迭代只是随机取了训练集中的一部分样本数据进行梯度计算，这样做最大的好处是可以避免有时候陷入局部极小值的情况（因为批量梯度下降法每次都使用全部数据，一旦到了某个局部极小值点可能就停止更新了；而随机梯度法由于每次都是随机取部分数据，所以就算局部极小值点，在下一步也还是可以跳出）</p><p>两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p><h3 id="6-6-牛顿法"><a href="#6-6-牛顿法" class="headerlink" title="6.6 牛顿法"></a>6.6 牛顿法</h3><h4 id="1-牛顿法介绍"><a href="#1-牛顿法介绍" class="headerlink" title="1.牛顿法介绍"></a>1.牛顿法介绍</h4><p>牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。</p><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法 每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以， 可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。</p><p><img src="https://pic3.zhimg.com/80/v2-e22ea8c565434e945a17a80bec5630b6_hd.jpg" alt=""><br>或者从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p><h4 id="2-牛顿法的推导"><a href="#2-牛顿法的推导" class="headerlink" title="2.牛顿法的推导"></a>2.牛顿法的推导</h4><p>将目标函数$f\left( x \right)  在x_{k}$ 处进行二阶泰勒展开，可得：   </p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k} \right) +f^{'} \left( x_{k} \right) \left( x-x_{k} \right) +\frac{1}{2} f^{''}\left( x_{k} \right) \left( x-x_{k} \right) ^{2}</script><p>因为目标函数$f\left( x \right)$ 有极值的必要条件是在极值点处一阶导数为0，即：$f^{‘} \left( x \right) =0 $</p><p>所以对上面的展开式两边同时求导（注意$x$才是变量，$x<em>{k}$ 是常量$\Rightarrow f^{‘} \left( x</em>{k} \right) ,f^{‘’} \left( x_{k} \right)$ 都是常量），并令$f^{‘} \left( x \right) =0$可得：  </p><script type="math/tex; mode=display">f^{'} \left( x_{k} \right) +f^{''} \left( x_{k} \right) \left( x-x_{k} \right) =0</script><p>即：   </p><script type="math/tex; mode=display">x=x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>于是可以构造如下的迭代公式：</p><script type="math/tex; mode=display">x_{k+1} =x_{k} -\frac{f^{'} \left( x_{k} \right) }{f^{''} \left( x_{k} \right) }</script><p>这样，我们就可以利用该迭代式依次产生的序列$\left\{x<em>{1},x</em>{2},…., x_{k} \right\} $才逐渐逼近$f\left( x \right)$ 的极小值点了。</p><p>牛顿法的迭代示意图如下：</p><p><img src="https://pic1.zhimg.com/80/v2-e908f9721cc82415fa7e70c763351f3a_hd.jpg" alt=""></p><p>上面讨论的是2维情况，高维情况的牛顿迭代公式是：</p><p>式中，$ ▽f是f\left( x \right)$ 的梯度，即：</p><p><img src="https://pic4.zhimg.com/80/v2-71df54a8e32e172596dcaa07e6b31899_hd.jpg" alt=""></p><p>H是Hessen矩阵，即：</p><p><img src="https://pic4.zhimg.com/80/v2-2891044fd02769c3148649e2a1a01fd5_hd.jpg" alt=""></p><h4 id="3-牛顿法的过程"><a href="#3-牛顿法的过程" class="headerlink" title="3.牛顿法的过程"></a>3.牛顿法的过程</h4><ol><li>给定初值$x_{0} $和精度阈值$\varepsilon $，并令$k=0$；</li><li>计算$x<em>{k} $和$H</em>{k}$ ；</li><li>若$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $则停止迭代；否则确定搜索方向：$d</em>{k} =-H<em>{k}^{-1} \cdot g</em>{k}$ ；</li><li>计算新的迭代点：$x<em>{k+1} =x</em>{k} +d_{k} $；</li><li>令$k=k+1$，转至$2$。</li></ol><h3 id="6-7-阻尼牛顿法"><a href="#6-7-阻尼牛顿法" class="headerlink" title="6.7 阻尼牛顿法"></a>6.7 阻尼牛顿法</h3><h4 id="1-引入-1"><a href="#1-引入-1" class="headerlink" title="1.引入"></a>1.引入</h4><p>注意到，牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现$f\left( x<em>{k+1} \right) &gt;f\left( x</em>{k} \right) $的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。</p><p>为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是$x<em>{k} $，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子$\lambda </em>{k}$ ，即：</p><p>$\lambda <em>{k} = minf\left( x</em>{k} +\lambda d_{k} \right) $</p><h4 id="2-算法过程"><a href="#2-算法过程" class="headerlink" title="2.算法过程"></a>2.算法过程</h4><ol><li>给定初值$x_{0}$ 和精度阈值$\varepsilon$ ，并令$k=0$；</li><li>计算$g<em>{k} （f\left( x \right) $在$x</em>{k}$ 处的梯度值）和$H_{k} $；</li><li>若$\left| \left| g<em>{k} \right| \right| &lt;\varepsilon $则停止迭代；否则确定搜索方向：$d</em>{k} =-H<em>{k}^{-1} \cdot g</em>{k}$ ；</li><li>利用$d<em>{k} =-H</em>{k}^{-1} \cdot g<em>{k} 得到步长\lambda </em>{k} $，并令$x<em>{k+1} =x</em>{k} +\lambda <em>{k} d</em>{k}$  </li><li>令$k=k+1$，转至$2$。</li></ol><h3 id="6-8-拟牛顿法"><a href="#6-8-拟牛顿法" class="headerlink" title="6.8 拟牛顿法"></a>6.8 拟牛顿法</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h4><p>由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵，计算量比较大（求矩阵的逆运算量比较大），因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。</p><h4 id="2-拟牛顿法的推导"><a href="#2-拟牛顿法的推导" class="headerlink" title="2.拟牛顿法的推导"></a>2.拟牛顿法的推导</h4><p>先将目标函数在$x_{k+1} $处展开，得到：  </p><script type="math/tex; mode=display">f\left( x \right) =f\left( x_{k+1} \right) +f^{'} \left( x_{k+1} \right) \left( x-x_{k+1} \right) +\frac{1}{2} f^{''}\left( x_{k+1} \right) \left( x-x_{k+1} \right) ^{2}</script><p>两边同时取梯度，得：  </p><script type="math/tex; mode=display">f^{'}\left( x \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>取上式中的$x=x_{k}$ ，得：   </p><script type="math/tex; mode=display">f^{'}\left( x_{k} \right) = f^{'} \left( x_{k+1} \right) +f^{''} \left( x_{k+1} \right) \left( x-x_{k+1} \right)</script><p>即：   </p><script type="math/tex; mode=display">g_{k+1} -g_{k} =H_{k+1} \cdot \left( x_{k+1} -x_{k} \right)</script><p>可得：  </p><script type="math/tex; mode=display">H_{k}^{-1} \cdot \left( g_{k+1} -g_{k} \right) =x_{k+1} -x_{k}</script><p>上面这个式子称为“拟牛顿条件”，由它来对$Hessen$矩阵做约束。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2-1创建图 启动图</title>
      <link href="/2017/12/26/2-1%E5%88%9B%E5%BB%BA%E5%9B%BE%20%E5%90%AF%E5%8A%A8%E5%9B%BE/"/>
      <url>/2017/12/26/2-1%E5%88%9B%E5%BB%BA%E5%9B%BE%20%E5%90%AF%E5%8A%A8%E5%9B%BE/</url>
      <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m1=tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">m2=tf.constant([[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">product=tf.matmul(m1,m2)</span><br><span class="line">print(product)</span><br></pre></td></tr></table></figure><pre><code>Tensor(&quot;MatMul_1:0&quot;, shape=(1, 1), dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.Session()</span><br><span class="line">result=sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result=sess.run(product)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Python学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>三体-星际穿越</title>
      <link href="/2017/10/02/%E4%B8%89%E4%BD%93-%E7%BB%B4%E5%BE%B7%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E7%94%B7%E4%B8%BB/"/>
      <url>/2017/10/02/%E4%B8%89%E4%BD%93-%E7%BB%B4%E5%BE%B7%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E7%94%B7%E4%B8%BB/</url>
      <content type="html"><![CDATA[<p>我是在敦煌的戈壁滩上，看完整个《三体》，</p><p>4月份的戈壁滩，外面虽然太阳高照，但是气温仍然很低，还包含着大风沙，听着风从车的缝隙中钻入，人在外面没有防护措施的呆不过十分钟，就会被大风带走身上大部分的热量，人在车外工作十分钟就会迫不及待的跳上车子，在这里，前前后后十几公里都不会有人存在，整个车子都沉浸在风沙中，在这里，你只会觉得整个戈壁滩上，只有这个车子才是你唯一的依靠，出了车子，你绝对活不过数十分钟的。<br>就是在这种环境下，我看完了整个三体，<br>将车想做是飞船，将外面的戈壁滩想做的荒凉的沙漠，<br>而我们的保证仅仅是几厘米厚的钢板，</p><p>其中的故事情节，有太多的解读，这里写下我的感受最深的。<br>维德-为了目的不择手段，是为了人类的延续，真正的男人<br>反观女主，优柔寡断，在关键时刻，没能把握机会</p><p>星际穿越，也是我看的感触比较深的电影，<br>一、是在男主骗女主要去新的星球，最后将机会给了女主，那句“物理学第二定律，总要留下些什么”<br>二、结尾，男主在恢复后，义无反顾的“偷来”宇航船，去奔向女主，不论是什么原因，爱情还是同甘共苦的情谊，感受到了人类的顽强生生不息，这种感觉不是简单的迸发，而是整个电影的铺垫，</p><p>《星际穿越》诺兰大神，拍出了不一样的意味，整个电影不像一部电影，而像一部纪录片，其中无数的人性冲突，理性的煎熬，诺兰德早就知道计算结果的局限，直到生命的最后一刻，才说出那句对不起，男主的女儿，苦苦思索，到电影最后，终于思索出，书架上的暗示，整个电影不喜不悲，或者说已经很艺术的处理物理环境的恶劣，人类无论是哲学上的处于“不自知”的状态，还是在整个宇宙环境下，都是脆弱的，人类距离外太空，只有几公里的大气层和地球的磁场的保护，没有的磁场保护，人类只能存活半小时，没有大气层，人类连十分钟都活不到，</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>星际穿越出现的诗</title>
      <link href="/2017/03/22/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E5%B0%8F%E8%AF%97/"/>
      <url>/2017/03/22/%E6%98%9F%E9%99%85%E7%A9%BF%E8%B6%8A-%E5%B0%8F%E8%AF%97/</url>
      <content type="html"><![CDATA[<p><strong>Do not go gentle into that good night</strong><br>-Dylan Thomas </p><p><em>Do not go gentle into that good night,<br>Old age should burn and rave at close of day;  　　<br>Rage, rage against the dying of the light.  　　  　　<br>Though wise men at their end know dark is right,  　　<br>Because their words had forked no lightning they  　　<br>Do not go gentle into that good night.  　　  　　<br>Good men, the last wave by, crying how bright  　　<br>Their frail deeds might have danced in a green bay,  　　<br>Rage, rage against the dying of the light.  　　  　　<br>Wild men who caught and sang the sun in flight,  　　<br>And learn, too late, they grieved it on its way,  　　<br>Do not go gentle into that good night.  　　  　　<br>Grave men, near death, who see with blinding sight  　　<br>Blind eyes could blaze like meteors and be gay,  　　<br>Rage, rage against the dying of the light.  　　  　　<br>And you, my father, there on the sad height,  　　<br>Curse, bless, me now with your fierce tears.,I pray.<br>Do not go gentle into that good night,    　<br>Rage, rage against the dying of the light. </em></p><p>《不要温和地走进那个良夜》</p><p>狄兰·托马斯</p><p>不要温和地走进那个良夜，<br>老年应当在日暮时燃烧咆哮；<br>怒斥，怒斥光明的消逝。</p><p>虽然智慧的人临终时懂得黑暗有理，<br>因为他们的话没有迸发出闪电，他们<br>也并不温和地走进那个良夜。</p><p>善良的人，当最后一浪过去，高呼他们脆弱的善行<br>可能曾会多么光辉地在绿色的海湾里舞蹈，<br>怒斥，怒斥光明的消逝。</p><p>狂暴的人抓住并歌唱过翱翔的太阳，<br>懂得，但为时太晚，他们使太阳在途中悲伤，<br>也并不温和地走进那个良夜。</p><p>严肃的人，接近死亡，用炫目的视觉看出<br>失明的眼睛可以像流星一样闪耀欢欣，<br>怒斥，怒斥光明的消逝。</p><p>你啊，我的父亲。在那悲哀的高处。<br>现在用您的热泪诅咒我，祝福我吧。我求您<br>不要温和地走进那个良夜。<br>怒斥，怒斥光明的消逝。</p>]]></content>
      
      
        <tags>
            
            <tag> 随想 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>New test</title>
      <link href="/2016/04/25/New%20test/"/>
      <url>/2016/04/25/New%20test/</url>
      <content type="html"><![CDATA[<p>fefefefe<br>abcd这也是一篇测试文章</p><p>假设X是一个离散随机变量，其可能的取值有：$\left\{ x<em>1 ,x_2 ,……,x</em>{n} \right\}$，各个取值对应的概率取值为：$P\left( x_k \right) , k=1,2,……,n $，则其数学期望被定义为：</p><p>$\sum=er_dew11$</p><p>$\{ x_1 ,x_2 ,……,x_n \}$<br>efefewfwefwefwefweefe<br>$P( x_k) , k=1,2,……,n $</p><script type="math/tex; mode=display">efewfw\frac{1}{trhtr7}</script><p><img src="/2016/04/25/New test/img8.jpg" alt=""></p>]]></content>
      
      
        <tags>
            
            <tag> 主题测试 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
